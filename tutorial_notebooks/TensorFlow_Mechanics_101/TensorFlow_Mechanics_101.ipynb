{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Mechanics 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes Chong Min made\n",
    "\n",
    "* Optimizer: from Gradient Descent to Adam\n",
    "* batch size: set to 10\n",
    "* size of hidden layer: 8\n",
    "* weight initializer: random_normal_initializer\n",
    "* max steps: 20000\n",
    "\n",
    "Among the above changes, `batch size`, `optimizer`, and `max steps` values were critical.\n",
    "\n",
    "Because batches were randomly generated, the accuracies were flucturated. It should be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tutorial is meant as a companion to the code [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist/)\n",
    "- The goal of this tutorial is to show how to use TensorFlow to train and evaluate a simple feed-forward neural network for handwritten digit classification using the (classic) MNIST data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`mnist.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py), the code for making a fully-connected MNIST model\n",
    "- [`fully_connected_feed.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py), the main code to train the built MNIST model against the downloaded dataset using a feed dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom Data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download test_data_revised.zip (in email since it can't be shared) and\n",
    "# save it somewhere, preferably in the directory in which this notebook is\n",
    "# stored. If it is somewhere else, just make sure to pass in the path when\n",
    "# this function is used.\n",
    "\n",
    "def read_data(dataset, macro_or_micro=\"macro\",\n",
    "              dev_set=True,\n",
    "              data_path=join(getcwd(), \"test_data_revised\")):\n",
    "    \"\"\"\n",
    "    Read in data from a directory\n",
    "\n",
    "    Either read in the \"macro\" files or the \"micro\" files.\n",
    "\n",
    "    Returns 1) training IDs, features, and labels,\n",
    "            2) test IDs, features, and labels, and\n",
    "            3) development set IDs, features, and labels (if `dev_set` is\n",
    "               False, the test test set will contain all data originally in\n",
    "               the \"testing\" file).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data_path = join(data_path,\n",
    "                           \"{}_dataset_training_{}.csv\"\n",
    "                           .format(dataset, macro_or_micro if macro_or_micro == \"macro\"\n",
    "                                   else \"micro_revised\"))\n",
    "    train_data = pd.read_csv(train_data_path, dtype={'appointment_id': str})\n",
    "    train_labels = train_data['H1'].apply(lambda x: x - 1)\n",
    "    train_features = train_data[[a for a in train_data.columns\n",
    "                                 if a not in ['appointment_id', 'H1']]]\n",
    "    train_ids = train_data['appointment_id']\n",
    "\n",
    "    test_data_path = join(data_path,\n",
    "                          \"{}_dataset_testing_{}.csv\"\n",
    "                          .format(dataset, macro_or_micro if macro_or_micro == \"macro\"\n",
    "                                  else \"micro_revised\"))\n",
    "    test_data = pd.read_csv(test_data_path, dtype={'appointment_id': str})\n",
    "     \n",
    "    test_labels = test_data['H1'].apply(lambda x: x - 1)\n",
    "    test_features = test_data[[a for a in test_data.columns\n",
    "                               if a not in ['appointment_id', 'H1']]]\n",
    "    test_ids = test_data['appointment_id']\n",
    "\n",
    "    dev_features = None\n",
    "    dev_labels = None\n",
    "    dev_ids = None\n",
    "    if dev_set:\n",
    "        test_amount = len(test_features) - 1000\n",
    "        dev_features = test_features.head(1000)\n",
    "        dev_features.index = range(len(dev_features))\n",
    "        test_features = test_features.tail(test_amount)\n",
    "        test_features.index = range(len(test_features))\n",
    "        dev_labels = test_labels.head(1000)\n",
    "        dev_labels.index = range(len(dev_labels))\n",
    "        test_labels = test_labels.tail(test_amount)\n",
    "        test_labels.index = range(len(test_labels))\n",
    "        dev_ids = test_ids[:1000]\n",
    "        test_ids = test_ids[1000:]\n",
    "\n",
    "    return (train_ids,\n",
    "            train_features,\n",
    "            train_labels,\n",
    "            test_ids,\n",
    "            test_features,\n",
    "            test_labels,\n",
    "            dev_ids,\n",
    "            dev_features,\n",
    "            dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "\n",
    "    def __init__(self, ids, features, labels, random_=True):\n",
    "        prng = np.random.RandomState(12345)\n",
    "        self._ids = ids\n",
    "        self._features = features\n",
    "        self._labels = labels\n",
    "        self._index_in_epoch = 0\n",
    "        self._num_examples = len(self._ids)\n",
    "        if random_:\n",
    "            reindex = prng.permutation(self._features.index)\n",
    "            self._features = self._features.reindex(reindex)\n",
    "            self._ids = self._ids.reindex(reindex)\n",
    "            self._labels = self._labels.reindex(reindex)\n",
    "\n",
    "    def get_size(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "\n",
    "        start = self._index_in_epoch\n",
    "\n",
    "        # Go to the next epoch\n",
    "        if start + batch_size > self._num_examples:\n",
    "\n",
    "            # Get the rest examples in this epoch\n",
    "            remaining_examples = self._num_examples - start\n",
    "            features_rest_part = self._features[start:self._num_examples]\n",
    "            labels_rest_part = self._labels[start:self._num_examples]\n",
    "            ids_rest_part = self._ids[start: self._num_examples]\n",
    "\n",
    "\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - remaining_examples\n",
    "            end = self._index_in_epoch\n",
    "            features_new_part = self._features[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            ids_new_part = self._ids[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            return (np.concatenate((ids_rest_part, ids_new_part), axis=0),\n",
    "                    np.concatenate((features_rest_part, features_new_part), axis = 0), \n",
    "                   np.concatenate((labels_rest_part, labels_new_part), axis = 0))\n",
    "\n",
    "        else:\n",
    "    \n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._ids[start:end], self._features[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Based on `mnist.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(inputs, num_features, num_classes, hidden1_units,\n",
    "              hidden2_units, hidden3_units=None):\n",
    "    \"\"\"\n",
    "    Build a model on the inputs up to where it may be used for\n",
    "    inference.\n",
    "\n",
    "    Args:\n",
    "        inputs: Placeholder for input data samples.\n",
    "        num_features: Number of features in input data.\n",
    "        num_classes: Number of classes/score labels.\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "        hidden2_units: Size of the second hidden layer.\n",
    "        hidden3_units: Size of the third hidden layer (None if no\n",
    "                       third layer).\n",
    "\n",
    "    Returns:\n",
    "        softmax_linear: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([num_features, hidden1_units]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(inputs, weights) + biases)\n",
    "\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([hidden1_units, hidden2_units]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    if hidden3_units is not None:\n",
    "\n",
    "        # Hidden 3\n",
    "        with tf.name_scope('hidden3'):\n",
    "            weights = tf.Variable(\n",
    "                tf.random_normal_initializer(0.0, 0.05)([hidden2_units, hidden3_units],\n",
    "                                    stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "                name='weights')\n",
    "            biases = tf.Variable(tf.zeros([hidden3_units]),\n",
    "                                 name='biases')\n",
    "            hidden3 = tf.nn.relu(tf.matmul(hidden2, weights) + biases)\n",
    "        \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        \n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([hidden1_units, num_classes]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([num_classes]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2 if hidden3_units is None else hidden3,\n",
    "                           weights) + biases\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Calculates the loss from the logits and the labels.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size].\n",
    "\n",
    "    Returns:\n",
    "        loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = tf.to_int64(labels)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=labels, logits=logits, name='xentropy')\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')    \n",
    "\n",
    "\n",
    "def training_adam(_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Sets up the training Ops.\n",
    "    \n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates an Adam optimizer and applies the gradients to all trainable\n",
    "    variables.\n",
    "    \n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "\n",
    "    Args:\n",
    "        _loss: Loss tensor, from loss().\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', _loss)\n",
    "\n",
    "    # Create the Adam optimizer with the given learning\n",
    "    # rate.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single\n",
    "    # training step.\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def training_gradient_descent(_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Sets up the training Ops.\n",
    "    \n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates a gradient descent optimizer and applies the gradients to\n",
    "    all trainable variables.\n",
    "    \n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "\n",
    "    Args:\n",
    "        _loss: Loss tensor, from loss().\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', _loss)\n",
    "\n",
    "    # Create the gradient descent optimizer with the given learning\n",
    "    # rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single\n",
    "    # training step.\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def training_momentum(_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Sets up the training Ops.\n",
    "    \n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates a MomentumOptimizer and applies the gradients to\n",
    "    all trainable variables.\n",
    "    \n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "\n",
    "    Args:\n",
    "        _loss: Loss tensor, from loss().\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', _loss)\n",
    "\n",
    "    # Create the gradient descent optimizer with the given learning\n",
    "    # rate.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 2)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single\n",
    "    # training step.\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def training_adadelta(_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Sets up the training Ops.\n",
    "    \n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates a AdaDelata optimizer and applies the gradients to\n",
    "    all trainable variables.\n",
    "    \n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "\n",
    "    Args:\n",
    "        _loss: Loss tensor, from loss().\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', _loss)\n",
    "\n",
    "    # Create the gradient descent optimizer with the given learning\n",
    "    # rate.\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single\n",
    "    # training step.\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "                range [0, NUM_CLASSES).\n",
    "\n",
    "    Returns:\n",
    "        A scalar int32 tensor with the number of examples (out of\n",
    "        batch_size) that were predicted correctly.\n",
    "    \"\"\"\n",
    "\n",
    "    # For a classifier model, we can use the in_top_k Op.\n",
    "    # It returns a bool tensor with shape [batch_size] that is true for\n",
    "    # the examples where the label is in the top k (here k=1)\n",
    "    # of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "    # Return the number of true entries.\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From fully_connected_feed.py\n",
    "def fill_feed_dict(data, inputs_pl, labels_pl, batch_size):\n",
    "    \"\"\"\n",
    "    Fills the feed_dict for training the given step.\n",
    "\n",
    "    A feed_dict takes the form of:\n",
    "    feed_dict = {\n",
    "        <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "        ....\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        data_set: The set of features and labels.\n",
    "        inputs_pl: The input data placeholder.\n",
    "        labels_pl: The input labels placeholder.\n",
    "        batch_size: Size of each batch.\n",
    "\n",
    "    Returns:\n",
    "        feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the feed_dict for the placeholders filled with the next\n",
    "    # `batch_size` samples.\n",
    "    ids, inputs_feed, labels_feed = data.next_batch(batch_size)\n",
    "    feed_dict = {\n",
    "        inputs_pl: inputs_feed,\n",
    "        labels_pl: labels_feed,\n",
    "    }\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def do_eval(sess, eval_correct, inputs_placeholder, labels_placeholder, data):\n",
    "    \"\"\"\n",
    "    Runs one evaluation against the full epoch of data.\n",
    "\n",
    "    Args:\n",
    "        sess: The session in which the model has been trained.\n",
    "        eval_correct: The Tensor that returns the number of correct\n",
    "                      predictions.\n",
    "        inputs_placeholder: The input data placeholder.\n",
    "        labels_placeholder: The labels placeholder.\n",
    "        data: The set of images and labels to evaluate.\n",
    "    \"\"\"\n",
    "\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data.get_size() // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "        true_cnt = sess.run(eval_correct,\n",
    "                                          feed_dict=feed_dict)\n",
    "        true_count += true_cnt\n",
    "\n",
    "    acc = float(true_count) / num_examples\n",
    "    return {'num_examples': num_examples, 'num_correct': true_count, 'accuracy': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_graph_for_evaluation(train_data, test_data, optimizer_type, max_steps, dev_data=None):\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        # Generate placeholders for the input feature data and labels.\n",
    "        inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                               NUM_FEATURES))\n",
    "        labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        logits = inference(inputs_placeholder,\n",
    "                           NUM_FEATURES,\n",
    "                           NUM_CLASSES,\n",
    "                           hidden1,\n",
    "                           hidden2,\n",
    "                           hidden3_units=hidden3)\n",
    "\n",
    "        # Add to the Graph the Ops for loss calculation.\n",
    "        loss_ = loss(logits, labels_placeholder)\n",
    "\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        if optimizer_type == \"adam\":\n",
    "            train_op = training_adam(loss_, learning_rate)\n",
    "        elif optimizer_type == \"gradient_descent\":\n",
    "            train_op = training_gradient_descent(loss_, learning_rate)\n",
    "        elif optimizer_type == \"momentum\":\n",
    "            train_op = training_momentum(loss_, learning_rate)\n",
    "        elif optimizer_type == \"adadelta\":\n",
    "            train_op = training_adadelta(loss_, learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Choose either \\\"adam\\\" or \\\"gradient descent\\\" for \"\n",
    "                             \"`optimizer_type`.\")\n",
    "\n",
    "        # Add the Op to compare the logits to the labels during evaluation.\n",
    "        eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "        # Build the summary Tensor based on the TF collection of Summaries.\n",
    "        summary = tf.summary.merge_all()\n",
    "\n",
    "        # Add the variable initializer Op.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session for running Ops on the Graph.\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "        summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "        # And then after everything is built:\n",
    "\n",
    "        # Run the Op to initialize the variables.\n",
    "        sess.run(init)\n",
    "\n",
    "        # Start the training loop.\n",
    "        for step in range(max_steps):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Fill a feed dictionary with the actual set of images and labels\n",
    "            # for this particular training step.\n",
    "            feed_dict = fill_feed_dict(train_data,\n",
    "                                       inputs_placeholder,\n",
    "                                       labels_placeholder,\n",
    "                                       batch_size)\n",
    "\n",
    "            # Run one step of the model.  The return values are the activations\n",
    "            # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "            # inspect the values of your Ops or variables, you may include them\n",
    "            # in the list passed to sess.run() and the value tensors will be\n",
    "            # returned in the tuple from the call.\n",
    "            _, loss_value = sess.run([train_op, loss_],\n",
    "                                     feed_dict=feed_dict)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Save a checkpoint and evaluate the model periodically.\n",
    "            #if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            if (step + 1) == max_steps:\n",
    "                #print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                # Update the events file.\n",
    "                summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "\n",
    "                checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "                # Evaluate against the training set.\n",
    "                #print('Train Data Eval:')\n",
    "                train_data_eval_dict = do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        train_data)\n",
    "\n",
    "                # Evaluate against the development set.\n",
    "                if dev_labels is not None:\n",
    "                    print('Development Data Eval:')\n",
    "                    do_eval(sess,\n",
    "                            eval_correct,\n",
    "                            inputs_placeholder,\n",
    "                            labels_placeholder,\n",
    "                            dev_data)\n",
    "\n",
    "                # Evaluate against the test set.\n",
    "                #print('Test Data Eval:')\n",
    "                test_data_eval_dict = do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        test_data)\n",
    "                train_data_eval_dict['datatype'] = 'train'\n",
    "                train_data_eval_dict['optimizer'] = optimizer_type\n",
    "                train_data_eval_dict['max_steps'] = max_steps\n",
    "\n",
    "                test_data_eval_dict['datatype'] = 'test'\n",
    "                test_data_eval_dict['optimizer'] = optimizer_type\n",
    "                test_data_eval_dict['max_steps'] = max_steps\n",
    "\n",
    "\n",
    "                return [train_data_eval_dict, test_data_eval_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: first, optimizer: gradient_descent, max steps: 10000\n",
      "dataset: first, optimizer: gradient_descent, max steps: 20000\n",
      "dataset: first, optimizer: gradient_descent, max steps: 40000\n",
      "dataset: first, optimizer: gradient_descent, max steps: 60000\n",
      "dataset: first, optimizer: gradient_descent, max steps: 80000\n",
      "dataset: first, optimizer: gradient_descent, max steps: 100000\n",
      "dataset: first, optimizer: adam, max steps: 10000\n",
      "dataset: first, optimizer: adam, max steps: 20000\n",
      "dataset: first, optimizer: adam, max steps: 40000\n",
      "dataset: first, optimizer: adam, max steps: 60000\n",
      "dataset: first, optimizer: adam, max steps: 80000\n",
      "dataset: first, optimizer: adam, max steps: 100000\n",
      "dataset: first, optimizer: momentum, max steps: 10000\n",
      "dataset: first, optimizer: momentum, max steps: 20000\n",
      "dataset: first, optimizer: momentum, max steps: 40000\n",
      "dataset: first, optimizer: momentum, max steps: 60000\n",
      "dataset: first, optimizer: momentum, max steps: 80000\n",
      "dataset: first, optimizer: momentum, max steps: 100000\n",
      "dataset: first, optimizer: adadelta, max steps: 10000\n",
      "dataset: first, optimizer: adadelta, max steps: 20000\n",
      "dataset: first, optimizer: adadelta, max steps: 40000\n",
      "dataset: first, optimizer: adadelta, max steps: 60000\n",
      "dataset: first, optimizer: adadelta, max steps: 80000\n",
      "dataset: first, optimizer: adadelta, max steps: 100000\n",
      "\n",
      "   dataset         optimizer  max_steps datatype  num_examples  num_correct  \\\n",
      "37   first          adadelta      10000     test         18990         7527   \n",
      "36   first          adadelta      10000    train         24990         9915   \n",
      "39   first          adadelta      20000     test         18990         7528   \n",
      "38   first          adadelta      20000    train         24990         9916   \n",
      "41   first          adadelta      40000     test         18990         7527   \n",
      "40   first          adadelta      40000    train         24990         9914   \n",
      "43   first          adadelta      60000     test         18990         7527   \n",
      "42   first          adadelta      60000    train         24990         9915   \n",
      "45   first          adadelta      80000     test         18990         7525   \n",
      "44   first          adadelta      80000    train         24990         9914   \n",
      "47   first          adadelta     100000     test         18990         7527   \n",
      "46   first          adadelta     100000    train         24990         9915   \n",
      "13   first              adam      10000     test         18990        10813   \n",
      "12   first              adam      10000    train         24990        14137   \n",
      "15   first              adam      20000     test         18990        11137   \n",
      "14   first              adam      20000    train         24990        14525   \n",
      "17   first              adam      40000     test         18990        10925   \n",
      "16   first              adam      40000    train         24990        14507   \n",
      "19   first              adam      60000     test         18990        11188   \n",
      "18   first              adam      60000    train         24990        14751   \n",
      "21   first              adam      80000     test         18990        10821   \n",
      "20   first              adam      80000    train         24990        14424   \n",
      "23   first              adam     100000     test         18990        11195   \n",
      "22   first              adam     100000    train         24990        14787   \n",
      "1    first  gradient_descent      10000     test         18990         7524   \n",
      "0    first  gradient_descent      10000    train         24990         9916   \n",
      "3    first  gradient_descent      20000     test         18990         9001   \n",
      "2    first  gradient_descent      20000    train         24990        11998   \n",
      "5    first  gradient_descent      40000     test         18990         9634   \n",
      "4    first  gradient_descent      40000    train         24990        12777   \n",
      "7    first  gradient_descent      60000     test         18990         9986   \n",
      "6    first  gradient_descent      60000    train         24990        13194   \n",
      "9    first  gradient_descent      80000     test         18990         9297   \n",
      "8    first  gradient_descent      80000    train         24990        12488   \n",
      "11   first  gradient_descent     100000     test         18990        10134   \n",
      "10   first  gradient_descent     100000    train         24990        13440   \n",
      "25   first          momentum      10000     test         18990            0   \n",
      "24   first          momentum      10000    train         24990            0   \n",
      "27   first          momentum      20000     test         18990            0   \n",
      "26   first          momentum      20000    train         24990            0   \n",
      "29   first          momentum      40000     test         18990            0   \n",
      "28   first          momentum      40000    train         24990            0   \n",
      "31   first          momentum      60000     test         18990            0   \n",
      "30   first          momentum      60000    train         24990            0   \n",
      "33   first          momentum      80000     test         18990            0   \n",
      "32   first          momentum      80000    train         24990            0   \n",
      "35   first          momentum     100000     test         18990            0   \n",
      "34   first          momentum     100000    train         24990            0   \n",
      "\n",
      "    accuracy  \n",
      "37  0.396367  \n",
      "36  0.396759  \n",
      "39  0.396419  \n",
      "38  0.396799  \n",
      "41  0.396367  \n",
      "40  0.396719  \n",
      "43  0.396367  \n",
      "42  0.396759  \n",
      "45  0.396261  \n",
      "44  0.396719  \n",
      "47  0.396367  \n",
      "46  0.396759  \n",
      "13  0.569405  \n",
      "12  0.565706  \n",
      "15  0.586467  \n",
      "14  0.581232  \n",
      "17  0.575303  \n",
      "16  0.580512  \n",
      "19  0.589152  \n",
      "18  0.590276  \n",
      "21  0.569826  \n",
      "20  0.577191  \n",
      "23  0.589521  \n",
      "22  0.591717  \n",
      "1   0.396209  \n",
      "0   0.396799  \n",
      "3   0.473986  \n",
      "2   0.480112  \n",
      "5   0.507320  \n",
      "4   0.511285  \n",
      "7   0.525856  \n",
      "6   0.527971  \n",
      "9   0.489573  \n",
      "8   0.499720  \n",
      "11  0.533649  \n",
      "10  0.537815  \n",
      "25  0.000000  \n",
      "24  0.000000  \n",
      "27  0.000000  \n",
      "26  0.000000  \n",
      "29  0.000000  \n",
      "28  0.000000  \n",
      "31  0.000000  \n",
      "30  0.000000  \n",
      "33  0.000000  \n",
      "32  0.000000  \n",
      "35  0.000000  \n",
      "34  0.000000  \n",
      "dataset: second, optimizer: gradient_descent, max steps: 10000\n",
      "dataset: second, optimizer: gradient_descent, max steps: 20000\n",
      "dataset: second, optimizer: gradient_descent, max steps: 40000\n",
      "dataset: second, optimizer: gradient_descent, max steps: 60000\n",
      "dataset: second, optimizer: gradient_descent, max steps: 80000\n",
      "dataset: second, optimizer: gradient_descent, max steps: 100000\n",
      "dataset: second, optimizer: adam, max steps: 10000\n",
      "dataset: second, optimizer: adam, max steps: 20000\n",
      "dataset: second, optimizer: adam, max steps: 40000\n",
      "dataset: second, optimizer: adam, max steps: 60000\n",
      "dataset: second, optimizer: adam, max steps: 80000\n",
      "dataset: second, optimizer: adam, max steps: 100000\n",
      "dataset: second, optimizer: momentum, max steps: 10000\n",
      "dataset: second, optimizer: momentum, max steps: 20000\n",
      "dataset: second, optimizer: momentum, max steps: 40000\n",
      "dataset: second, optimizer: momentum, max steps: 60000\n",
      "dataset: second, optimizer: momentum, max steps: 80000\n",
      "dataset: second, optimizer: momentum, max steps: 100000\n",
      "dataset: second, optimizer: adadelta, max steps: 10000\n",
      "dataset: second, optimizer: adadelta, max steps: 20000\n",
      "dataset: second, optimizer: adadelta, max steps: 40000\n",
      "dataset: second, optimizer: adadelta, max steps: 60000\n",
      "dataset: second, optimizer: adadelta, max steps: 80000\n",
      "dataset: second, optimizer: adadelta, max steps: 100000\n",
      "\n",
      "   dataset         optimizer  max_steps datatype  num_examples  num_correct  \\\n",
      "37  second          adadelta      10000     test         18990         8042   \n",
      "36  second          adadelta      10000    train         24990        10514   \n",
      "39  second          adadelta      20000     test         18990         8042   \n",
      "38  second          adadelta      20000    train         24990        10514   \n",
      "41  second          adadelta      40000     test         18990         8045   \n",
      "40  second          adadelta      40000    train         24990        10514   \n",
      "43  second          adadelta      60000     test         18990         8042   \n",
      "42  second          adadelta      60000    train         24990        10514   \n",
      "45  second          adadelta      80000     test         18990         8043   \n",
      "44  second          adadelta      80000    train         24990        10514   \n",
      "47  second          adadelta     100000     test         18990         8042   \n",
      "46  second          adadelta     100000    train         24990        10514   \n",
      "13  second              adam      10000     test         18990        12004   \n",
      "12  second              adam      10000    train         24990        16004   \n",
      "15  second              adam      20000     test         18990        12144   \n",
      "14  second              adam      20000    train         24990        16121   \n",
      "17  second              adam      40000     test         18990        12122   \n",
      "16  second              adam      40000    train         24990        16176   \n",
      "19  second              adam      60000     test         18990        11901   \n",
      "18  second              adam      60000    train         24990        15829   \n",
      "21  second              adam      80000     test         18990        11958   \n",
      "20  second              adam      80000    train         24990        15955   \n",
      "23  second              adam     100000     test         18990         8043   \n",
      "22  second              adam     100000    train         24990        10514   \n",
      "1   second  gradient_descent      10000     test         18990         8042   \n",
      "0   second  gradient_descent      10000    train         24990        10514   \n",
      "3   second  gradient_descent      20000     test         18990         9977   \n",
      "2   second  gradient_descent      20000    train         24990        13142   \n",
      "5   second  gradient_descent      40000     test         18990         9231   \n",
      "4   second  gradient_descent      40000    train         24990        12237   \n",
      "7   second  gradient_descent      60000     test         18990         9530   \n",
      "6   second  gradient_descent      60000    train         24990        12407   \n",
      "9   second  gradient_descent      80000     test         18990         8437   \n",
      "8   second  gradient_descent      80000    train         24990        11020   \n",
      "11  second  gradient_descent     100000     test         18990        10845   \n",
      "10  second  gradient_descent     100000    train         24990        14275   \n",
      "25  second          momentum      10000     test         18990            0   \n",
      "24  second          momentum      10000    train         24990            0   \n",
      "27  second          momentum      20000     test         18990            0   \n",
      "26  second          momentum      20000    train         24990            0   \n",
      "29  second          momentum      40000     test         18990            0   \n",
      "28  second          momentum      40000    train         24990            0   \n",
      "31  second          momentum      60000     test         18990            0   \n",
      "30  second          momentum      60000    train         24990            0   \n",
      "33  second          momentum      80000     test         18990            0   \n",
      "32  second          momentum      80000    train         24990            0   \n",
      "35  second          momentum     100000     test         18990            0   \n",
      "34  second          momentum     100000    train         24990            0   \n",
      "\n",
      "    accuracy  \n",
      "37  0.423486  \n",
      "36  0.420728  \n",
      "39  0.423486  \n",
      "38  0.420728  \n",
      "41  0.423644  \n",
      "40  0.420728  \n",
      "43  0.423486  \n",
      "42  0.420728  \n",
      "45  0.423539  \n",
      "44  0.420728  \n",
      "47  0.423486  \n",
      "46  0.420728  \n",
      "13  0.632122  \n",
      "12  0.640416  \n",
      "15  0.639494  \n",
      "14  0.645098  \n",
      "17  0.638336  \n",
      "16  0.647299  \n",
      "19  0.626698  \n",
      "18  0.633413  \n",
      "21  0.629700  \n",
      "20  0.638455  \n",
      "23  0.423539  \n",
      "22  0.420728  \n",
      "1   0.423486  \n",
      "0   0.420728  \n",
      "3   0.525382  \n",
      "2   0.525890  \n",
      "5   0.486098  \n",
      "4   0.489676  \n",
      "7   0.501843  \n",
      "6   0.496479  \n",
      "9   0.444286  \n",
      "8   0.440976  \n",
      "11  0.571090  \n",
      "10  0.571228  \n",
      "25  0.000000  \n",
      "24  0.000000  \n",
      "27  0.000000  \n",
      "26  0.000000  \n",
      "29  0.000000  \n",
      "28  0.000000  \n",
      "31  0.000000  \n",
      "30  0.000000  \n",
      "33  0.000000  \n",
      "32  0.000000  \n",
      "35  0.000000  \n",
      "34  0.000000  \n",
      "dataset: third, optimizer: gradient_descent, max steps: 10000\n",
      "dataset: third, optimizer: gradient_descent, max steps: 20000\n",
      "dataset: third, optimizer: gradient_descent, max steps: 40000\n",
      "dataset: third, optimizer: gradient_descent, max steps: 60000\n",
      "dataset: third, optimizer: gradient_descent, max steps: 80000\n",
      "dataset: third, optimizer: gradient_descent, max steps: 100000\n",
      "dataset: third, optimizer: adam, max steps: 10000\n",
      "dataset: third, optimizer: adam, max steps: 20000\n",
      "dataset: third, optimizer: adam, max steps: 40000\n",
      "dataset: third, optimizer: adam, max steps: 60000\n",
      "dataset: third, optimizer: adam, max steps: 80000\n",
      "dataset: third, optimizer: adam, max steps: 100000\n",
      "dataset: third, optimizer: momentum, max steps: 10000\n",
      "dataset: third, optimizer: momentum, max steps: 20000\n",
      "dataset: third, optimizer: momentum, max steps: 40000\n",
      "dataset: third, optimizer: momentum, max steps: 60000\n",
      "dataset: third, optimizer: momentum, max steps: 80000\n",
      "dataset: third, optimizer: momentum, max steps: 100000\n",
      "dataset: third, optimizer: adadelta, max steps: 10000\n",
      "dataset: third, optimizer: adadelta, max steps: 20000\n",
      "dataset: third, optimizer: adadelta, max steps: 40000\n",
      "dataset: third, optimizer: adadelta, max steps: 60000\n",
      "dataset: third, optimizer: adadelta, max steps: 80000\n",
      "dataset: third, optimizer: adadelta, max steps: 100000\n",
      "\n",
      "   dataset         optimizer  max_steps datatype  num_examples  num_correct  \\\n",
      "37   third          adadelta      10000     test         26580        13671   \n",
      "36   third          adadelta      10000    train         33420        16831   \n",
      "39   third          adadelta      20000     test         26580        13672   \n",
      "38   third          adadelta      20000    train         33420        16831   \n",
      "41   third          adadelta      40000     test         26580        13673   \n",
      "40   third          adadelta      40000    train         33420        16832   \n",
      "43   third          adadelta      60000     test         26580        13672   \n",
      "42   third          adadelta      60000    train         33420        16832   \n",
      "45   third          adadelta      80000     test         26580        13671   \n",
      "44   third          adadelta      80000    train         33420        16831   \n",
      "47   third          adadelta     100000     test         26580        13671   \n",
      "46   third          adadelta     100000    train         33420        16833   \n",
      "13   third              adam      10000     test         26580        15863   \n",
      "12   third              adam      10000    train         33420        19737   \n",
      "15   third              adam      20000     test         26580        15775   \n",
      "14   third              adam      20000    train         33420        19715   \n",
      "17   third              adam      40000     test         26580        16139   \n",
      "16   third              adam      40000    train         33420        20250   \n",
      "19   third              adam      60000     test         26580        15673   \n",
      "18   third              adam      60000    train         33420        19690   \n",
      "21   third              adam      80000     test         26580        16336   \n",
      "20   third              adam      80000    train         33420        20592   \n",
      "23   third              adam     100000     test         26580        16557   \n",
      "22   third              adam     100000    train         33420        20845   \n",
      "1    third  gradient_descent      10000     test         26580        13674   \n",
      "0    third  gradient_descent      10000    train         33420        16833   \n",
      "3    third  gradient_descent      20000     test         26580        13673   \n",
      "2    third  gradient_descent      20000    train         33420        16833   \n",
      "5    third  gradient_descent      40000     test         26580        14395   \n",
      "4    third  gradient_descent      40000    train         33420        18097   \n",
      "7    third  gradient_descent      60000     test         26580        14398   \n",
      "6    third  gradient_descent      60000    train         33420        17934   \n",
      "9    third  gradient_descent      80000     test         26580        14762   \n",
      "8    third  gradient_descent      80000    train         33420        18573   \n",
      "11   third  gradient_descent     100000     test         26580        14691   \n",
      "10   third  gradient_descent     100000    train         33420        18397   \n",
      "25   third          momentum      10000     test         26580            0   \n",
      "24   third          momentum      10000    train         33420            0   \n",
      "27   third          momentum      20000     test         26580            0   \n",
      "26   third          momentum      20000    train         33420            0   \n",
      "29   third          momentum      40000     test         26580            0   \n",
      "28   third          momentum      40000    train         33420            0   \n",
      "31   third          momentum      60000     test         26580            0   \n",
      "30   third          momentum      60000    train         33420            0   \n",
      "33   third          momentum      80000     test         26580            0   \n",
      "32   third          momentum      80000    train         33420            0   \n",
      "35   third          momentum     100000     test         26580            0   \n",
      "34   third          momentum     100000    train         33420            0   \n",
      "\n",
      "    accuracy  \n",
      "37  0.514334  \n",
      "36  0.503621  \n",
      "39  0.514372  \n",
      "38  0.503621  \n",
      "41  0.514409  \n",
      "40  0.503651  \n",
      "43  0.514372  \n",
      "42  0.503651  \n",
      "45  0.514334  \n",
      "44  0.503621  \n",
      "47  0.514334  \n",
      "46  0.503680  \n",
      "13  0.596802  \n",
      "12  0.590575  \n",
      "15  0.593491  \n",
      "14  0.589916  \n",
      "17  0.607186  \n",
      "16  0.605925  \n",
      "19  0.589654  \n",
      "18  0.589168  \n",
      "21  0.614597  \n",
      "20  0.616158  \n",
      "23  0.622912  \n",
      "22  0.623728  \n",
      "1   0.514447  \n",
      "0   0.503680  \n",
      "3   0.514409  \n",
      "2   0.503680  \n",
      "5   0.541573  \n",
      "4   0.541502  \n",
      "7   0.541685  \n",
      "6   0.536625  \n",
      "9   0.555380  \n",
      "8   0.555745  \n",
      "11  0.552709  \n",
      "10  0.550479  \n",
      "25  0.000000  \n",
      "24  0.000000  \n",
      "27  0.000000  \n",
      "26  0.000000  \n",
      "29  0.000000  \n",
      "28  0.000000  \n",
      "31  0.000000  \n",
      "30  0.000000  \n",
      "33  0.000000  \n",
      "32  0.000000  \n",
      "35  0.000000  \n",
      "34  0.000000  \n",
      "dataset: fourth, optimizer: gradient_descent, max steps: 10000\n",
      "dataset: fourth, optimizer: gradient_descent, max steps: 20000\n",
      "dataset: fourth, optimizer: gradient_descent, max steps: 40000\n",
      "dataset: fourth, optimizer: gradient_descent, max steps: 60000\n",
      "dataset: fourth, optimizer: gradient_descent, max steps: 80000\n",
      "dataset: fourth, optimizer: gradient_descent, max steps: 100000\n",
      "dataset: fourth, optimizer: adam, max steps: 10000\n",
      "dataset: fourth, optimizer: adam, max steps: 20000\n",
      "dataset: fourth, optimizer: adam, max steps: 40000\n",
      "dataset: fourth, optimizer: adam, max steps: 60000\n",
      "dataset: fourth, optimizer: adam, max steps: 80000\n",
      "dataset: fourth, optimizer: adam, max steps: 100000\n",
      "dataset: fourth, optimizer: momentum, max steps: 10000\n",
      "dataset: fourth, optimizer: momentum, max steps: 20000\n",
      "dataset: fourth, optimizer: momentum, max steps: 40000\n",
      "dataset: fourth, optimizer: momentum, max steps: 60000\n",
      "dataset: fourth, optimizer: momentum, max steps: 80000\n",
      "dataset: fourth, optimizer: momentum, max steps: 100000\n",
      "dataset: fourth, optimizer: adadelta, max steps: 10000\n",
      "dataset: fourth, optimizer: adadelta, max steps: 20000\n",
      "dataset: fourth, optimizer: adadelta, max steps: 40000\n",
      "dataset: fourth, optimizer: adadelta, max steps: 60000\n",
      "dataset: fourth, optimizer: adadelta, max steps: 80000\n",
      "dataset: fourth, optimizer: adadelta, max steps: 100000\n",
      "\n",
      "   dataset         optimizer  max_steps datatype  num_examples  num_correct  \\\n",
      "37  fourth          adadelta      10000     test         26740         9730   \n",
      "36  fourth          adadelta      10000    train         32400        11487   \n",
      "39  fourth          adadelta      20000     test         26740         9731   \n",
      "38  fourth          adadelta      20000    train         32400        11488   \n",
      "41  fourth          adadelta      40000     test         26740         9730   \n",
      "40  fourth          adadelta      40000    train         32400        11488   \n",
      "43  fourth          adadelta      60000     test         26740         9731   \n",
      "42  fourth          adadelta      60000    train         32400        11488   \n",
      "45  fourth          adadelta      80000     test         26740         9730   \n",
      "44  fourth          adadelta      80000    train         32400        11487   \n",
      "47  fourth          adadelta     100000     test         26740         9731   \n",
      "46  fourth          adadelta     100000    train         32400        11487   \n",
      "13  fourth              adam      10000     test         26740         9731   \n",
      "12  fourth              adam      10000    train         32400        11488   \n",
      "15  fourth              adam      20000     test         26740         9730   \n",
      "14  fourth              adam      20000    train         32400        11484   \n",
      "17  fourth              adam      40000     test         26740         9730   \n",
      "16  fourth              adam      40000    train         32400        11486   \n",
      "19  fourth              adam      60000     test         26740         9731   \n",
      "18  fourth              adam      60000    train         32400        11487   \n",
      "21  fourth              adam      80000     test         26740         9731   \n",
      "20  fourth              adam      80000    train         32400        11489   \n",
      "23  fourth              adam     100000     test         26740         9730   \n",
      "22  fourth              adam     100000    train         32400        11490   \n",
      "1   fourth  gradient_descent      10000     test         26740         9731   \n",
      "0   fourth  gradient_descent      10000    train         32400        11488   \n",
      "3   fourth  gradient_descent      20000     test         26740         9772   \n",
      "2   fourth  gradient_descent      20000    train         32400        11547   \n",
      "5   fourth  gradient_descent      40000     test         26740         9731   \n",
      "4   fourth  gradient_descent      40000    train         32400        11489   \n",
      "7   fourth  gradient_descent      60000     test         26740         9273   \n",
      "6   fourth  gradient_descent      60000    train         32400        11267   \n",
      "9   fourth  gradient_descent      80000     test         26740        10709   \n",
      "8   fourth  gradient_descent      80000    train         32400        12789   \n",
      "11  fourth  gradient_descent     100000     test         26740        10702   \n",
      "10  fourth  gradient_descent     100000    train         32400        12896   \n",
      "25  fourth          momentum      10000     test         26740            0   \n",
      "24  fourth          momentum      10000    train         32400            0   \n",
      "27  fourth          momentum      20000     test         26740            0   \n",
      "26  fourth          momentum      20000    train         32400            0   \n",
      "29  fourth          momentum      40000     test         26740            0   \n",
      "28  fourth          momentum      40000    train         32400            0   \n",
      "31  fourth          momentum      60000     test         26740            0   \n",
      "30  fourth          momentum      60000    train         32400            0   \n",
      "33  fourth          momentum      80000     test         26740            0   \n",
      "32  fourth          momentum      80000    train         32400            0   \n",
      "35  fourth          momentum     100000     test         26740            0   \n",
      "34  fourth          momentum     100000    train         32400            0   \n",
      "\n",
      "    accuracy  \n",
      "37  0.363874  \n",
      "36  0.354537  \n",
      "39  0.363912  \n",
      "38  0.354568  \n",
      "41  0.363874  \n",
      "40  0.354568  \n",
      "43  0.363912  \n",
      "42  0.354568  \n",
      "45  0.363874  \n",
      "44  0.354537  \n",
      "47  0.363912  \n",
      "46  0.354537  \n",
      "13  0.363912  \n",
      "12  0.354568  \n",
      "15  0.363874  \n",
      "14  0.354444  \n",
      "17  0.363874  \n",
      "16  0.354506  \n",
      "19  0.363912  \n",
      "18  0.354537  \n",
      "21  0.363912  \n",
      "20  0.354599  \n",
      "23  0.363874  \n",
      "22  0.354630  \n",
      "1   0.363912  \n",
      "0   0.354568  \n",
      "3   0.365445  \n",
      "2   0.356389  \n",
      "5   0.363912  \n",
      "4   0.354599  \n",
      "7   0.346784  \n",
      "6   0.347747  \n",
      "9   0.400486  \n",
      "8   0.394722  \n",
      "11  0.400224  \n",
      "10  0.398025  \n",
      "25  0.000000  \n",
      "24  0.000000  \n",
      "27  0.000000  \n",
      "26  0.000000  \n",
      "29  0.000000  \n",
      "28  0.000000  \n",
      "31  0.000000  \n",
      "30  0.000000  \n",
      "33  0.000000  \n",
      "32  0.000000  \n",
      "35  0.000000  \n",
      "34  0.000000  \n",
      "dataset: fifth, optimizer: gradient_descent, max steps: 10000\n",
      "dataset: fifth, optimizer: gradient_descent, max steps: 20000\n",
      "dataset: fifth, optimizer: gradient_descent, max steps: 40000\n",
      "dataset: fifth, optimizer: gradient_descent, max steps: 60000\n",
      "dataset: fifth, optimizer: gradient_descent, max steps: 80000\n",
      "dataset: fifth, optimizer: gradient_descent, max steps: 100000\n",
      "dataset: fifth, optimizer: adam, max steps: 10000\n",
      "dataset: fifth, optimizer: adam, max steps: 20000\n",
      "dataset: fifth, optimizer: adam, max steps: 40000\n",
      "dataset: fifth, optimizer: adam, max steps: 60000\n",
      "dataset: fifth, optimizer: adam, max steps: 80000\n",
      "dataset: fifth, optimizer: adam, max steps: 100000\n",
      "dataset: fifth, optimizer: momentum, max steps: 10000\n",
      "dataset: fifth, optimizer: momentum, max steps: 20000\n",
      "dataset: fifth, optimizer: momentum, max steps: 40000\n",
      "dataset: fifth, optimizer: momentum, max steps: 60000\n",
      "dataset: fifth, optimizer: momentum, max steps: 80000\n",
      "dataset: fifth, optimizer: momentum, max steps: 100000\n",
      "dataset: fifth, optimizer: adadelta, max steps: 10000\n",
      "dataset: fifth, optimizer: adadelta, max steps: 20000\n",
      "dataset: fifth, optimizer: adadelta, max steps: 40000\n",
      "dataset: fifth, optimizer: adadelta, max steps: 60000\n",
      "dataset: fifth, optimizer: adadelta, max steps: 80000\n",
      "dataset: fifth, optimizer: adadelta, max steps: 100000\n",
      "\n",
      "   dataset         optimizer  max_steps datatype  num_examples  num_correct  \\\n",
      "37   fifth          adadelta      10000     test          2750         1395   \n",
      "36   fifth          adadelta      10000    train          4000         2086   \n",
      "39   fifth          adadelta      20000     test          2750         1395   \n",
      "38   fifth          adadelta      20000    train          4000         2086   \n",
      "41   fifth          adadelta      40000     test          2750         1395   \n",
      "40   fifth          adadelta      40000    train          4000         2086   \n",
      "43   fifth          adadelta      60000     test          2750         1395   \n",
      "42   fifth          adadelta      60000    train          4000         2086   \n",
      "45   fifth          adadelta      80000     test          2750         1395   \n",
      "44   fifth          adadelta      80000    train          4000         2086   \n",
      "47   fifth          adadelta     100000     test          2750         1395   \n",
      "46   fifth          adadelta     100000    train          4000         2086   \n",
      "13   fifth              adam      10000     test          2750         1877   \n",
      "12   fifth              adam      10000    train          4000         2726   \n",
      "15   fifth              adam      20000     test          2750         1902   \n",
      "14   fifth              adam      20000    train          4000         2758   \n",
      "17   fifth              adam      40000     test          2750         1870   \n",
      "16   fifth              adam      40000    train          4000         2714   \n",
      "19   fifth              adam      60000     test          2750         1922   \n",
      "18   fifth              adam      60000    train          4000         2787   \n",
      "21   fifth              adam      80000     test          2750         1926   \n",
      "20   fifth              adam      80000    train          4000         2791   \n",
      "23   fifth              adam     100000     test          2750         1886   \n",
      "22   fifth              adam     100000    train          4000         2737   \n",
      "1    fifth  gradient_descent      10000     test          2750         1395   \n",
      "0    fifth  gradient_descent      10000    train          4000         2086   \n",
      "3    fifth  gradient_descent      20000     test          2750         1395   \n",
      "2    fifth  gradient_descent      20000    train          4000         2086   \n",
      "5    fifth  gradient_descent      40000     test          2750         1651   \n",
      "4    fifth  gradient_descent      40000    train          4000         2332   \n",
      "7    fifth  gradient_descent      60000     test          2750         1722   \n",
      "6    fifth  gradient_descent      60000    train          4000         2479   \n",
      "9    fifth  gradient_descent      80000     test          2750         1760   \n",
      "8    fifth  gradient_descent      80000    train          4000         2522   \n",
      "11   fifth  gradient_descent     100000     test          2750         1744   \n",
      "10   fifth  gradient_descent     100000    train          4000         2541   \n",
      "25   fifth          momentum      10000     test          2750            0   \n",
      "24   fifth          momentum      10000    train          4000            0   \n",
      "27   fifth          momentum      20000     test          2750            0   \n",
      "26   fifth          momentum      20000    train          4000            0   \n",
      "29   fifth          momentum      40000     test          2750            0   \n",
      "28   fifth          momentum      40000    train          4000            0   \n",
      "31   fifth          momentum      60000     test          2750            0   \n",
      "30   fifth          momentum      60000    train          4000            0   \n",
      "33   fifth          momentum      80000     test          2750            0   \n",
      "32   fifth          momentum      80000    train          4000            0   \n",
      "35   fifth          momentum     100000     test          2750            0   \n",
      "34   fifth          momentum     100000    train          4000            0   \n",
      "\n",
      "    accuracy  \n",
      "37  0.507273  \n",
      "36  0.521500  \n",
      "39  0.507273  \n",
      "38  0.521500  \n",
      "41  0.507273  \n",
      "40  0.521500  \n",
      "43  0.507273  \n",
      "42  0.521500  \n",
      "45  0.507273  \n",
      "44  0.521500  \n",
      "47  0.507273  \n",
      "46  0.521500  \n",
      "13  0.682545  \n",
      "12  0.681500  \n",
      "15  0.691636  \n",
      "14  0.689500  \n",
      "17  0.680000  \n",
      "16  0.678500  \n",
      "19  0.698909  \n",
      "18  0.696750  \n",
      "21  0.700364  \n",
      "20  0.697750  \n",
      "23  0.685818  \n",
      "22  0.684250  \n",
      "1   0.507273  \n",
      "0   0.521500  \n",
      "3   0.507273  \n",
      "2   0.521500  \n",
      "5   0.600364  \n",
      "4   0.583000  \n",
      "7   0.626182  \n",
      "6   0.619750  \n",
      "9   0.640000  \n",
      "8   0.630500  \n",
      "11  0.634182  \n",
      "10  0.635250  \n",
      "25  0.000000  \n",
      "24  0.000000  \n",
      "27  0.000000  \n",
      "26  0.000000  \n",
      "29  0.000000  \n",
      "28  0.000000  \n",
      "31  0.000000  \n",
      "30  0.000000  \n",
      "33  0.000000  \n",
      "32  0.000000  \n",
      "35  0.000000  \n",
      "34  0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Choose \"micro\" or \"macro\". This will change the types of features we're\n",
    "# using. There are 220 \"micro\" features in total while thre are 9 macro\n",
    "# features.\n",
    "dataset_type = \"macro\"\n",
    "show_data=False\n",
    "random_sampler = True\n",
    "\n",
    "if dataset_type == \"macro\":\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 8\n",
    "    hidden2 = 8\n",
    "    hidden3 = None\n",
    "    NUM_FEATURES = 9\n",
    "    batch_size = 10\n",
    "else:\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 512\n",
    "    hidden2 = 128\n",
    "    hidden3 = 16\n",
    "    NUM_FEATURES = 220\n",
    "    batch_size = 200\n",
    "\n",
    "for dataset in ['first', 'second', 'third', 'fourth', 'fifth']:\n",
    "    all_evaluation_result = []\n",
    "    if dataset =='third' or dataset == 'fourth':\n",
    "        NUM_CLASSES = 5\n",
    "    else:\n",
    "        NUM_CLASSES = 6\n",
    "    \n",
    "    # Read in data\n",
    "    (train_ids, train_features, train_labels,\n",
    "     test_ids, test_features, test_labels,\n",
    "     dev_ids, dev_features, dev_labels) = read_data(dataset, macro_or_micro=dataset_type,\n",
    "                                                    dev_set=False)\n",
    "    #random_sampler = False\n",
    "    train_data = DataSet(train_ids, train_features, train_labels)\n",
    "    test_data = DataSet(test_ids, test_features, test_labels)\n",
    "    if dev_labels is not None:\n",
    "        dev_data = DataSet(dev_ids, dev_features, dev_labels)\n",
    "    \n",
    "    if show_data:\n",
    "        print(\"dataset: {}\".format(dataset))\n",
    "        print(\"Shape of data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "              .format(train_features.shape,\n",
    "                      \"\" if dev_features is None\n",
    "                         else \"Development: {}\".format(dev_features.shape),\n",
    "                      test_features.shape))\n",
    "\n",
    "        print(\"train features\\n\")\n",
    "        print(train_features.head())\n",
    "        \n",
    "        print(\"test features\\n\")\n",
    "        print(test_features.head())\n",
    "        print(\"train labels\\n\")\n",
    "        print(train_labels[:10])\n",
    "    \n",
    "    # run evaluation\n",
    "    for optimizer_type in ['gradient_descent', 'adam', 'momentum', 'adadelta']:\n",
    "        for max_steps in [10000, 20000, 40000, 60000, 80000, 100000]:\n",
    "            print(\"dataset: {}, optimizer: {}, max steps: {}\".format(dataset, \n",
    "                                                                     optimizer_type, \n",
    "                                                                     max_steps))\n",
    "            train_data_eval_dict, test_data_eval_dict = run_graph_for_evaluation(train_data, \n",
    "                                                                                 test_data, \n",
    "                                                                                 optimizer_type, \n",
    "                                                                                 max_steps)\n",
    "            train_data_eval_dict['dataset'] = dataset\n",
    "            test_data_eval_dict['dataset'] = dataset\n",
    "            all_evaluation_result.extend([train_data_eval_dict, \n",
    "                                          test_data_eval_dict])\n",
    "    print(\"\")\n",
    "\n",
    "    df = pd.DataFrame(all_evaluation_result)\n",
    "    df = df[['dataset', 'optimizer', 'max_steps', 'datatype', \n",
    "             'num_examples', 'num_correct', \n",
    "             'accuracy']].sort_values(by=['dataset', 'optimizer', \n",
    "                                          'max_steps', 'datatype'])\n",
    "    df.to_csv(\"{}_evaluation_results.csv\".format(dataset), index=False)\n",
    "    print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
