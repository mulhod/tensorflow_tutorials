{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Mechanics 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes Chong Min made\n",
    "\n",
    "* Optimizer: from Gradient Descent to Adam\n",
    "* batch size: set to 10\n",
    "* size of hidden layer: 8\n",
    "* weight initializer: random_normal_initializer\n",
    "* max steps: 20000\n",
    "\n",
    "Among the above changes, `batch size`, `optimizer`, and `max steps` values were critical.\n",
    "\n",
    "Because batches were randomly generated, the accuracies were flucturated. It should be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tutorial is meant as a companion to the code [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist/)\n",
    "- The goal of this tutorial is to show how to use TensorFlow to train and evaluate a simple feed-forward neural network for handwritten digit classification using the (classic) MNIST data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`mnist.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py), the code for making a fully-connected MNIST model\n",
    "- [`fully_connected_feed.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py), the main code to train the built MNIST model against the downloaded dataset using a feed dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom e-rater Data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download test_data_revised.zip (in email since it can't be shared) and\n",
    "# save it somewhere, preferably in the directory in which this notebook is\n",
    "# stored. If it is somewhere else, just make sure to pass in the path when\n",
    "# this function is used.\n",
    "\n",
    "def read_praxis_data(macro_or_micro=\"macro\",\n",
    "                     dev_set=True,\n",
    "                     data_path=join(getcwd(), \"test_data_revised\")):\n",
    "    \"\"\"\n",
    "    Read in data from a directory with the following files:\n",
    "    testing_macro.csv and training_macro.csv.\n",
    "\n",
    "    Either read in the \"macro\" files or the \"micro\" files.\n",
    "\n",
    "    Returns 1) training IDs, features, and labels,\n",
    "            2) test IDs, features, and labels, and\n",
    "            3) development set IDs, features, and labels (if `dev_set` is\n",
    "               False, the test test set will contain all data originally in\n",
    "               the \"testing\" file).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data_path = join(data_path,\n",
    "                           \"training_{}.csv\"\n",
    "                           .format(macro_or_micro if macro_or_micro == \"macro\"\n",
    "                                   else \"micro_revised\"))\n",
    "    train_data = pd.read_csv(train_data_path, dtype={'appointment_id': str})\n",
    "    train_labels = train_data['H1'].apply(lambda x: x - 1)\n",
    "    train_features = train_data[[a for a in train_data.columns\n",
    "                                 if a not in ['appointment_id', 'H1']]]\n",
    "    train_ids = train_data['appointment_id']\n",
    "\n",
    "    test_data_path = join(data_path,\n",
    "                          \"testing_{}.csv\"\n",
    "                          .format(macro_or_micro if macro_or_micro == \"macro\"\n",
    "                                  else \"micro_revised\"))\n",
    "    test_data = pd.read_csv(test_data_path, dtype={'appointment_id': str})\n",
    "     \n",
    "    test_labels = test_data['H1'].apply(lambda x: x - 1)\n",
    "    test_features = test_data[[a for a in test_data.columns\n",
    "                               if a not in ['appointment_id', 'H1']]]\n",
    "    test_ids = test_data['appointment_id']\n",
    "\n",
    "    dev_features = None\n",
    "    dev_labels = None\n",
    "    dev_ids = None\n",
    "    if dev_set:\n",
    "        test_amount = len(test_features) - 1000\n",
    "        dev_features = test_features.head(1000)\n",
    "        dev_features.index = range(len(dev_features))\n",
    "        test_features = test_features.tail(test_amount)\n",
    "        test_features.index = range(len(test_features))\n",
    "        dev_labels = test_labels.head(1000)\n",
    "        dev_labels.index = range(len(dev_labels))\n",
    "        test_labels = test_labels.tail(test_amount)\n",
    "        test_labels.index = range(len(test_labels))\n",
    "        dev_ids = test_ids[:1000]\n",
    "        test_ids = test_ids[1000:]\n",
    "\n",
    "    return (train_ids,\n",
    "            train_features,\n",
    "            train_labels,\n",
    "            test_ids,\n",
    "            test_features,\n",
    "            test_labels,\n",
    "            dev_ids,\n",
    "            dev_features,\n",
    "            dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose \"micro\" or \"macro\". This will change the types of features we're\n",
    "# using. There are 220 \"micro\" features in total while thre are 9 macro\n",
    "# features.\n",
    "dataset_type = \"macro\"\n",
    "# dataset_type = \"micro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "(train_ids, train_features, train_labels,\n",
    " test_ids, test_features, test_labels,\n",
    " dev_ids, dev_features, dev_labels) = read_praxis_data(macro_or_micro=dataset_type,\n",
    "                                                       dev_set=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"Shape of data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "#       .format(train_features.shape,\n",
    "#               \"\" if dev_features is None\n",
    "#                  else \"Development: {}\\n\\t\".format(dev_features.shape),\n",
    "#               test_features.shape))\n",
    "# print(\"Shape of labels data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "#       .format(train_labels.shape,\n",
    "#               \"\" if dev_labels is None\n",
    "#                  else \"Development: {}\\n\\t\".format(dev_labels.shape),\n",
    "#               test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What the features look like\n",
    "# train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Labels are on a 0 to 5 scale (scores 1 to 6)\n",
    "# train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_labels = np.array(train_labels, dtype=np.float32)\n",
    "# test_labels = np.array(test_labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if dev_labels is not None:\n",
    "#     print(dev_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n",
    "max_steps = 20000\n",
    "if dataset_type == \"macro\":\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 8  # 128\n",
    "    hidden2 = 8  # 32\n",
    "    hidden3 = 8  # 16\n",
    "    NUM_FEATURES = 9\n",
    "    batch_size = 10\n",
    "else:\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 512\n",
    "    hidden2 = 128\n",
    "    hidden3 = 16\n",
    "    NUM_FEATURES = 220\n",
    "    batch_size = 200\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Based on `mnist.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(inputs, num_features, num_classes, hidden1_units,\n",
    "              hidden2_units, hidden3_units):\n",
    "    \"\"\"\n",
    "    Build a model on the inputs up to where it may be used for\n",
    "    inference.\n",
    "\n",
    "    Args:\n",
    "        inputs: Placeholder for input data samples.\n",
    "        num_features: Number of features in input data.\n",
    "        num_classes: Number of classes/score labels.\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "        hidden2_units: Size of the second hidden layer.\n",
    "        hidden3_units: Size of the third hidden layer.\n",
    "\n",
    "    Returns:\n",
    "        softmax_linear: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([num_features, hidden1_units]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(inputs, weights) + biases)\n",
    "\n",
    "#     # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([hidden1_units, hidden2_units]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "#                                 stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "#                              name='biases')\n",
    "#         hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "#     # Hidden 3\n",
    "#     with tf.name_scope('hidden3'):\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([hidden2_units, hidden3_units],\n",
    "#                                 stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([hidden3_units]),\n",
    "#                              name='biases')\n",
    "#         hidden3 = tf.nn.relu(tf.matmul(hidden2, weights) + biases)\n",
    "\n",
    "#     # Linear\n",
    "#     with tf.name_scope('softmax_linear'):\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([hidden3_units, num_classes],\n",
    "#                                 stddev=1.0 / math.sqrt(float(hidden3_units))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([num_classes]),\n",
    "#                              name='biases')\n",
    "#         logits = tf.matmul(hidden3, weights) + biases\n",
    "        \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        \n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([hidden1_units, num_classes]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([num_classes]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([hidden3_units, num_classes],\n",
    "#                                 stddev=1.0 / math.sqrt(float(hidden3_units))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([num_classes]),\n",
    "#                              name='biases')\n",
    "#         logits = tf.matmul(hidden3, weights) + biases\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Calculates the loss from the logits and the labels.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size].\n",
    "\n",
    "    Returns:\n",
    "        loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = tf.to_int64(labels)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=labels, logits=logits, name='xentropy')\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')    \n",
    "\n",
    "#     cross_entropy = \\\n",
    "#         tf.nn.softmax_cross_entropy_with_logits(labels=labels,\n",
    "#                                                        logits=logits,\n",
    "#                                                        name='xentropy')\n",
    "\n",
    "#     return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "#     labels = tf.cast(labels, tf.float32)\n",
    "#     labels = tf.to_float32(labels)\n",
    "#     return tf.nn.l2_loss(tf.squared_difference(tf.argmax(logits, 1), labels))  # tf.reduce_mean(tf.square(tf.sub(labels, tf.argmax(logits, 1))))\n",
    "\n",
    "\n",
    "def training(_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Sets up the training Ops.\n",
    "    \n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates an optimizer and applies the gradients to all trainable\n",
    "    variables.\n",
    "    \n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "\n",
    "    Args:\n",
    "        _loss: Loss tensor, from loss().\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', _loss)\n",
    "\n",
    "    # Create the gradient descent optimizer with the given learning\n",
    "    # rate.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single\n",
    "    # training step.\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "                range [0, NUM_CLASSES).\n",
    "\n",
    "    Returns:\n",
    "        A scalar int32 tensor with the number of examples (out of\n",
    "        batch_size) that were predicted correctly.\n",
    "    \"\"\"\n",
    "\n",
    "    # For a classifier model, we can use the in_top_k Op.\n",
    "    # It returns a bool tensor with shape [batch_size] that is true for\n",
    "    # the examples where the label is in the top k (here k=1)\n",
    "    # of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "#     Return the number of true entries.\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "\n",
    "#     return tf.sqrt(tf.reduce_mean(tf.square(tf.sub(logits, labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From fully_connected_feed.py\n",
    "def fill_feed_dict(features, labels, inputs_pl, labels_pl, batch_size):\n",
    "    \"\"\"\n",
    "    Fills the feed_dict for training the given step.\n",
    "\n",
    "    A feed_dict takes the form of:\n",
    "    feed_dict = {\n",
    "        <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "        ....\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        data_set: The set of features and labels.\n",
    "        inputs_pl: The input data placeholder.\n",
    "        labels_pl: The input labels placeholder.\n",
    "        batch_size: Size of each randomly-selected batch.\n",
    "\n",
    "    Returns:\n",
    "        feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the feed_dict for the placeholders filled with the next\n",
    "    # `batch_size` samples.\n",
    "    sample = random.sample(range(len(labels)), k=batch_size)\n",
    "    inputs_feed = features.ix[sample]\n",
    "    labels_feed = labels.ix[sample]\n",
    "\n",
    "    feed_dict = {\n",
    "        inputs_pl: inputs_feed,\n",
    "        labels_pl: labels_feed,\n",
    "    }\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def do_eval(sess, eval_correct, inputs_placeholder, labels_placeholder,\n",
    "            features, labels):\n",
    "    \"\"\"\n",
    "    Runs one evaluation against the full epoch of data.\n",
    "\n",
    "    Args:\n",
    "        sess: The session in which the model has been trained.\n",
    "        eval_correct: The Tensor that returns the number of correct\n",
    "                      predictions.\n",
    "        inputs_placeholder: The input data placeholder.\n",
    "        labels_placeholder: The labels placeholder.\n",
    "        data_set: The set of images and labels to evaluate, from\n",
    "                  input_data.read_data_sets().\n",
    "    \"\"\"\n",
    "\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = features.shape[0] // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(features, labels,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "        logit_output, true_cnt = sess.run([logits, eval_correct],\n",
    "                                          feed_dict=feed_dict)\n",
    "        true_count += true_cnt\n",
    "\n",
    "    precision = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "          (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.80 (0.009 sec)\n",
      "Step 100: loss = 1.22 (0.001 sec)\n",
      "Step 200: loss = 1.13 (0.001 sec)\n",
      "Step 300: loss = 1.37 (0.001 sec)\n",
      "Step 400: loss = 1.10 (0.001 sec)\n",
      "Step 500: loss = 1.28 (0.001 sec)\n",
      "Step 600: loss = 0.90 (0.001 sec)\n",
      "Step 700: loss = 1.08 (0.002 sec)\n",
      "Step 800: loss = 0.93 (0.002 sec)\n",
      "Step 900: loss = 1.14 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2058  Precision @ 1: 0.5145\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1462  Precision @ 1: 0.5316\n",
      "Step 1000: loss = 1.37 (0.002 sec)\n",
      "Step 1100: loss = 1.27 (0.002 sec)\n",
      "Step 1200: loss = 0.87 (0.001 sec)\n",
      "Step 1300: loss = 0.96 (0.001 sec)\n",
      "Step 1400: loss = 0.65 (0.001 sec)\n",
      "Step 1500: loss = 1.11 (0.002 sec)\n",
      "Step 1600: loss = 0.86 (0.001 sec)\n",
      "Step 1700: loss = 1.13 (0.002 sec)\n",
      "Step 1800: loss = 1.34 (0.001 sec)\n",
      "Step 1900: loss = 1.01 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2538  Precision @ 1: 0.6345\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1706  Precision @ 1: 0.6204\n",
      "Step 2000: loss = 0.80 (0.002 sec)\n",
      "Step 2100: loss = 0.73 (0.001 sec)\n",
      "Step 2200: loss = 1.43 (0.001 sec)\n",
      "Step 2300: loss = 0.64 (0.001 sec)\n",
      "Step 2400: loss = 0.78 (0.001 sec)\n",
      "Step 2500: loss = 1.45 (0.002 sec)\n",
      "Step 2600: loss = 1.10 (0.001 sec)\n",
      "Step 2700: loss = 0.72 (0.002 sec)\n",
      "Step 2800: loss = 0.70 (0.001 sec)\n",
      "Step 2900: loss = 0.52 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2645  Precision @ 1: 0.6613\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1759  Precision @ 1: 0.6396\n",
      "Step 3000: loss = 0.86 (0.002 sec)\n",
      "Step 3100: loss = 0.85 (0.001 sec)\n",
      "Step 3200: loss = 0.95 (0.004 sec)\n",
      "Step 3300: loss = 0.56 (0.001 sec)\n",
      "Step 3400: loss = 0.76 (0.002 sec)\n",
      "Step 3500: loss = 0.69 (0.001 sec)\n",
      "Step 3600: loss = 0.82 (0.001 sec)\n",
      "Step 3700: loss = 0.84 (0.001 sec)\n",
      "Step 3800: loss = 0.58 (0.002 sec)\n",
      "Step 3900: loss = 1.09 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2670  Precision @ 1: 0.6675\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1858  Precision @ 1: 0.6756\n",
      "Step 4000: loss = 0.63 (0.002 sec)\n",
      "Step 4100: loss = 1.03 (0.001 sec)\n",
      "Step 4200: loss = 0.98 (0.001 sec)\n",
      "Step 4300: loss = 0.87 (0.001 sec)\n",
      "Step 4400: loss = 0.57 (0.001 sec)\n",
      "Step 4500: loss = 2.00 (0.001 sec)\n",
      "Step 4600: loss = 0.67 (0.002 sec)\n",
      "Step 4700: loss = 1.03 (0.001 sec)\n",
      "Step 4800: loss = 0.58 (0.001 sec)\n",
      "Step 4900: loss = 0.66 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2301  Precision @ 1: 0.5753\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1615  Precision @ 1: 0.5873\n",
      "Step 5000: loss = 1.52 (0.002 sec)\n",
      "Step 5100: loss = 0.77 (0.001 sec)\n",
      "Step 5200: loss = 0.68 (0.001 sec)\n",
      "Step 5300: loss = 0.55 (0.001 sec)\n",
      "Step 5400: loss = 0.55 (0.001 sec)\n",
      "Step 5500: loss = 0.78 (0.001 sec)\n",
      "Step 5600: loss = 1.05 (0.002 sec)\n",
      "Step 5700: loss = 0.93 (0.001 sec)\n",
      "Step 5800: loss = 0.77 (0.001 sec)\n",
      "Step 5900: loss = 0.67 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2754  Precision @ 1: 0.6885\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1865  Precision @ 1: 0.6782\n",
      "Step 6000: loss = 0.87 (0.002 sec)\n",
      "Step 6100: loss = 0.55 (0.002 sec)\n",
      "Step 6200: loss = 0.82 (0.001 sec)\n",
      "Step 6300: loss = 0.79 (0.001 sec)\n",
      "Step 6400: loss = 1.80 (0.001 sec)\n",
      "Step 6500: loss = 0.64 (0.001 sec)\n",
      "Step 6600: loss = 0.59 (0.002 sec)\n",
      "Step 6700: loss = 0.47 (0.001 sec)\n",
      "Step 6800: loss = 0.90 (0.002 sec)\n",
      "Step 6900: loss = 0.81 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2640  Precision @ 1: 0.6600\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1881  Precision @ 1: 0.6840\n",
      "Step 7000: loss = 0.67 (0.002 sec)\n",
      "Step 7100: loss = 0.85 (0.001 sec)\n",
      "Step 7200: loss = 1.12 (0.002 sec)\n",
      "Step 7300: loss = 0.80 (0.002 sec)\n",
      "Step 7400: loss = 0.63 (0.004 sec)\n",
      "Step 7500: loss = 0.59 (0.001 sec)\n",
      "Step 7600: loss = 0.70 (0.001 sec)\n",
      "Step 7700: loss = 0.60 (0.001 sec)\n",
      "Step 7800: loss = 0.75 (0.001 sec)\n",
      "Step 7900: loss = 1.12 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2487  Precision @ 1: 0.6218\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1781  Precision @ 1: 0.6476\n",
      "Step 8000: loss = 0.62 (0.002 sec)\n",
      "Step 8100: loss = 0.98 (0.001 sec)\n",
      "Step 8200: loss = 0.91 (0.001 sec)\n",
      "Step 8300: loss = 0.41 (0.001 sec)\n",
      "Step 8400: loss = 1.05 (0.001 sec)\n",
      "Step 8500: loss = 0.79 (0.001 sec)\n",
      "Step 8600: loss = 0.51 (0.002 sec)\n",
      "Step 8700: loss = 0.97 (0.003 sec)\n",
      "Step 8800: loss = 0.97 (0.003 sec)\n",
      "Step 8900: loss = 1.09 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2629  Precision @ 1: 0.6573\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1791  Precision @ 1: 0.6513\n",
      "Step 9000: loss = 0.83 (0.002 sec)\n",
      "Step 9100: loss = 0.84 (0.001 sec)\n",
      "Step 9200: loss = 0.41 (0.001 sec)\n",
      "Step 9300: loss = 0.52 (0.002 sec)\n",
      "Step 9400: loss = 1.05 (0.001 sec)\n",
      "Step 9500: loss = 0.76 (0.001 sec)\n",
      "Step 9600: loss = 0.80 (0.001 sec)\n",
      "Step 9700: loss = 0.58 (0.002 sec)\n",
      "Step 9800: loss = 0.84 (0.003 sec)\n",
      "Step 9900: loss = 0.57 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2791  Precision @ 1: 0.6977\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1920  Precision @ 1: 0.6982\n",
      "Step 10000: loss = 0.54 (0.002 sec)\n",
      "Step 10100: loss = 0.73 (0.001 sec)\n",
      "Step 10200: loss = 0.52 (0.001 sec)\n",
      "Step 10300: loss = 1.55 (0.002 sec)\n",
      "Step 10400: loss = 0.83 (0.001 sec)\n",
      "Step 10500: loss = 0.56 (0.002 sec)\n",
      "Step 10600: loss = 0.74 (0.002 sec)\n",
      "Step 10700: loss = 1.07 (0.001 sec)\n",
      "Step 10800: loss = 1.11 (0.001 sec)\n",
      "Step 10900: loss = 0.79 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2758  Precision @ 1: 0.6895\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1954  Precision @ 1: 0.7105\n",
      "Step 11000: loss = 1.05 (0.002 sec)\n",
      "Step 11100: loss = 0.76 (0.003 sec)\n",
      "Step 11200: loss = 1.02 (0.001 sec)\n",
      "Step 11300: loss = 0.59 (0.002 sec)\n",
      "Step 11400: loss = 0.74 (0.001 sec)\n",
      "Step 11500: loss = 0.54 (0.002 sec)\n",
      "Step 11600: loss = 0.71 (0.002 sec)\n",
      "Step 11700: loss = 0.60 (0.003 sec)\n",
      "Step 11800: loss = 0.42 (0.002 sec)\n",
      "Step 11900: loss = 0.64 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2770  Precision @ 1: 0.6925\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1880  Precision @ 1: 0.6836\n",
      "Step 12000: loss = 1.06 (0.002 sec)\n",
      "Step 12100: loss = 0.53 (0.003 sec)\n",
      "Step 12200: loss = 1.10 (0.002 sec)\n",
      "Step 12300: loss = 0.44 (0.002 sec)\n",
      "Step 12400: loss = 0.49 (0.002 sec)\n",
      "Step 12500: loss = 1.04 (0.001 sec)\n",
      "Step 12600: loss = 0.27 (0.002 sec)\n",
      "Step 12700: loss = 0.50 (0.002 sec)\n",
      "Step 12800: loss = 0.93 (0.002 sec)\n",
      "Step 12900: loss = 0.78 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2806  Precision @ 1: 0.7015\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1941  Precision @ 1: 0.7058\n",
      "Step 13000: loss = 0.59 (0.002 sec)\n",
      "Step 13100: loss = 0.60 (0.002 sec)\n",
      "Step 13200: loss = 0.54 (0.002 sec)\n",
      "Step 13300: loss = 1.14 (0.002 sec)\n",
      "Step 13400: loss = 0.80 (0.002 sec)\n",
      "Step 13500: loss = 0.96 (0.002 sec)\n",
      "Step 13600: loss = 0.65 (0.002 sec)\n",
      "Step 13700: loss = 0.67 (0.003 sec)\n",
      "Step 13800: loss = 0.32 (0.002 sec)\n",
      "Step 13900: loss = 0.72 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2775  Precision @ 1: 0.6937\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1912  Precision @ 1: 0.6953\n",
      "Step 14000: loss = 0.65 (0.003 sec)\n",
      "Step 14100: loss = 0.94 (0.003 sec)\n",
      "Step 14200: loss = 0.61 (0.002 sec)\n",
      "Step 14300: loss = 0.66 (0.002 sec)\n",
      "Step 14400: loss = 0.77 (0.002 sec)\n",
      "Step 14500: loss = 0.75 (0.001 sec)\n",
      "Step 14600: loss = 0.99 (0.003 sec)\n",
      "Step 14700: loss = 0.67 (0.003 sec)\n",
      "Step 14800: loss = 0.75 (0.002 sec)\n",
      "Step 14900: loss = 0.65 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2787  Precision @ 1: 0.6967\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1888  Precision @ 1: 0.6865\n",
      "Step 15000: loss = 0.90 (0.002 sec)\n",
      "Step 15100: loss = 0.45 (0.002 sec)\n",
      "Step 15200: loss = 0.70 (0.002 sec)\n",
      "Step 15300: loss = 0.82 (0.001 sec)\n",
      "Step 15400: loss = 0.48 (0.002 sec)\n",
      "Step 15500: loss = 0.72 (0.002 sec)\n",
      "Step 15600: loss = 1.21 (0.002 sec)\n",
      "Step 15700: loss = 0.62 (0.002 sec)\n",
      "Step 15800: loss = 0.59 (0.003 sec)\n",
      "Step 15900: loss = 0.95 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2843  Precision @ 1: 0.7107\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1882  Precision @ 1: 0.6844\n",
      "Step 16000: loss = 0.68 (0.002 sec)\n",
      "Step 16100: loss = 0.70 (0.002 sec)\n",
      "Step 16200: loss = 1.04 (0.001 sec)\n",
      "Step 16300: loss = 0.80 (0.002 sec)\n",
      "Step 16400: loss = 0.88 (0.001 sec)\n",
      "Step 16500: loss = 0.85 (0.001 sec)\n",
      "Step 16600: loss = 0.38 (0.001 sec)\n",
      "Step 16700: loss = 0.43 (0.002 sec)\n",
      "Step 16800: loss = 0.57 (0.003 sec)\n",
      "Step 16900: loss = 0.55 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2699  Precision @ 1: 0.6747\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1927  Precision @ 1: 0.7007\n",
      "Step 17000: loss = 0.48 (0.003 sec)\n",
      "Step 17100: loss = 0.89 (0.002 sec)\n",
      "Step 17200: loss = 0.60 (0.002 sec)\n",
      "Step 17300: loss = 0.52 (0.002 sec)\n",
      "Step 17400: loss = 0.78 (0.002 sec)\n",
      "Step 17500: loss = 0.79 (0.002 sec)\n",
      "Step 17600: loss = 0.48 (0.002 sec)\n",
      "Step 17700: loss = 0.58 (0.002 sec)\n",
      "Step 17800: loss = 0.68 (0.002 sec)\n",
      "Step 17900: loss = 0.42 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2762  Precision @ 1: 0.6905\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1919  Precision @ 1: 0.6978\n",
      "Step 18000: loss = 0.42 (0.002 sec)\n",
      "Step 18100: loss = 0.88 (0.002 sec)\n",
      "Step 18200: loss = 0.68 (0.002 sec)\n",
      "Step 18300: loss = 0.55 (0.001 sec)\n",
      "Step 18400: loss = 0.86 (0.002 sec)\n",
      "Step 18500: loss = 1.00 (0.001 sec)\n",
      "Step 18600: loss = 1.83 (0.002 sec)\n",
      "Step 18700: loss = 1.29 (0.002 sec)\n",
      "Step 18800: loss = 0.87 (0.001 sec)\n",
      "Step 18900: loss = 0.78 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2739  Precision @ 1: 0.6847\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1932  Precision @ 1: 0.7025\n",
      "Step 19000: loss = 0.53 (0.002 sec)\n",
      "Step 19100: loss = 1.01 (0.004 sec)\n",
      "Step 19200: loss = 0.74 (0.002 sec)\n",
      "Step 19300: loss = 0.88 (0.002 sec)\n",
      "Step 19400: loss = 0.97 (0.003 sec)\n",
      "Step 19500: loss = 0.62 (0.001 sec)\n",
      "Step 19600: loss = 0.82 (0.004 sec)\n",
      "Step 19700: loss = 0.72 (0.004 sec)\n",
      "Step 19800: loss = 0.77 (0.002 sec)\n",
      "Step 19900: loss = 0.77 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2772  Precision @ 1: 0.6930\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1933  Precision @ 1: 0.7029\n"
     ]
    }
   ],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Generate placeholders for the input feature data and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           NUM_FEATURES))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(inputs_placeholder,\n",
    "                       NUM_FEATURES,\n",
    "                       NUM_CLASSES,\n",
    "                       hidden1,\n",
    "                       hidden2,\n",
    "                       hidden3)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_ = loss(logits, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op = training(loss_, learning_rate)\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with the actual set of images and labels\n",
    "        # for this particular training step.\n",
    "        feed_dict = fill_feed_dict(train_features, train_labels,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss_],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 100 == 0:\n",
    "\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Train Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    inputs_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    train_features, train_labels)\n",
    "\n",
    "            # Evaluate against the development set.\n",
    "            if dev_labels is not None:\n",
    "                print('Development Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        dev_features,\n",
    "                        dev_labels)\n",
    "\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    inputs_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
