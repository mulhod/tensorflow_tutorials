{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Mechanics 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes Chong Min made\n",
    "\n",
    "* Optimizer: from Gradient Descent to Adam\n",
    "* batch size: set to 10\n",
    "* size of hidden layer: 8\n",
    "* weight initializer: random_normal_initializer\n",
    "* max steps: 20000\n",
    "\n",
    "Among the above changes, `batch size`, `optimizer`, and `max steps` values were critical.\n",
    "\n",
    "Because batches were randomly generated, the accuracies were flucturated. It should be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tutorial is meant as a companion to the code [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist/)\n",
    "- The goal of this tutorial is to show how to use TensorFlow to train and evaluate a simple feed-forward neural network for handwritten digit classification using the (classic) MNIST data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`mnist.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py), the code for making a fully-connected MNIST model\n",
    "- [`fully_connected_feed.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py), the main code to train the built MNIST model against the downloaded dataset using a feed dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from os import getcwd\n",
    "from os.path import join, dirname\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(join(dirname(getcwd()), \"src\"))\n",
    "from utils import (read_data, DataSet, inference, loss, training_adam,\n",
    "                   training_gradient_descent, evaluation, fill_feed_dict,\n",
    "                   do_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = join(dirname(getcwd()), \"data\", \"test_data_revised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom e-rater Data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "# NOTE: Download test_data_revised.zip (in email since it can't be shared)\n",
    "# and save it somewhere, preferably in the \"tutorial_notebooks/data\"\n",
    "# directory. If it is somewhere else, just make sure to pass in the path when\n",
    "# this function is used.\n",
    "\n",
    "# Choose \"micro\" or \"macro\". This will change the types of features we're\n",
    "# using. There are 220 \"micro\" features in total while thre are 9 macro\n",
    "# features.\n",
    "dataset_type = \"macro\"\n",
    "# dataset_type = \"micro\"\n",
    "\n",
    "(train_ids, train_features, train_labels,\n",
    " test_ids, test_features, test_labels,\n",
    " dev_ids, dev_features, dev_labels) = read_data(data_path,\n",
    "                                                macro_or_micro=dataset_type,\n",
    "                                                dev_set=False)\n",
    "#random_sampler = False\n",
    "random_sampler = True\n",
    "train_data = DataSet(train_ids, train_features, train_labels, random_=random_sampler)\n",
    "test_data = DataSet(test_ids, test_features, test_labels)\n",
    "if dev_labels is not None:\n",
    "    dev_data = DataSet(dev_ids, dev_features, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_data = False\n",
    "#show_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    print(\"Shape of data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "          .format(train_features.shape,\n",
    "                  \"\" if dev_features is None\n",
    "                     else \"Development: {}\\n\\t\".format(dev_features.shape),\n",
    "                  test_features.shape))\n",
    "    print(\"Shape of labels data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "          .format(train_labels.shape,\n",
    "                  \"\" if dev_labels is None\n",
    "                     else \"Development: {}\\n\\t\".format(dev_labels.shape),\n",
    "                  test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What the features look like\n",
    "if show_data:\n",
    "    train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Labels are on a 0 to 5 scale (scores 1 to 6)\n",
    "if show_data:\n",
    "    train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    train_labels = np.array(train_labels, dtype=np.float32)\n",
    "    test_labels = np.array(test_labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    if dev_labels is not None:\n",
    "        print(dev_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n",
    "max_steps = 20000\n",
    "optimizer_type = \"adam\"\n",
    "#optimizer_type = \"gradient descent\"\n",
    "if dataset_type == \"macro\":\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 8\n",
    "    hidden2 = 8\n",
    "    hidden3 = None\n",
    "    NUM_FEATURES = 9\n",
    "    batch_size = 10\n",
    "else:\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 512\n",
    "    hidden2 = 128\n",
    "    hidden3 = 16\n",
    "    NUM_FEATURES = 220\n",
    "    batch_size = 200\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.79 (0.005 sec)\n",
      "Step 100: loss = 1.62 (0.001 sec)\n",
      "Step 200: loss = 0.90 (0.001 sec)\n",
      "Step 300: loss = 0.87 (0.001 sec)\n",
      "Step 400: loss = 1.54 (0.001 sec)\n",
      "Step 500: loss = 1.61 (0.001 sec)\n",
      "Step 600: loss = 0.88 (0.001 sec)\n",
      "Step 700: loss = 0.86 (0.001 sec)\n",
      "Step 800: loss = 1.51 (0.001 sec)\n",
      "Step 900: loss = 1.59 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 1000: loss = 0.89 (0.002 sec)\n",
      "Step 1100: loss = 0.85 (0.002 sec)\n",
      "Step 1200: loss = 1.42 (0.002 sec)\n",
      "Step 1300: loss = 1.45 (0.001 sec)\n",
      "Step 1400: loss = 0.85 (0.001 sec)\n",
      "Step 1500: loss = 0.64 (0.001 sec)\n",
      "Step 1600: loss = 1.21 (0.002 sec)\n",
      "Step 1700: loss = 1.25 (0.001 sec)\n",
      "Step 1800: loss = 0.67 (0.001 sec)\n",
      "Step 1900: loss = 0.53 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2437  Accuracy @ 1: 0.6092\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1694  Accuracy @ 1: 0.6160\n",
      "Step 2000: loss = 1.16 (0.004 sec)\n",
      "Step 2100: loss = 1.06 (0.001 sec)\n",
      "Step 2200: loss = 0.59 (0.001 sec)\n",
      "Step 2300: loss = 0.43 (0.001 sec)\n",
      "Step 2400: loss = 1.05 (0.001 sec)\n",
      "Step 2500: loss = 0.95 (0.001 sec)\n",
      "Step 2600: loss = 0.51 (0.001 sec)\n",
      "Step 2700: loss = 0.43 (0.001 sec)\n",
      "Step 2800: loss = 0.99 (0.002 sec)\n",
      "Step 2900: loss = 0.87 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2376  Accuracy @ 1: 0.5940\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1628  Accuracy @ 1: 0.5920\n",
      "Step 3000: loss = 0.48 (0.001 sec)\n",
      "Step 3100: loss = 0.41 (0.001 sec)\n",
      "Step 3200: loss = 0.96 (0.002 sec)\n",
      "Step 3300: loss = 0.90 (0.002 sec)\n",
      "Step 3400: loss = 0.44 (0.001 sec)\n",
      "Step 3500: loss = 0.38 (0.001 sec)\n",
      "Step 3600: loss = 0.96 (0.002 sec)\n",
      "Step 3700: loss = 0.88 (0.002 sec)\n",
      "Step 3800: loss = 0.42 (0.002 sec)\n",
      "Step 3900: loss = 0.39 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2605  Accuracy @ 1: 0.6512\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1786  Accuracy @ 1: 0.6495\n",
      "Step 4000: loss = 0.95 (0.003 sec)\n",
      "Step 4100: loss = 0.88 (0.001 sec)\n",
      "Step 4200: loss = 0.40 (0.001 sec)\n",
      "Step 4300: loss = 0.40 (0.001 sec)\n",
      "Step 4400: loss = 0.94 (0.001 sec)\n",
      "Step 4500: loss = 0.89 (0.001 sec)\n",
      "Step 4600: loss = 0.39 (0.001 sec)\n",
      "Step 4700: loss = 0.41 (0.001 sec)\n",
      "Step 4800: loss = 0.93 (0.001 sec)\n",
      "Step 4900: loss = 0.88 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2511  Accuracy @ 1: 0.6278\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1719  Accuracy @ 1: 0.6251\n",
      "Step 5000: loss = 0.38 (0.002 sec)\n",
      "Step 5100: loss = 0.41 (0.001 sec)\n",
      "Step 5200: loss = 0.90 (0.001 sec)\n",
      "Step 5300: loss = 0.89 (0.001 sec)\n",
      "Step 5400: loss = 0.38 (0.001 sec)\n",
      "Step 5500: loss = 0.41 (0.002 sec)\n",
      "Step 5600: loss = 0.90 (0.001 sec)\n",
      "Step 5700: loss = 0.89 (0.001 sec)\n",
      "Step 5800: loss = 0.38 (0.001 sec)\n",
      "Step 5900: loss = 0.42 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2682  Accuracy @ 1: 0.6705\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1861  Accuracy @ 1: 0.6767\n",
      "Step 6000: loss = 0.89 (0.002 sec)\n",
      "Step 6100: loss = 0.89 (0.001 sec)\n",
      "Step 6200: loss = 0.38 (0.002 sec)\n",
      "Step 6300: loss = 0.43 (0.001 sec)\n",
      "Step 6400: loss = 0.88 (0.002 sec)\n",
      "Step 6500: loss = 0.88 (0.001 sec)\n",
      "Step 6600: loss = 0.37 (0.001 sec)\n",
      "Step 6700: loss = 0.43 (0.001 sec)\n",
      "Step 6800: loss = 0.84 (0.002 sec)\n",
      "Step 6900: loss = 0.89 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2588  Accuracy @ 1: 0.6470\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1762  Accuracy @ 1: 0.6407\n",
      "Step 7000: loss = 0.37 (0.002 sec)\n",
      "Step 7100: loss = 0.44 (0.001 sec)\n",
      "Step 7200: loss = 0.89 (0.001 sec)\n",
      "Step 7300: loss = 0.88 (0.001 sec)\n",
      "Step 7400: loss = 0.38 (0.001 sec)\n",
      "Step 7500: loss = 0.44 (0.002 sec)\n",
      "Step 7600: loss = 0.85 (0.002 sec)\n",
      "Step 7700: loss = 0.88 (0.001 sec)\n",
      "Step 7800: loss = 0.37 (0.001 sec)\n",
      "Step 7900: loss = 0.44 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2710  Accuracy @ 1: 0.6775\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1861  Accuracy @ 1: 0.6767\n",
      "Step 8000: loss = 0.84 (0.003 sec)\n",
      "Step 8100: loss = 0.89 (0.001 sec)\n",
      "Step 8200: loss = 0.37 (0.001 sec)\n",
      "Step 8300: loss = 0.45 (0.002 sec)\n",
      "Step 8400: loss = 0.83 (0.002 sec)\n",
      "Step 8500: loss = 0.90 (0.001 sec)\n",
      "Step 8600: loss = 0.37 (0.005 sec)\n",
      "Step 8700: loss = 0.45 (0.001 sec)\n",
      "Step 8800: loss = 0.84 (0.002 sec)\n",
      "Step 8900: loss = 0.92 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2639  Accuracy @ 1: 0.6597\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1807  Accuracy @ 1: 0.6571\n",
      "Step 9000: loss = 0.38 (0.001 sec)\n",
      "Step 9100: loss = 0.45 (0.001 sec)\n",
      "Step 9200: loss = 0.86 (0.002 sec)\n",
      "Step 9300: loss = 0.91 (0.001 sec)\n",
      "Step 9400: loss = 0.39 (0.001 sec)\n",
      "Step 9500: loss = 0.45 (0.002 sec)\n",
      "Step 9600: loss = 0.86 (0.002 sec)\n",
      "Step 9700: loss = 0.93 (0.001 sec)\n",
      "Step 9800: loss = 0.39 (0.001 sec)\n",
      "Step 9900: loss = 0.46 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2667  Accuracy @ 1: 0.6667\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1844  Accuracy @ 1: 0.6705\n",
      "Step 10000: loss = 0.89 (0.002 sec)\n",
      "Step 10100: loss = 0.94 (0.001 sec)\n",
      "Step 10200: loss = 0.40 (0.001 sec)\n",
      "Step 10300: loss = 0.45 (0.001 sec)\n",
      "Step 10400: loss = 0.85 (0.002 sec)\n",
      "Step 10500: loss = 0.92 (0.001 sec)\n",
      "Step 10600: loss = 0.41 (0.001 sec)\n",
      "Step 10700: loss = 0.47 (0.001 sec)\n",
      "Step 10800: loss = 0.85 (0.001 sec)\n",
      "Step 10900: loss = 0.92 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2703  Accuracy @ 1: 0.6757\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1864  Accuracy @ 1: 0.6778\n",
      "Step 11000: loss = 0.41 (0.001 sec)\n",
      "Step 11100: loss = 0.46 (0.001 sec)\n",
      "Step 11200: loss = 0.87 (0.001 sec)\n",
      "Step 11300: loss = 0.90 (0.001 sec)\n",
      "Step 11400: loss = 0.41 (0.001 sec)\n",
      "Step 11500: loss = 0.47 (0.001 sec)\n",
      "Step 11600: loss = 0.87 (0.001 sec)\n",
      "Step 11700: loss = 0.92 (0.001 sec)\n",
      "Step 11800: loss = 0.41 (0.001 sec)\n",
      "Step 11900: loss = 0.47 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2676  Accuracy @ 1: 0.6690\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1846  Accuracy @ 1: 0.6713\n",
      "Step 12000: loss = 0.88 (0.002 sec)\n",
      "Step 12100: loss = 0.92 (0.001 sec)\n",
      "Step 12200: loss = 0.42 (0.002 sec)\n",
      "Step 12300: loss = 0.46 (0.001 sec)\n",
      "Step 12400: loss = 0.90 (0.003 sec)\n",
      "Step 12500: loss = 0.94 (0.001 sec)\n",
      "Step 12600: loss = 0.43 (0.002 sec)\n",
      "Step 12700: loss = 0.47 (0.001 sec)\n",
      "Step 12800: loss = 0.91 (0.001 sec)\n",
      "Step 12900: loss = 0.93 (0.001 sec)\n"
     ]
    }
   ],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Generate placeholders for the input feature data and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           NUM_FEATURES))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(inputs_placeholder,\n",
    "                       NUM_FEATURES,\n",
    "                       NUM_CLASSES,\n",
    "                       hidden1,\n",
    "                       hidden2,\n",
    "                       hidden3_units=hidden3)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_ = loss(logits, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    if optimizer_type == \"adam\":\n",
    "        train_op = training_adam(loss_, learning_rate)\n",
    "    elif optimizer_type == \"gradient descent\":\n",
    "        train_op = training_gradient_descent(loss_, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Choose either \\\"adam\\\" or \\\"gradient descent\\\" for \"\n",
    "                         \"`optimizer_type`.\")\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with the actual set of images and labels\n",
    "        # for this particular training step.\n",
    "        feed_dict = fill_feed_dict(train_data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss_],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 100 == 0:\n",
    "\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Train Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    inputs_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    train_data,\n",
    "                    logits,\n",
    "                    batch_size)\n",
    "\n",
    "            # Evaluate against the development set.\n",
    "            if dev_labels is not None:\n",
    "                print('Development Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        dev_data,\n",
    "                        logits,\n",
    "                        batch_size)\n",
    "\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    inputs_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    test_data,\n",
    "                    logits,\n",
    "                    batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
