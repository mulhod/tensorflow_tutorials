{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Mechanics 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes Chong Min made\n",
    "\n",
    "* Optimizer: from Gradient Descent to Adam\n",
    "* batch size: set to 10\n",
    "* size of hidden layer: 8\n",
    "* weight initializer: random_normal_initializer\n",
    "* max steps: 20000\n",
    "\n",
    "Among the above changes, `batch size`, `optimizer`, and `max steps` values were critical.\n",
    "\n",
    "Because batches were randomly generated, the accuracies were flucturated. It should be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tutorial is meant as a companion to the code [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist/)\n",
    "- The goal of this tutorial is to show how to use TensorFlow to train and evaluate a simple feed-forward neural network for handwritten digit classification using the (classic) MNIST data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`mnist.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py), the code for making a fully-connected MNIST model\n",
    "- [`fully_connected_feed.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py), the main code to train the built MNIST model against the downloaded dataset using a feed dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom e-rater Data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download test_data_revised.zip (in email since it can't be shared) and\n",
    "# save it somewhere, preferably in the directory in which this notebook is\n",
    "# stored. If it is somewhere else, just make sure to pass in the path when\n",
    "# this function is used.\n",
    "\n",
    "def read_praxis_data(macro_or_micro=\"macro\",\n",
    "                     dev_set=True,\n",
    "                     data_path=join(getcwd(), \"test_data_revised\")):\n",
    "    \"\"\"\n",
    "    Read in data from a directory with the following files:\n",
    "    testing_macro.csv and training_macro.csv.\n",
    "\n",
    "    Either read in the \"macro\" files or the \"micro\" files.\n",
    "\n",
    "    Returns 1) training IDs, features, and labels,\n",
    "            2) test IDs, features, and labels, and\n",
    "            3) development set IDs, features, and labels (if `dev_set` is\n",
    "               False, the test test set will contain all data originally in\n",
    "               the \"testing\" file).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data_path = join(data_path,\n",
    "                           \"training_{}.csv\"\n",
    "                           .format(macro_or_micro if macro_or_micro == \"macro\"\n",
    "                                   else \"micro_revised\"))\n",
    "    train_data = pd.read_csv(train_data_path, dtype={'appointment_id': str})\n",
    "    train_labels = train_data['H1'].apply(lambda x: x - 1)\n",
    "    train_features = train_data[[a for a in train_data.columns\n",
    "                                 if a not in ['appointment_id', 'H1']]]\n",
    "    train_ids = train_data['appointment_id']\n",
    "\n",
    "    test_data_path = join(data_path,\n",
    "                          \"testing_{}.csv\"\n",
    "                          .format(macro_or_micro if macro_or_micro == \"macro\"\n",
    "                                  else \"micro_revised\"))\n",
    "    test_data = pd.read_csv(test_data_path, dtype={'appointment_id': str})\n",
    "     \n",
    "    test_labels = test_data['H1'].apply(lambda x: x - 1)\n",
    "    test_features = test_data[[a for a in test_data.columns\n",
    "                               if a not in ['appointment_id', 'H1']]]\n",
    "    test_ids = test_data['appointment_id']\n",
    "\n",
    "    dev_features = None\n",
    "    dev_labels = None\n",
    "    dev_ids = None\n",
    "    if dev_set:\n",
    "        test_amount = len(test_features) - 1000\n",
    "        dev_features = test_features.head(1000)\n",
    "        dev_features.index = range(len(dev_features))\n",
    "        test_features = test_features.tail(test_amount)\n",
    "        test_features.index = range(len(test_features))\n",
    "        dev_labels = test_labels.head(1000)\n",
    "        dev_labels.index = range(len(dev_labels))\n",
    "        test_labels = test_labels.tail(test_amount)\n",
    "        test_labels.index = range(len(test_labels))\n",
    "        dev_ids = test_ids[:1000]\n",
    "        test_ids = test_ids[1000:]\n",
    "\n",
    "    return (train_ids,\n",
    "            train_features,\n",
    "            train_labels,\n",
    "            test_ids,\n",
    "            test_features,\n",
    "            test_labels,\n",
    "            dev_ids,\n",
    "            dev_features,\n",
    "            dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose \"micro\" or \"macro\". This will change the types of features we're\n",
    "# using. There are 220 \"micro\" features in total while thre are 9 macro\n",
    "# features.\n",
    "dataset_type = \"macro\"\n",
    "# dataset_type = \"micro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "\n",
    "    def __init__(self, ids, features, labels, random_=False):\n",
    "        prng = np.random.RandomState(12345)\n",
    "        self._ids = ids\n",
    "        self._features = features\n",
    "        self._labels = labels\n",
    "        self._index_in_epoch = 0\n",
    "        self._num_examples = len(self._ids)\n",
    "        if random_:\n",
    "            reindex = prng.permutation(self._features.index)\n",
    "            self._features = self._features.reindex(reindex)\n",
    "            self._ids = self._ids.reindex(reindex)\n",
    "            self._labels = self._labels.reindex(reindex)\n",
    "\n",
    "    def get_size(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "\n",
    "        start = self._index_in_epoch\n",
    "\n",
    "        # Go to the next epoch\n",
    "        if start + batch_size > self._num_examples:\n",
    "\n",
    "            # Get the rest examples in this epoch\n",
    "            remaining_examples = self._num_examples - start\n",
    "            features_rest_part = self._features[start:self._num_examples]\n",
    "            labels_rest_part = self._labels[start:self._num_examples]\n",
    "            ids_rest_part = self._ids[start: self._num_examples]\n",
    "\n",
    "\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - remaining_examples\n",
    "            end = self._index_in_epoch\n",
    "            features_new_part = self._features[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            ids_new_part = self._ids[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            return (np.concatenate((ids_rest_part, ids_new_part), axis=0),\n",
    "                    np.concatenate((features_rest_part, features_new_part), axis = 0), \n",
    "                   np.concatenate((labels_rest_part, labels_new_part), axis = 0))\n",
    "\n",
    "        else:\n",
    "    \n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._ids[start:end], self._features[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "(train_ids, train_features, train_labels,\n",
    " test_ids, test_features, test_labels,\n",
    " dev_ids, dev_features, dev_labels) = read_praxis_data(macro_or_micro=dataset_type,\n",
    "                                                       dev_set=False)\n",
    "#random_sampler = False\n",
    "random_sampler = True\n",
    "train_data = DataSet(train_ids, train_features, train_labels, random_=random_sampler)\n",
    "test_data = DataSet(test_ids, test_features, test_labels)\n",
    "if dev_labels is not None:\n",
    "    dev_data = DataSet(dev_ids, dev_features, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_data = False\n",
    "#show_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    print(\"Shape of data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "          .format(train_features.shape,\n",
    "                  \"\" if dev_features is None\n",
    "                     else \"Development: {}\\n\\t\".format(dev_features.shape),\n",
    "                  test_features.shape))\n",
    "    print(\"Shape of labels data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "          .format(train_labels.shape,\n",
    "                  \"\" if dev_labels is None\n",
    "                     else \"Development: {}\\n\\t\".format(dev_labels.shape),\n",
    "                  test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What the features look like\n",
    "if show_data:\n",
    "    train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Labels are on a 0 to 5 scale (scores 1 to 6)\n",
    "if show_data:\n",
    "    train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    train_labels = np.array(train_labels, dtype=np.float32)\n",
    "    test_labels = np.array(test_labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    if dev_labels is not None:\n",
    "        print(dev_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n",
    "max_steps = 20000\n",
    "optimizer_type = \"adam\"\n",
    "#optimizer_type = \"gradient descent\"\n",
    "if dataset_type == \"macro\":\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 8\n",
    "    hidden2 = 8\n",
    "    hidden3 = None\n",
    "    NUM_FEATURES = 9\n",
    "    batch_size = 10\n",
    "else:\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 512\n",
    "    hidden2 = 128\n",
    "    hidden3 = 16\n",
    "    NUM_FEATURES = 220\n",
    "    batch_size = 200\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Based on `mnist.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(inputs, num_features, num_classes, hidden1_units,\n",
    "              hidden2_units, hidden3_units=None):\n",
    "    \"\"\"\n",
    "    Build a model on the inputs up to where it may be used for\n",
    "    inference.\n",
    "\n",
    "    Args:\n",
    "        inputs: Placeholder for input data samples.\n",
    "        num_features: Number of features in input data.\n",
    "        num_classes: Number of classes/score labels.\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "        hidden2_units: Size of the second hidden layer.\n",
    "        hidden3_units: Size of the third hidden layer (None if no\n",
    "                       third layer).\n",
    "\n",
    "    Returns:\n",
    "        softmax_linear: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([num_features, hidden1_units]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(inputs, weights) + biases)\n",
    "\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([hidden1_units, hidden2_units]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    if hidden3_units is not None:\n",
    "\n",
    "        # Hidden 3\n",
    "        with tf.name_scope('hidden3'):\n",
    "            weights = tf.Variable(\n",
    "                tf.random_normal_initializer(0.0, 0.05)([hidden2_units, hidden3_units],\n",
    "                                    stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "                name='weights')\n",
    "            biases = tf.Variable(tf.zeros([hidden3_units]),\n",
    "                                 name='biases')\n",
    "            hidden3 = tf.nn.relu(tf.matmul(hidden2, weights) + biases)\n",
    "        \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        \n",
    "        weights = tf.Variable(\n",
    "              tf.random_normal_initializer(0.0, 0.05)([hidden1_units, num_classes]),\n",
    "              name='weights')\n",
    "        biases = tf.Variable(tf.zeros([num_classes]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2 if hidden3_units is None else hidden3,\n",
    "                           weights) + biases\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Calculates the loss from the logits and the labels.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size].\n",
    "\n",
    "    Returns:\n",
    "        loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = tf.to_int64(labels)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=labels, logits=logits, name='xentropy')\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')    \n",
    "\n",
    "\n",
    "def training_adam(_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Sets up the training Ops.\n",
    "    \n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates an Adam optimizer and applies the gradients to all trainable\n",
    "    variables.\n",
    "    \n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "\n",
    "    Args:\n",
    "        _loss: Loss tensor, from loss().\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', _loss)\n",
    "\n",
    "    # Create the Adam optimizer with the given learning\n",
    "    # rate.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single\n",
    "    # training step.\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def training_gradient_descent(_loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Sets up the training Ops.\n",
    "    \n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates a gradient descent optimizer and applies the gradients to\n",
    "    all trainable variables.\n",
    "    \n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "\n",
    "    Args:\n",
    "        _loss: Loss tensor, from loss().\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', _loss)\n",
    "\n",
    "    # Create the gradient descent optimizer with the given learning\n",
    "    # rate.\n",
    "    tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single\n",
    "    # training step.\n",
    "    train_op = optimizer.minimize(_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "                range [0, NUM_CLASSES).\n",
    "\n",
    "    Returns:\n",
    "        A scalar int32 tensor with the number of examples (out of\n",
    "        batch_size) that were predicted correctly.\n",
    "    \"\"\"\n",
    "\n",
    "    # For a classifier model, we can use the in_top_k Op.\n",
    "    # It returns a bool tensor with shape [batch_size] that is true for\n",
    "    # the examples where the label is in the top k (here k=1)\n",
    "    # of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "    # Return the number of true entries.\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From fully_connected_feed.py\n",
    "def fill_feed_dict(data, inputs_pl, labels_pl, batch_size):\n",
    "    \"\"\"\n",
    "    Fills the feed_dict for training the given step.\n",
    "\n",
    "    A feed_dict takes the form of:\n",
    "    feed_dict = {\n",
    "        <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "        ....\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        data_set: The set of features and labels.\n",
    "        inputs_pl: The input data placeholder.\n",
    "        labels_pl: The input labels placeholder.\n",
    "        batch_size: Size of each batch.\n",
    "\n",
    "    Returns:\n",
    "        feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the feed_dict for the placeholders filled with the next\n",
    "    # `batch_size` samples.\n",
    "    ids, inputs_feed, labels_feed = data.next_batch(batch_size)\n",
    "    feed_dict = {\n",
    "        inputs_pl: inputs_feed,\n",
    "        labels_pl: labels_feed,\n",
    "    }\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def do_eval(sess, eval_correct, inputs_placeholder, labels_placeholder, data):\n",
    "    \"\"\"\n",
    "    Runs one evaluation against the full epoch of data.\n",
    "\n",
    "    Args:\n",
    "        sess: The session in which the model has been trained.\n",
    "        eval_correct: The Tensor that returns the number of correct\n",
    "                      predictions.\n",
    "        inputs_placeholder: The input data placeholder.\n",
    "        labels_placeholder: The labels placeholder.\n",
    "        data_set: The set of images and labels to evaluate, from\n",
    "                  input_data.read_data_sets().\n",
    "    \"\"\"\n",
    "\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data.get_size() // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "        logit_output, true_cnt = sess.run([logits, eval_correct],\n",
    "                                          feed_dict=feed_dict)\n",
    "        true_count += true_cnt\n",
    "\n",
    "    acc = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Accuracy @ 1: %0.04f' %\n",
    "          (num_examples, true_count, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.80 (0.005 sec)\n",
      "Step 100: loss = 1.62 (0.001 sec)\n",
      "Step 200: loss = 0.88 (0.001 sec)\n",
      "Step 300: loss = 0.86 (0.001 sec)\n",
      "Step 400: loss = 1.56 (0.002 sec)\n",
      "Step 500: loss = 1.61 (0.001 sec)\n",
      "Step 600: loss = 0.87 (0.001 sec)\n",
      "Step 700: loss = 0.86 (0.001 sec)\n",
      "Step 800: loss = 1.54 (0.002 sec)\n",
      "Step 900: loss = 1.60 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 1000: loss = 0.86 (0.002 sec)\n",
      "Step 1100: loss = 0.85 (0.001 sec)\n",
      "Step 1200: loss = 1.47 (0.002 sec)\n",
      "Step 1300: loss = 1.47 (0.001 sec)\n",
      "Step 1400: loss = 0.88 (0.001 sec)\n",
      "Step 1500: loss = 0.76 (0.001 sec)\n",
      "Step 1600: loss = 1.28 (0.002 sec)\n",
      "Step 1700: loss = 1.36 (0.001 sec)\n",
      "Step 1800: loss = 0.71 (0.001 sec)\n",
      "Step 1900: loss = 0.56 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2474  Accuracy @ 1: 0.6185\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1701  Accuracy @ 1: 0.6185\n",
      "Step 2000: loss = 1.13 (0.002 sec)\n",
      "Step 2100: loss = 1.21 (0.002 sec)\n",
      "Step 2200: loss = 0.63 (0.003 sec)\n",
      "Step 2300: loss = 0.52 (0.001 sec)\n",
      "Step 2400: loss = 1.10 (0.001 sec)\n",
      "Step 2500: loss = 1.09 (0.001 sec)\n",
      "Step 2600: loss = 0.58 (0.001 sec)\n",
      "Step 2700: loss = 0.48 (0.001 sec)\n",
      "Step 2800: loss = 1.03 (0.003 sec)\n",
      "Step 2900: loss = 1.01 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2449  Accuracy @ 1: 0.6122\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1677  Accuracy @ 1: 0.6098\n",
      "Step 3000: loss = 0.53 (0.001 sec)\n",
      "Step 3100: loss = 0.46 (0.001 sec)\n",
      "Step 3200: loss = 0.94 (0.002 sec)\n",
      "Step 3300: loss = 0.94 (0.001 sec)\n",
      "Step 3400: loss = 0.50 (0.001 sec)\n",
      "Step 3500: loss = 0.44 (0.002 sec)\n",
      "Step 3600: loss = 0.90 (0.001 sec)\n",
      "Step 3700: loss = 0.89 (0.001 sec)\n",
      "Step 3800: loss = 0.49 (0.001 sec)\n",
      "Step 3900: loss = 0.43 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2629  Accuracy @ 1: 0.6573\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1805  Accuracy @ 1: 0.6564\n",
      "Step 4000: loss = 0.87 (0.002 sec)\n",
      "Step 4100: loss = 0.88 (0.001 sec)\n",
      "Step 4200: loss = 0.49 (0.001 sec)\n",
      "Step 4300: loss = 0.42 (0.001 sec)\n",
      "Step 4400: loss = 0.86 (0.003 sec)\n",
      "Step 4500: loss = 0.88 (0.001 sec)\n",
      "Step 4600: loss = 0.50 (0.002 sec)\n",
      "Step 4700: loss = 0.42 (0.002 sec)\n",
      "Step 4800: loss = 0.84 (0.002 sec)\n",
      "Step 4900: loss = 0.87 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2637  Accuracy @ 1: 0.6593\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1805  Accuracy @ 1: 0.6564\n",
      "Step 5000: loss = 0.50 (0.001 sec)\n",
      "Step 5100: loss = 0.42 (0.002 sec)\n",
      "Step 5200: loss = 0.85 (0.001 sec)\n",
      "Step 5300: loss = 0.87 (0.001 sec)\n",
      "Step 5400: loss = 0.50 (0.001 sec)\n",
      "Step 5500: loss = 0.42 (0.002 sec)\n",
      "Step 5600: loss = 0.84 (0.001 sec)\n",
      "Step 5700: loss = 0.87 (0.002 sec)\n",
      "Step 5800: loss = 0.50 (0.002 sec)\n",
      "Step 5900: loss = 0.42 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2664  Accuracy @ 1: 0.6660\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1837  Accuracy @ 1: 0.6680\n",
      "Step 6000: loss = 0.84 (0.001 sec)\n",
      "Step 6100: loss = 0.88 (0.001 sec)\n",
      "Step 6200: loss = 0.52 (0.001 sec)\n",
      "Step 6300: loss = 0.42 (0.001 sec)\n",
      "Step 6400: loss = 0.83 (0.001 sec)\n",
      "Step 6500: loss = 0.88 (0.002 sec)\n",
      "Step 6600: loss = 0.53 (0.001 sec)\n",
      "Step 6700: loss = 0.43 (0.001 sec)\n",
      "Step 6800: loss = 0.83 (0.003 sec)\n",
      "Step 6900: loss = 0.88 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2713  Accuracy @ 1: 0.6783\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1859  Accuracy @ 1: 0.6760\n",
      "Step 7000: loss = 0.55 (0.001 sec)\n",
      "Step 7100: loss = 0.43 (0.002 sec)\n",
      "Step 7200: loss = 0.83 (0.002 sec)\n",
      "Step 7300: loss = 0.87 (0.001 sec)\n",
      "Step 7400: loss = 0.56 (0.001 sec)\n",
      "Step 7500: loss = 0.43 (0.001 sec)\n",
      "Step 7600: loss = 0.83 (0.001 sec)\n",
      "Step 7700: loss = 0.88 (0.001 sec)\n",
      "Step 7800: loss = 0.55 (0.001 sec)\n",
      "Step 7900: loss = 0.44 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2748  Accuracy @ 1: 0.6870\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1879  Accuracy @ 1: 0.6833\n",
      "Step 8000: loss = 0.83 (0.002 sec)\n",
      "Step 8100: loss = 0.88 (0.002 sec)\n",
      "Step 8200: loss = 0.56 (0.002 sec)\n",
      "Step 8300: loss = 0.44 (0.001 sec)\n",
      "Step 8400: loss = 0.83 (0.001 sec)\n",
      "Step 8500: loss = 0.88 (0.001 sec)\n",
      "Step 8600: loss = 0.58 (0.003 sec)\n",
      "Step 8700: loss = 0.45 (0.001 sec)\n",
      "Step 8800: loss = 0.83 (0.003 sec)\n",
      "Step 8900: loss = 0.89 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2724  Accuracy @ 1: 0.6810\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1877  Accuracy @ 1: 0.6825\n",
      "Step 9000: loss = 0.59 (0.001 sec)\n",
      "Step 9100: loss = 0.45 (0.003 sec)\n",
      "Step 9200: loss = 0.83 (0.002 sec)\n",
      "Step 9300: loss = 0.88 (0.001 sec)\n",
      "Step 9400: loss = 0.60 (0.001 sec)\n",
      "Step 9500: loss = 0.46 (0.001 sec)\n",
      "Step 9600: loss = 0.83 (0.003 sec)\n",
      "Step 9700: loss = 0.88 (0.001 sec)\n",
      "Step 9800: loss = 0.60 (0.001 sec)\n",
      "Step 9900: loss = 0.46 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2745  Accuracy @ 1: 0.6863\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1880  Accuracy @ 1: 0.6836\n",
      "Step 10000: loss = 0.84 (0.002 sec)\n",
      "Step 10100: loss = 0.89 (0.001 sec)\n",
      "Step 10200: loss = 0.55 (0.001 sec)\n",
      "Step 10300: loss = 0.46 (0.002 sec)\n",
      "Step 10400: loss = 0.83 (0.002 sec)\n",
      "Step 10500: loss = 0.89 (0.001 sec)\n",
      "Step 10600: loss = 0.57 (0.001 sec)\n",
      "Step 10700: loss = 0.46 (0.001 sec)\n",
      "Step 10800: loss = 0.84 (0.002 sec)\n",
      "Step 10900: loss = 0.89 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2732  Accuracy @ 1: 0.6830\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1873  Accuracy @ 1: 0.6811\n",
      "Step 11000: loss = 0.58 (0.001 sec)\n",
      "Step 11100: loss = 0.46 (0.001 sec)\n",
      "Step 11200: loss = 0.83 (0.001 sec)\n",
      "Step 11300: loss = 0.89 (0.001 sec)\n",
      "Step 11400: loss = 0.52 (0.001 sec)\n",
      "Step 11500: loss = 0.47 (0.001 sec)\n",
      "Step 11600: loss = 0.83 (0.001 sec)\n",
      "Step 11700: loss = 0.89 (0.001 sec)\n",
      "Step 11800: loss = 0.52 (0.001 sec)\n",
      "Step 11900: loss = 0.47 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2745  Accuracy @ 1: 0.6863\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1881  Accuracy @ 1: 0.6840\n",
      "Step 12000: loss = 0.84 (0.001 sec)\n",
      "Step 12100: loss = 0.89 (0.003 sec)\n",
      "Step 12200: loss = 0.52 (0.002 sec)\n",
      "Step 12300: loss = 0.47 (0.001 sec)\n",
      "Step 12400: loss = 0.83 (0.001 sec)\n",
      "Step 12500: loss = 0.88 (0.001 sec)\n",
      "Step 12600: loss = 0.52 (0.001 sec)\n",
      "Step 12700: loss = 0.47 (0.001 sec)\n",
      "Step 12800: loss = 0.83 (0.001 sec)\n",
      "Step 12900: loss = 0.88 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2744  Accuracy @ 1: 0.6860\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1896  Accuracy @ 1: 0.6895\n",
      "Step 13000: loss = 0.52 (0.001 sec)\n",
      "Step 13100: loss = 0.47 (0.001 sec)\n",
      "Step 13200: loss = 0.84 (0.002 sec)\n",
      "Step 13300: loss = 0.88 (0.001 sec)\n",
      "Step 13400: loss = 0.52 (0.001 sec)\n",
      "Step 13500: loss = 0.48 (0.001 sec)\n",
      "Step 13600: loss = 0.84 (0.003 sec)\n",
      "Step 13700: loss = 0.89 (0.001 sec)\n",
      "Step 13800: loss = 0.52 (0.001 sec)\n",
      "Step 13900: loss = 0.48 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2751  Accuracy @ 1: 0.6877\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1886  Accuracy @ 1: 0.6858\n",
      "Step 14000: loss = 0.84 (0.002 sec)\n",
      "Step 14100: loss = 0.89 (0.001 sec)\n",
      "Step 14200: loss = 0.53 (0.001 sec)\n",
      "Step 14300: loss = 0.48 (0.004 sec)\n",
      "Step 14400: loss = 0.84 (0.002 sec)\n",
      "Step 14500: loss = 0.89 (0.001 sec)\n",
      "Step 14600: loss = 0.54 (0.002 sec)\n",
      "Step 14700: loss = 0.48 (0.002 sec)\n",
      "Step 14800: loss = 0.84 (0.002 sec)\n",
      "Step 14900: loss = 0.89 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2751  Accuracy @ 1: 0.6877\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1895  Accuracy @ 1: 0.6891\n",
      "Step 15000: loss = 0.54 (0.001 sec)\n",
      "Step 15100: loss = 0.48 (0.001 sec)\n",
      "Step 15200: loss = 0.84 (0.002 sec)\n",
      "Step 15300: loss = 0.89 (0.001 sec)\n",
      "Step 15400: loss = 0.60 (0.001 sec)\n",
      "Step 15500: loss = 0.48 (0.002 sec)\n",
      "Step 15600: loss = 0.84 (0.001 sec)\n",
      "Step 15700: loss = 0.89 (0.001 sec)\n",
      "Step 15800: loss = 0.60 (0.001 sec)\n",
      "Step 15900: loss = 0.48 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2756  Accuracy @ 1: 0.6890\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1893  Accuracy @ 1: 0.6884\n",
      "Step 16000: loss = 0.84 (0.002 sec)\n",
      "Step 16100: loss = 0.89 (0.001 sec)\n",
      "Step 16200: loss = 0.61 (0.001 sec)\n",
      "Step 16300: loss = 0.48 (0.001 sec)\n",
      "Step 16400: loss = 0.84 (0.001 sec)\n",
      "Step 16500: loss = 0.89 (0.001 sec)\n",
      "Step 16600: loss = 0.61 (0.002 sec)\n",
      "Step 16700: loss = 0.48 (0.001 sec)\n",
      "Step 16800: loss = 0.84 (0.002 sec)\n",
      "Step 16900: loss = 0.89 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2724  Accuracy @ 1: 0.6810\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1879  Accuracy @ 1: 0.6833\n",
      "Step 17000: loss = 0.61 (0.001 sec)\n",
      "Step 17100: loss = 0.48 (0.001 sec)\n",
      "Step 17200: loss = 0.84 (0.001 sec)\n",
      "Step 17300: loss = 0.89 (0.001 sec)\n",
      "Step 17400: loss = 0.61 (0.001 sec)\n",
      "Step 17500: loss = 0.48 (0.001 sec)\n",
      "Step 17600: loss = 0.84 (0.002 sec)\n",
      "Step 17700: loss = 0.89 (0.001 sec)\n",
      "Step 17800: loss = 0.61 (0.001 sec)\n",
      "Step 17900: loss = 0.49 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2753  Accuracy @ 1: 0.6883\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1897  Accuracy @ 1: 0.6898\n",
      "Step 18000: loss = 0.84 (0.003 sec)\n",
      "Step 18100: loss = 0.89 (0.001 sec)\n",
      "Step 18200: loss = 0.61 (0.001 sec)\n",
      "Step 18300: loss = 0.49 (0.001 sec)\n",
      "Step 18400: loss = 0.84 (0.002 sec)\n",
      "Step 18500: loss = 0.89 (0.002 sec)\n",
      "Step 18600: loss = 0.62 (0.001 sec)\n",
      "Step 18700: loss = 0.49 (0.002 sec)\n",
      "Step 18800: loss = 0.84 (0.001 sec)\n",
      "Step 18900: loss = 0.89 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2729  Accuracy @ 1: 0.6823\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1876  Accuracy @ 1: 0.6822\n",
      "Step 19000: loss = 0.62 (0.002 sec)\n",
      "Step 19100: loss = 0.49 (0.001 sec)\n",
      "Step 19200: loss = 0.84 (0.001 sec)\n",
      "Step 19300: loss = 0.89 (0.001 sec)\n",
      "Step 19400: loss = 0.62 (0.001 sec)\n",
      "Step 19500: loss = 0.49 (0.001 sec)\n",
      "Step 19600: loss = 0.84 (0.002 sec)\n",
      "Step 19700: loss = 0.89 (0.001 sec)\n",
      "Step 19800: loss = 0.62 (0.001 sec)\n",
      "Step 19900: loss = 0.49 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2758  Accuracy @ 1: 0.6895\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1897  Accuracy @ 1: 0.6898\n"
     ]
    }
   ],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Generate placeholders for the input feature data and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           NUM_FEATURES))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(inputs_placeholder,\n",
    "                       NUM_FEATURES,\n",
    "                       NUM_CLASSES,\n",
    "                       hidden1,\n",
    "                       hidden2,\n",
    "                       hidden3_units=hidden3)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_ = loss(logits, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    if optimizer_type == \"adam\":\n",
    "        train_op = training_adam(loss_, learning_rate)\n",
    "    elif optimizer_type == \"gradient descent\":\n",
    "        train_op = training_gradient_descent(loss_, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Choose either \\\"adam\\\" or \\\"gradient descent\\\" for \"\n",
    "                         \"`optimizer_type`.\")\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with the actual set of images and labels\n",
    "        # for this particular training step.\n",
    "        feed_dict = fill_feed_dict(train_data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss_],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 100 == 0:\n",
    "\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Train Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    inputs_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    train_data)\n",
    "\n",
    "            # Evaluate against the development set.\n",
    "            if dev_labels is not None:\n",
    "                print('Development Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        dev_data)\n",
    "\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    inputs_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
