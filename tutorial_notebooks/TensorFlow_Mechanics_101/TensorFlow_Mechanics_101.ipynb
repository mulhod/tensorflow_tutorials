{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Mechanics 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes Chong Min made\n",
    "\n",
    "* Optimizer: from Gradient Descent to Adam\n",
    "* batch size: set to 10\n",
    "* size of hidden layer: 8\n",
    "* weight initializer: random_normal_initializer\n",
    "* max steps: 20000\n",
    "\n",
    "Among the above changes, `batch size`, `optimizer`, and `max steps` values were critical.\n",
    "\n",
    "Because batches were randomly generated, the accuracies were flucturated. It should be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tutorial is meant as a companion to the code [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist/)\n",
    "- The goal of this tutorial is to show how to use TensorFlow to train and evaluate a simple feed-forward neural network for handwritten digit classification using the (classic) MNIST data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`mnist.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py), the code for making a fully-connected MNIST model\n",
    "- [`fully_connected_feed.py`](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py), the main code to train the built MNIST model against the downloaded dataset using a feed dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from os import getcwd\n",
    "from os.path import join, dirname\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(join(dirname(getcwd()), \"src\"))\n",
    "from utils import (read_data, DataSet, inference, loss, training_adam,\n",
    "                   training_gradient_descent, evaluation, fill_feed_dict,\n",
    "                   do_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = join(dirname(getcwd()), \"data\", \"test_data_revised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom e-rater Data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "# NOTE: Download test_data_revised.zip (in email since it can't be shared)\n",
    "# and save it somewhere, preferably in the \"tutorial_notebooks/data\"\n",
    "# directory. If it is somewhere else, just make sure to pass in the path when\n",
    "# this function is used.\n",
    "\n",
    "# Choose \"micro\" or \"macro\". This will change the types of features we're\n",
    "# using. There are 220 \"micro\" features in total while thre are 9 macro\n",
    "# features.\n",
    "dataset_type = \"macro\"\n",
    "# dataset_type = \"micro\"\n",
    "\n",
    "(train_ids, train_features, train_labels,\n",
    " test_ids, test_features, test_labels,\n",
    " dev_ids, dev_features, dev_labels) = read_data(data_path,\n",
    "                                                macro_or_micro=dataset_type,\n",
    "                                                dev_set=False)\n",
    "#random_sampler = False\n",
    "random_sampler = True\n",
    "train_data = DataSet(train_ids, train_features, train_labels, random_=random_sampler)\n",
    "test_data = DataSet(test_ids, test_features, test_labels)\n",
    "if dev_labels is not None:\n",
    "    dev_data = DataSet(dev_ids, dev_features, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_data = False\n",
    "#show_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    print(\"Shape of data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "          .format(train_features.shape,\n",
    "                  \"\" if dev_features is None\n",
    "                     else \"Development: {}\\n\\t\".format(dev_features.shape),\n",
    "                  test_features.shape))\n",
    "    print(\"Shape of labels data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "          .format(train_labels.shape,\n",
    "                  \"\" if dev_labels is None\n",
    "                     else \"Development: {}\\n\\t\".format(dev_labels.shape),\n",
    "                  test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What the features look like\n",
    "if show_data:\n",
    "    train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Labels are on a 0 to 5 scale (scores 1 to 6)\n",
    "if show_data:\n",
    "    train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    train_labels = np.array(train_labels, dtype=np.float32)\n",
    "    test_labels = np.array(test_labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if show_data:\n",
    "    if dev_labels is not None:\n",
    "        print(dev_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n",
    "max_steps = 20000\n",
    "optimizer_type = \"adam\"\n",
    "#optimizer_type = \"gradient descent\"\n",
    "if dataset_type == \"macro\":\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 8\n",
    "    hidden2 = 8\n",
    "    hidden3 = None\n",
    "    NUM_FEATURES = 9\n",
    "    batch_size = 10\n",
    "else:\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 512\n",
    "    hidden2 = 128\n",
    "    hidden3 = 16\n",
    "    NUM_FEATURES = 220\n",
    "    batch_size = 200\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Based on `mnist.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.79 (0.005 sec)\n",
      "Step 100: loss = 1.62 (0.001 sec)\n",
      "Step 200: loss = 0.89 (0.001 sec)\n",
      "Step 300: loss = 0.87 (0.001 sec)\n",
      "Step 400: loss = 1.56 (0.002 sec)\n",
      "Step 500: loss = 1.66 (0.001 sec)\n",
      "Step 600: loss = 0.87 (0.001 sec)\n",
      "Step 700: loss = 0.85 (0.001 sec)\n",
      "Step 800: loss = 1.54 (0.002 sec)\n",
      "Step 900: loss = 1.59 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 1000: loss = 0.89 (0.001 sec)\n",
      "Step 1100: loss = 0.81 (0.001 sec)\n",
      "Step 1200: loss = 1.36 (0.001 sec)\n",
      "Step 1300: loss = 1.44 (0.001 sec)\n",
      "Step 1400: loss = 0.72 (0.001 sec)\n",
      "Step 1500: loss = 0.53 (0.001 sec)\n",
      "Step 1600: loss = 1.04 (0.002 sec)\n",
      "Step 1700: loss = 1.13 (0.001 sec)\n",
      "Step 1800: loss = 0.58 (0.001 sec)\n",
      "Step 1900: loss = 0.42 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2534  Accuracy @ 1: 0.6335\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1751  Accuracy @ 1: 0.6367\n",
      "Step 2000: loss = 0.96 (0.001 sec)\n",
      "Step 2100: loss = 0.99 (0.001 sec)\n",
      "Step 2200: loss = 0.52 (0.001 sec)\n",
      "Step 2300: loss = 0.39 (0.001 sec)\n",
      "Step 2400: loss = 0.94 (0.002 sec)\n",
      "Step 2500: loss = 0.96 (0.001 sec)\n",
      "Step 2600: loss = 0.46 (0.001 sec)\n",
      "Step 2700: loss = 0.38 (0.001 sec)\n",
      "Step 2800: loss = 0.92 (0.002 sec)\n",
      "Step 2900: loss = 0.95 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2423  Accuracy @ 1: 0.6058\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1666  Accuracy @ 1: 0.6058\n",
      "Step 3000: loss = 0.43 (0.001 sec)\n",
      "Step 3100: loss = 0.38 (0.001 sec)\n",
      "Step 3200: loss = 0.93 (0.001 sec)\n",
      "Step 3300: loss = 0.93 (0.001 sec)\n",
      "Step 3400: loss = 0.41 (0.002 sec)\n",
      "Step 3500: loss = 0.38 (0.001 sec)\n",
      "Step 3600: loss = 0.92 (0.002 sec)\n",
      "Step 3700: loss = 0.93 (0.001 sec)\n",
      "Step 3800: loss = 0.41 (0.001 sec)\n",
      "Step 3900: loss = 0.38 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2639  Accuracy @ 1: 0.6597\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1821  Accuracy @ 1: 0.6622\n",
      "Step 4000: loss = 0.90 (0.002 sec)\n",
      "Step 4100: loss = 0.93 (0.001 sec)\n",
      "Step 4200: loss = 0.40 (0.001 sec)\n",
      "Step 4300: loss = 0.38 (0.001 sec)\n",
      "Step 4400: loss = 0.89 (0.002 sec)\n",
      "Step 4500: loss = 0.95 (0.001 sec)\n",
      "Step 4600: loss = 0.42 (0.001 sec)\n",
      "Step 4700: loss = 0.39 (0.001 sec)\n",
      "Step 4800: loss = 0.90 (0.001 sec)\n",
      "Step 4900: loss = 0.95 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2663  Accuracy @ 1: 0.6657\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1835  Accuracy @ 1: 0.6673\n",
      "Step 5000: loss = 0.45 (0.002 sec)\n",
      "Step 5100: loss = 0.40 (0.001 sec)\n",
      "Step 5200: loss = 0.92 (0.002 sec)\n",
      "Step 5300: loss = 0.96 (0.001 sec)\n",
      "Step 5400: loss = 0.47 (0.001 sec)\n",
      "Step 5500: loss = 0.41 (0.002 sec)\n",
      "Step 5600: loss = 0.90 (0.002 sec)\n",
      "Step 5700: loss = 0.95 (0.002 sec)\n",
      "Step 5800: loss = 0.49 (0.001 sec)\n",
      "Step 5900: loss = 0.42 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2646  Accuracy @ 1: 0.6615\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1813  Accuracy @ 1: 0.6593\n",
      "Step 6000: loss = 0.89 (0.001 sec)\n",
      "Step 6100: loss = 0.94 (0.001 sec)\n",
      "Step 6200: loss = 0.50 (0.001 sec)\n",
      "Step 6300: loss = 0.42 (0.001 sec)\n",
      "Step 6400: loss = 0.90 (0.001 sec)\n",
      "Step 6500: loss = 0.94 (0.001 sec)\n",
      "Step 6600: loss = 0.47 (0.001 sec)\n",
      "Step 6700: loss = 0.43 (0.001 sec)\n",
      "Step 6800: loss = 0.89 (0.002 sec)\n",
      "Step 6900: loss = 0.94 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2727  Accuracy @ 1: 0.6817\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1870  Accuracy @ 1: 0.6800\n",
      "Step 7000: loss = 0.52 (0.002 sec)\n",
      "Step 7100: loss = 0.43 (0.001 sec)\n",
      "Step 7200: loss = 0.89 (0.001 sec)\n",
      "Step 7300: loss = 0.94 (0.001 sec)\n",
      "Step 7400: loss = 0.49 (0.001 sec)\n",
      "Step 7500: loss = 0.44 (0.001 sec)\n",
      "Step 7600: loss = 0.90 (0.002 sec)\n",
      "Step 7700: loss = 0.93 (0.001 sec)\n",
      "Step 7800: loss = 0.49 (0.001 sec)\n",
      "Step 7900: loss = 0.44 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2617  Accuracy @ 1: 0.6542\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1805  Accuracy @ 1: 0.6564\n",
      "Step 8000: loss = 0.89 (0.001 sec)\n",
      "Step 8100: loss = 0.92 (0.001 sec)\n",
      "Step 8200: loss = 0.51 (0.002 sec)\n",
      "Step 8300: loss = 0.44 (0.001 sec)\n",
      "Step 8400: loss = 0.87 (0.002 sec)\n",
      "Step 8500: loss = 0.90 (0.002 sec)\n",
      "Step 8600: loss = 0.54 (0.001 sec)\n",
      "Step 8700: loss = 0.44 (0.001 sec)\n",
      "Step 8800: loss = 0.87 (0.002 sec)\n",
      "Step 8900: loss = 0.89 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2733  Accuracy @ 1: 0.6833\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1878  Accuracy @ 1: 0.6829\n",
      "Step 9000: loss = 0.52 (0.001 sec)\n",
      "Step 9100: loss = 0.45 (0.001 sec)\n",
      "Step 9200: loss = 0.89 (0.001 sec)\n",
      "Step 9300: loss = 0.88 (0.001 sec)\n",
      "Step 9400: loss = 0.49 (0.001 sec)\n",
      "Step 9500: loss = 0.46 (0.001 sec)\n",
      "Step 9600: loss = 0.87 (0.001 sec)\n",
      "Step 9700: loss = 0.86 (0.001 sec)\n",
      "Step 9800: loss = 0.50 (0.001 sec)\n",
      "Step 9900: loss = 0.46 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2679  Accuracy @ 1: 0.6697\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1851  Accuracy @ 1: 0.6731\n",
      "Step 10000: loss = 0.88 (0.001 sec)\n",
      "Step 10100: loss = 0.85 (0.001 sec)\n",
      "Step 10200: loss = 0.48 (0.001 sec)\n",
      "Step 10300: loss = 0.46 (0.001 sec)\n",
      "Step 10400: loss = 0.88 (0.002 sec)\n",
      "Step 10500: loss = 0.85 (0.001 sec)\n",
      "Step 10600: loss = 0.48 (0.001 sec)\n",
      "Step 10700: loss = 0.46 (0.001 sec)\n",
      "Step 10800: loss = 0.87 (0.001 sec)\n",
      "Step 10900: loss = 0.85 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2745  Accuracy @ 1: 0.6863\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1890  Accuracy @ 1: 0.6873\n",
      "Step 11000: loss = 0.48 (0.001 sec)\n",
      "Step 11100: loss = 0.46 (0.001 sec)\n",
      "Step 11200: loss = 0.88 (0.001 sec)\n",
      "Step 11300: loss = 0.85 (0.001 sec)\n",
      "Step 11400: loss = 0.48 (0.002 sec)\n",
      "Step 11500: loss = 0.46 (0.002 sec)\n",
      "Step 11600: loss = 0.88 (0.001 sec)\n",
      "Step 11700: loss = 0.83 (0.001 sec)\n",
      "Step 11800: loss = 0.47 (0.002 sec)\n",
      "Step 11900: loss = 0.47 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2711  Accuracy @ 1: 0.6777\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1864  Accuracy @ 1: 0.6778\n",
      "Step 12000: loss = 0.88 (0.001 sec)\n",
      "Step 12100: loss = 0.85 (0.001 sec)\n",
      "Step 12200: loss = 0.48 (0.001 sec)\n",
      "Step 12300: loss = 0.47 (0.002 sec)\n",
      "Step 12400: loss = 0.88 (0.002 sec)\n",
      "Step 12500: loss = 0.85 (0.001 sec)\n",
      "Step 12600: loss = 0.48 (0.001 sec)\n",
      "Step 12700: loss = 0.48 (0.001 sec)\n",
      "Step 12800: loss = 0.89 (0.001 sec)\n",
      "Step 12900: loss = 0.83 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2756  Accuracy @ 1: 0.6890\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1895  Accuracy @ 1: 0.6891\n",
      "Step 13000: loss = 0.48 (0.001 sec)\n",
      "Step 13100: loss = 0.48 (0.001 sec)\n",
      "Step 13200: loss = 0.87 (0.002 sec)\n",
      "Step 13300: loss = 0.83 (0.003 sec)\n",
      "Step 13400: loss = 0.47 (0.001 sec)\n",
      "Step 13500: loss = 0.48 (0.003 sec)\n",
      "Step 13600: loss = 0.87 (0.001 sec)\n",
      "Step 13700: loss = 0.85 (0.001 sec)\n",
      "Step 13800: loss = 0.48 (0.001 sec)\n",
      "Step 13900: loss = 0.48 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2711  Accuracy @ 1: 0.6777\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1869  Accuracy @ 1: 0.6796\n",
      "Step 14000: loss = 0.87 (0.001 sec)\n",
      "Step 14100: loss = 0.83 (0.001 sec)\n",
      "Step 14200: loss = 0.48 (0.001 sec)\n",
      "Step 14300: loss = 0.48 (0.001 sec)\n",
      "Step 14400: loss = 0.87 (0.001 sec)\n",
      "Step 14500: loss = 0.83 (0.001 sec)\n",
      "Step 14600: loss = 0.48 (0.002 sec)\n",
      "Step 14700: loss = 0.48 (0.001 sec)\n",
      "Step 14800: loss = 0.87 (0.001 sec)\n",
      "Step 14900: loss = 0.84 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2760  Accuracy @ 1: 0.6900\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1897  Accuracy @ 1: 0.6898\n",
      "Step 15000: loss = 0.48 (0.001 sec)\n",
      "Step 15100: loss = 0.48 (0.001 sec)\n",
      "Step 15200: loss = 0.87 (0.001 sec)\n",
      "Step 15300: loss = 0.84 (0.001 sec)\n",
      "Step 15400: loss = 0.49 (0.001 sec)\n",
      "Step 15500: loss = 0.48 (0.001 sec)\n",
      "Step 15600: loss = 0.87 (0.002 sec)\n",
      "Step 15700: loss = 0.84 (0.001 sec)\n",
      "Step 15800: loss = 0.49 (0.001 sec)\n",
      "Step 15900: loss = 0.49 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2714  Accuracy @ 1: 0.6785\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1867  Accuracy @ 1: 0.6789\n",
      "Step 16000: loss = 0.87 (0.001 sec)\n",
      "Step 16100: loss = 0.84 (0.001 sec)\n",
      "Step 16200: loss = 0.49 (0.002 sec)\n",
      "Step 16300: loss = 0.49 (0.002 sec)\n",
      "Step 16400: loss = 0.87 (0.001 sec)\n",
      "Step 16500: loss = 0.84 (0.001 sec)\n",
      "Step 16600: loss = 0.49 (0.001 sec)\n",
      "Step 16700: loss = 0.49 (0.001 sec)\n",
      "Step 16800: loss = 0.87 (0.002 sec)\n",
      "Step 16900: loss = 0.84 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2764  Accuracy @ 1: 0.6910\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1902  Accuracy @ 1: 0.6916\n",
      "Step 17000: loss = 0.49 (0.002 sec)\n",
      "Step 17100: loss = 0.48 (0.001 sec)\n",
      "Step 17200: loss = 0.88 (0.003 sec)\n",
      "Step 17300: loss = 0.84 (0.001 sec)\n",
      "Step 17400: loss = 0.49 (0.001 sec)\n",
      "Step 17500: loss = 0.48 (0.001 sec)\n",
      "Step 17600: loss = 0.87 (0.001 sec)\n",
      "Step 17700: loss = 0.84 (0.001 sec)\n",
      "Step 17800: loss = 0.49 (0.001 sec)\n",
      "Step 17900: loss = 0.48 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2724  Accuracy @ 1: 0.6810\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1870  Accuracy @ 1: 0.6800\n",
      "Step 18000: loss = 0.87 (0.002 sec)\n",
      "Step 18100: loss = 0.84 (0.001 sec)\n",
      "Step 18200: loss = 0.50 (0.001 sec)\n",
      "Step 18300: loss = 0.48 (0.001 sec)\n",
      "Step 18400: loss = 0.87 (0.003 sec)\n",
      "Step 18500: loss = 0.84 (0.001 sec)\n",
      "Step 18600: loss = 0.50 (0.001 sec)\n",
      "Step 18700: loss = 0.48 (0.001 sec)\n",
      "Step 18800: loss = 0.87 (0.002 sec)\n",
      "Step 18900: loss = 0.84 (0.002 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2770  Accuracy @ 1: 0.6925\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1903  Accuracy @ 1: 0.6920\n",
      "Step 19000: loss = 0.50 (0.002 sec)\n",
      "Step 19100: loss = 0.48 (0.001 sec)\n",
      "Step 19200: loss = 0.87 (0.001 sec)\n",
      "Step 19300: loss = 0.84 (0.001 sec)\n",
      "Step 19400: loss = 0.50 (0.001 sec)\n",
      "Step 19500: loss = 0.48 (0.001 sec)\n",
      "Step 19600: loss = 0.87 (0.001 sec)\n",
      "Step 19700: loss = 0.84 (0.001 sec)\n",
      "Step 19800: loss = 0.50 (0.001 sec)\n",
      "Step 19900: loss = 0.48 (0.001 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2724  Accuracy @ 1: 0.6810\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1872  Accuracy @ 1: 0.6807\n"
     ]
    }
   ],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Generate placeholders for the input feature data and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           NUM_FEATURES))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(inputs_placeholder,\n",
    "                       NUM_FEATURES,\n",
    "                       NUM_CLASSES,\n",
    "                       hidden1,\n",
    "                       hidden2,\n",
    "                       hidden3_units=hidden3)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_ = loss(logits, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    if optimizer_type == \"adam\":\n",
    "        train_op = training_adam(loss_, learning_rate)\n",
    "    elif optimizer_type == \"gradient descent\":\n",
    "        train_op = training_gradient_descent(loss_, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Choose either \\\"adam\\\" or \\\"gradient descent\\\" for \"\n",
    "                         \"`optimizer_type`.\")\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with the actual set of images and labels\n",
    "        # for this particular training step.\n",
    "        feed_dict = fill_feed_dict(train_data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss_],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 100 == 0:\n",
    "\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Train Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    inputs_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    train_data,\n",
    "                    logits,\n",
    "                    batch_size)\n",
    "\n",
    "            # Evaluate against the development set.\n",
    "            if dev_labels is not None:\n",
    "                print('Development Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        dev_data,\n",
    "                        logits,\n",
    "                        batch_size)\n",
    "\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    inputs_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    test_data,\n",
    "                    logits,\n",
    "                    batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
