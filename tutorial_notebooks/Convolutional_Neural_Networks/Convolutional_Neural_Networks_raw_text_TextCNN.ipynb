{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from os import getcwd, makedirs\n",
    "from os.path import join, dirname, realpath, curdir, exists\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(join(dirname(getcwd()), \"src\"))\n",
    "from utils import (read_text_files_and_labels_with_vocab_processor,\n",
    "                   TextCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_data_path = join(dirname(getcwd()), \"data\")\n",
    "training_data_path = join(base_data_path, \"test_data_revised\",\n",
    "                          \"PRAXIS_rapid_eval_MODEL_TRAINING_2015/*/*.txt\")\n",
    "test_data_path = join(base_data_path, \"test_data_revised\",\n",
    "                      \"PRAXIS_rapid_eval_TESTING_2015/*/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_labels_path = join(base_data_path, \"test_data_revised\",\n",
    "                         \"training_macro.csv\")\n",
    "test_labels_path = join(base_data_path, \"test_data_revised\",\n",
    "                        \"testing_macro.csv\")\n",
    "df = pd.read_csv(train_labels_path)\n",
    "df = pd.concat([df, pd.read_csv(test_labels_path)])\n",
    "df = df[[\"appointment_id\", \"H1\"]]\n",
    "df.rename(columns={\"appointment_id\": \"id\", \"H1\": \"label\"}, inplace=True)\n",
    "ids_to_labels_dict = {}\n",
    "if len(df.id) != len(set(df.id)):\n",
    "    raise ValueError(\"Duplicate IDs!\")\n",
    "for id_ in df.id:\n",
    "    ids_to_labels_dict[id_] = df[df.id == id_].iloc[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create DataSet objects by using read_text_files_and_labels\n",
    "(train_data, test_data, dev_data) = \\\n",
    "    read_text_files_and_labels_with_vocab_processor(ids_to_labels_dict,\n",
    "                                                    training_data_path,\n",
    "                                                    test_data_path,\n",
    "                                                    get_id_from_text_file_func=\n",
    "                                                        lambda x: int(x[:16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# show_data = False\n",
    "show_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:\n",
      "\tTraining: (4000, 1255)\n",
      "\tTest: (2750, 1255)\n"
     ]
    }
   ],
   "source": [
    "if show_data:\n",
    "    print(\"Shape of data:\\n\\tTraining: {}\\n\\tTest: {}\"\n",
    "          .format(train_data._features.shape,\n",
    "                  test_data._features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2,    3, ...,    0,    0,    0],\n",
       "       [   1,   89,  148, ...,    0,    0,    0],\n",
       "       [  19,  203,  244, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [  53,  942,  340, ...,    0,    0,    0],\n",
       "       [  24,  157,   43, ...,    0,    0,    0],\n",
       "       [ 456,   89, 3212, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data._features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allow device soft device placement\n",
    "allow_soft_placement = True\n",
    "\n",
    "# Log placement of ops on devices\n",
    "log_device_placement = False\n",
    "\n",
    "# Number of checkpoints to store\n",
    "NUM_CHECKPOINTS = 5\n",
    "\n",
    "# Evaluate model on dev set after this many steps\n",
    "EVALUATE_EVERY = 100\n",
    "\n",
    "# Number of training epochs\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Use shape of `train_data._features` since it should be the same as for\n",
    "# `test_data._features`/`dev_data._features`\n",
    "NUM_FEATURES = train_data._features.shape[1]\n",
    "\n",
    "# Dimensionality of character embedding\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "# Filter sizes\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "\n",
    "# Number of filters per filter size\n",
    "NUM_FILTERS = 128\n",
    "\n",
    "# L2 regularization lambda\n",
    "L2_REG_LAMBDA = 0.0\n",
    "\n",
    "# Dropout keep probability\n",
    "DROPOUT_KEEP_PROB = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected int32, got list containing Tensors of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-22360998976c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                       \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFILTER_SIZES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                       \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_FILTERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                       l2_reg_lambda=L2_REG_LAMBDA)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Define Training procedure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mmulholland/Documents/learning/tensorflow/tutorial_notebooks/src/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;31m# Combine all the pooled features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mnum_filters_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_filters\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_pool_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters_total\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mmulholland/Documents/learning/tensorflow/tf_env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(concat_dim, values, name)\u001b[0m\n\u001b[1;32m   1073\u001b[0m       ops.convert_to_tensor(concat_dim,\n\u001b[1;32m   1074\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat_dim\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m                             ).assert_is_compatible_with(tensor_shape.scalar())\n\u001b[1;32m   1077\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mmulholland/Documents/learning/tensorflow/tf_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mmulholland/Documents/learning/tensorflow/tf_env/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    174\u001b[0m                                          as_ref=False):\n\u001b[1;32m    175\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mmulholland/Documents/learning/tensorflow/tf_env/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 165\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    166\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/Users/mmulholland/Documents/learning/tensorflow/tf_env/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mmulholland/Documents/learning/tensorflow/tf_env/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 302\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected int32, got list containing Tensors of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = \\\n",
    "        tf.ConfigProto(allow_soft_placement=allow_soft_placement,\n",
    "                       log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=train_data.get_size(),\n",
    "                      num_classes=train_data.get_num_classes(),\n",
    "                      vocab_size=NUM_FEATURES,\n",
    "                      embedding_size=EMBEDDING_DIM,\n",
    "                      filter_sizes=FILTER_SIZES,\n",
    "                      num_filters=NUM_FILTERS,\n",
    "                      l2_reg_lambda=L2_REG_LAMBDA)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars,\n",
    "                                             global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = \\\n",
    "                    tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = \\\n",
    "                    tf.summary.scalar(\"{}/grad/sparsity\".format(v.name),\n",
    "                                      tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = realpath(join(curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary,\n",
    "                                             acc_summary,\n",
    "                                             grad_summaries_merged])\n",
    "        train_summary_dir = join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir,\n",
    "                                                     sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir,\n",
    "                                                   sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory\n",
    "        # already exists so we need to create it\n",
    "        checkpoint_dir = realpath(join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = join(checkpoint_dir, \"model\")\n",
    "        makedirs(checkpoint_dir, exist_ok=True)\n",
    "        saver = tf.train.Saver(tf.global_variables(),\n",
    "                               max_to_keep=NUM_CHECKPOINTS)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "\n",
    "            :param x_batch: batch of training features\n",
    "            :type x_batch: np.array\n",
    "            :param y_batch: batch of labels\n",
    "            :type y_batch: np.array\n",
    "\n",
    "            :returns: None\n",
    "            :rtype: None\n",
    "            \"\"\"\n",
    "\n",
    "            feed_dict = {cnn.input_x: x_batch,\n",
    "                         cnn.input_y: y_batch,\n",
    "                         cnn.dropout_keep_prob: DROPOUT_KEEP_PROB}\n",
    "            (_,\n",
    "             step,\n",
    "             summaries,\n",
    "             loss,\n",
    "             accuracy) = sess.run([train_op,\n",
    "                                   global_step,\n",
    "                                   train_summary_op,\n",
    "                                   cnn.loss,\n",
    "                                   cnn.accuracy],\n",
    "                                  feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\"\n",
    "                  .format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set.\n",
    "\n",
    "            :param x_batch: batch of test features\n",
    "            :type x_batch: np.array\n",
    "            :param y_batch: batch of labels\n",
    "            :type y_batch: np.array\n",
    "\n",
    "            :returns: None\n",
    "            :rtype: None\n",
    "            \"\"\"\n",
    "            \n",
    "            feed_dict = {cnn.input_x: x_batch,\n",
    "                         cnn.input_y: y_batch,\n",
    "                         cnn.dropout_keep_prob: 1.0}\n",
    "            (step,\n",
    "             summaries,\n",
    "             loss,\n",
    "             accuracy) = sess.run([global_step,\n",
    "                                   dev_summary_op,\n",
    "                                   cnn.loss,\n",
    "                                   cnn.accuracy],\n",
    "                                  feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\"\n",
    "                  .format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        #batches = data_helpers.batch_iter(list(zip(x_train, y_train)),\n",
    "        #                                  BATCH_SIZE,\n",
    "        #                                  NUM_EPOCHS)\n",
    "\n",
    "        # Training loop. For each batch...\n",
    "        while True:\n",
    "            batch = train_data.next_batch(BATCH_SIZE)\n",
    "            if not batch: break\n",
    "            ids, features, labels = batch\n",
    "            train_step(features, labels)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(test_data._features, test_data._labels,\n",
    "                         writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix,\n",
    "                                  global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data._labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n",
    "max_steps = 10000\n",
    "optimizer_type = \"adam\"\n",
    "#optimizer_type = \"gradient descent\"\n",
    "learning_rate = 0.01\n",
    "hidden1 = 512\n",
    "hidden2 = 128\n",
    "hidden3 = 16\n",
    "# Use shape of `train_data._features` since it should be the same as for\n",
    "# `test_data._features`/`dev_data._features`\n",
    "NUM_FEATURES = train_data._features.shape[1]\n",
    "batch_size = 10\n",
    "NUM_CLASSES = 6\n",
    "dropout = 0.5\n",
    "NUM_FILTERS = 20\n",
    "FILTERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Generate placeholders for the input feature data and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           NUM_FEATURES))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    x = tf.reshape(inputs_placeholder, shape=[-1, 1, NUM_FEATURES, 1])\n",
    "\n",
    "    conv1 = conv_layer(x, 2, NUM_FILTERS, 2, 2)\n",
    "    print(conv1.get_shape())\n",
    "    conv2 = conv_layer(x, 3, NUM_FILTERS, 2, 2)\n",
    "    print(conv2.get_shape())\n",
    "    conv3 = conv_layer(x, 4, NUM_FILTERS, 2, 2)\n",
    "    print(conv3.get_shape())\n",
    "    \n",
    "    conv = tf.concat(2, [conv1, conv2, conv3])\n",
    "    print(conv.get_shape())\n",
    "    \n",
    "    reshape_length = NUM_FILTERS*FILTERS*conv1.get_shape().as_list()[2]\n",
    "    input_fc = tf.reshape(conv, [-1, reshape_length])\n",
    "    \n",
    "    logits = fully_connected_network(input_fc, [reshape_length, 14, 14], keep_prob, NUM_CLASSES)\n",
    "    \n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_ = loss(logits, labels_placeholder)\n",
    "    \n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    if optimizer_type == \"adam\":\n",
    "        train_op = training_adam(loss_, learning_rate)\n",
    "    elif optimizer_type == \"gradient descent\":\n",
    "        train_op = training_gradient_descent(loss_, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Choose either \\\"adam\\\" or \\\"gradient descent\\\" for \"\n",
    "                         \"`optimizer_type`.\")\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with the actual set of images and labels\n",
    "        # for this particular training step.\n",
    "        feed_dict = fill_feed_dict(train_data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "\n",
    "        feed_dict[keep_prob] = dropout\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss_],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 100 == 0:\n",
    "\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Train Data Eval:')\n",
    "            do_eval_cnn(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        train_data,\n",
    "                        logits,\n",
    "                        batch_size,\n",
    "                        keep_prob, dropout)\n",
    "\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval_cnn(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        test_data,\n",
    "                        logits,\n",
    "                        batch_size,\n",
    "                        keep_prob, dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
