{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from os import getcwd\n",
    "from os.path import join, dirname\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(join(dirname(getcwd()), \"src\"))\n",
    "from utils import (read_text_files_and_labels_with_vocab_processor,\n",
    "                   loss, training_adam, training_gradient_descent,\n",
    "                   evaluation, fill_feed_dict, do_eval_cnn,\n",
    "                   fully_connected_network, conv_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_data_path = join(dirname(getcwd()), \"data\")\n",
    "training_data_path = join(base_data_path, \"test_data_revised\",\n",
    "                          \"PRAXIS_rapid_eval_MODEL_TRAINING_2015/*/*.txt\")\n",
    "test_data_path = join(base_data_path, \"test_data_revised\",\n",
    "                      \"PRAXIS_rapid_eval_TESTING_2015/*/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_labels_path = join(base_data_path, \"test_data_revised\",\n",
    "                         \"training_macro.csv\")\n",
    "test_labels_path = join(base_data_path, \"test_data_revised\",\n",
    "                        \"testing_macro.csv\")\n",
    "df = pd.read_csv(train_labels_path)\n",
    "df = pd.concat([df, pd.read_csv(test_labels_path)])\n",
    "df = df[[\"appointment_id\", \"H1\"]]\n",
    "df.rename(columns={\"appointment_id\": \"id\", \"H1\": \"label\"}, inplace=True)\n",
    "ids_to_labels_dict = {}\n",
    "if len(df.id) != len(set(df.id)):\n",
    "    raise ValueError(\"Duplicate IDs!\")\n",
    "for id_ in df.id:\n",
    "    ids_to_labels_dict[id_] = df[df.id == id_].iloc[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create DataSet objects by using read_text_files_and_labels\n",
    "(train_data, test_data, dev_data) = \\\n",
    "    read_text_files_and_labels_with_vocab_processor(ids_to_labels_dict,\n",
    "                                                    training_data_path,\n",
    "                                                    test_data_path,\n",
    "                                                    get_id_from_text_file_func=\n",
    "                                                        lambda x: int(x[:16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# show_data = False\n",
    "show_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:\n",
      "\tTraining: (4000, 1255)\n",
      "\tTest: (2750, 1255)\n"
     ]
    }
   ],
   "source": [
    "if show_data:\n",
    "    print(\"Shape of data:\\n\\tTraining: {}\\n\\tTest: {}\"\n",
    "          .format(train_data._features.shape,\n",
    "                  test_data._features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2,    3, ...,    0,    0,    0],\n",
       "       [   1,   89,  148, ...,    0,    0,    0],\n",
       "       [  19,  203,  244, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [  53,  942,  340, ...,    0,    0,    0],\n",
       "       [  24,  157,   43, ...,    0,    0,    0],\n",
       "       [ 456,   89, 3212, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data._features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n",
    "max_steps = 10000\n",
    "optimizer_type = \"adam\"\n",
    "#optimizer_type = \"gradient descent\"\n",
    "learning_rate = 0.01\n",
    "hidden1 = 512\n",
    "hidden2 = 128\n",
    "hidden3 = 16\n",
    "# Use shape of `train_data._features` since it should be the same as for\n",
    "# `test_data._features`/`dev_data._features`\n",
    "NUM_FEATURES = train_data._features.shape[1]\n",
    "batch_size = 10\n",
    "NUM_CLASSES = 6\n",
    "dropout = 0.5\n",
    "NUM_FILTERS = 20\n",
    "FILTERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 627, 20)\n",
      "(10, 1, 627, 20)\n",
      "(10, 1, 627, 20)\n",
      "(10, 1, 1881, 20)\n",
      "Step 0: loss = 283788.19 (0.022 sec)\n",
      "Step 100: loss = 1.17 (0.007 sec)\n",
      "Step 200: loss = 0.91 (0.008 sec)\n",
      "Step 300: loss = 1.23 (0.009 sec)\n",
      "Step 400: loss = 1.18 (0.007 sec)\n",
      "Step 500: loss = 1.29 (0.009 sec)\n",
      "Step 600: loss = 0.92 (0.008 sec)\n",
      "Step 700: loss = 1.45 (0.008 sec)\n",
      "Step 800: loss = 1.19 (0.007 sec)\n",
      "Step 900: loss = 1.32 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1394  Accuracy @ 1: 0.5069\n",
      "Step 1000: loss = 0.91 (0.008 sec)\n",
      "Step 1100: loss = 1.35 (0.008 sec)\n",
      "Step 1200: loss = 1.19 (0.010 sec)\n",
      "Step 1300: loss = 1.32 (0.009 sec)\n",
      "Step 1400: loss = 0.90 (0.008 sec)\n",
      "Step 1500: loss = 1.35 (0.009 sec)\n",
      "Step 1600: loss = 1.19 (0.008 sec)\n",
      "Step 1700: loss = 1.31 (0.008 sec)\n",
      "Step 1800: loss = 0.90 (0.008 sec)\n",
      "Step 1900: loss = 1.35 (0.010 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 2000: loss = 1.19 (0.008 sec)\n",
      "Step 2100: loss = 1.31 (0.009 sec)\n",
      "Step 2200: loss = 0.90 (0.008 sec)\n",
      "Step 2300: loss = 1.36 (0.008 sec)\n",
      "Step 2400: loss = 1.19 (0.009 sec)\n",
      "Step 2500: loss = 1.31 (0.009 sec)\n",
      "Step 2600: loss = 0.90 (0.008 sec)\n",
      "Step 2700: loss = 1.36 (0.009 sec)\n",
      "Step 2800: loss = 1.19 (0.008 sec)\n",
      "Step 2900: loss = 1.31 (0.009 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1393  Accuracy @ 1: 0.5065\n",
      "Step 3000: loss = 0.90 (0.009 sec)\n",
      "Step 3100: loss = 1.36 (0.008 sec)\n",
      "Step 3200: loss = 1.19 (0.009 sec)\n",
      "Step 3300: loss = 1.31 (0.009 sec)\n",
      "Step 3400: loss = 0.90 (0.010 sec)\n",
      "Step 3500: loss = 1.36 (0.008 sec)\n",
      "Step 3600: loss = 1.19 (0.009 sec)\n",
      "Step 3700: loss = 1.31 (0.009 sec)\n",
      "Step 3800: loss = 0.90 (0.011 sec)\n",
      "Step 3900: loss = 1.36 (0.009 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1394  Accuracy @ 1: 0.5069\n",
      "Step 4000: loss = 1.19 (0.010 sec)\n",
      "Step 4100: loss = 1.30 (0.009 sec)\n",
      "Step 4200: loss = 0.90 (0.008 sec)\n",
      "Step 4300: loss = 1.36 (0.008 sec)\n",
      "Step 4400: loss = 1.19 (0.008 sec)\n",
      "Step 4500: loss = 1.30 (0.009 sec)\n",
      "Step 4600: loss = 0.90 (0.009 sec)\n",
      "Step 4700: loss = 1.36 (0.008 sec)\n",
      "Step 4800: loss = 1.19 (0.008 sec)\n",
      "Step 4900: loss = 1.30 (0.009 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1397  Accuracy @ 1: 0.5080\n",
      "Step 5000: loss = 0.90 (0.009 sec)\n",
      "Step 5100: loss = 1.36 (0.008 sec)\n",
      "Step 5200: loss = 1.19 (0.008 sec)\n",
      "Step 5300: loss = 1.30 (0.009 sec)\n",
      "Step 5400: loss = 0.90 (0.008 sec)\n",
      "Step 5500: loss = 1.36 (0.008 sec)\n",
      "Step 5600: loss = 1.19 (0.009 sec)\n",
      "Step 5700: loss = 1.30 (0.009 sec)\n",
      "Step 5800: loss = 0.90 (0.009 sec)\n",
      "Step 5900: loss = 1.36 (0.009 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1394  Accuracy @ 1: 0.5069\n",
      "Step 6000: loss = 1.19 (0.009 sec)\n",
      "Step 6100: loss = 1.30 (0.008 sec)\n",
      "Step 6200: loss = 0.90 (0.009 sec)\n",
      "Step 6300: loss = 1.36 (0.027 sec)\n",
      "Step 6400: loss = 1.19 (0.009 sec)\n",
      "Step 6500: loss = 1.30 (0.009 sec)\n",
      "Step 6600: loss = 0.90 (0.008 sec)\n",
      "Step 6700: loss = 1.36 (0.009 sec)\n",
      "Step 6800: loss = 1.19 (0.009 sec)\n",
      "Step 6900: loss = 1.30 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1396  Accuracy @ 1: 0.5076\n",
      "Step 7000: loss = 0.90 (0.009 sec)\n",
      "Step 7100: loss = 1.36 (0.008 sec)\n",
      "Step 7200: loss = 1.19 (0.008 sec)\n",
      "Step 7300: loss = 1.30 (0.008 sec)\n",
      "Step 7400: loss = 0.90 (0.009 sec)\n",
      "Step 7500: loss = 1.36 (0.009 sec)\n",
      "Step 7600: loss = 1.19 (0.008 sec)\n",
      "Step 7700: loss = 1.30 (0.008 sec)\n",
      "Step 7800: loss = 0.90 (0.008 sec)\n",
      "Step 7900: loss = 1.36 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 8000: loss = 1.19 (0.009 sec)\n",
      "Step 8100: loss = 1.30 (0.009 sec)\n",
      "Step 8200: loss = 0.90 (0.008 sec)\n",
      "Step 8300: loss = 1.36 (0.008 sec)\n",
      "Step 8400: loss = 1.19 (0.008 sec)\n",
      "Step 8500: loss = 1.30 (0.009 sec)\n",
      "Step 8600: loss = 0.90 (0.008 sec)\n",
      "Step 8700: loss = 1.36 (0.009 sec)\n",
      "Step 8800: loss = 1.19 (0.009 sec)\n",
      "Step 8900: loss = 1.30 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1392  Accuracy @ 1: 0.5062\n",
      "Step 9000: loss = 0.90 (0.009 sec)\n",
      "Step 9100: loss = 1.36 (0.010 sec)\n",
      "Step 9200: loss = 1.19 (0.008 sec)\n",
      "Step 9300: loss = 1.30 (0.008 sec)\n",
      "Step 9400: loss = 0.90 (0.009 sec)\n",
      "Step 9500: loss = 1.36 (0.008 sec)\n",
      "Step 9600: loss = 1.19 (0.009 sec)\n",
      "Step 9700: loss = 1.30 (0.009 sec)\n",
      "Step 9800: loss = 0.90 (0.008 sec)\n",
      "Step 9900: loss = 1.36 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1398  Accuracy @ 1: 0.5084\n"
     ]
    }
   ],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Generate placeholders for the input feature data and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           NUM_FEATURES))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    x = tf.reshape(inputs_placeholder, shape=[-1, 1, NUM_FEATURES, 1])\n",
    "\n",
    "    conv1 = conv_layer(x, 2, NUM_FILTERS, 2, 2)\n",
    "    print(conv1.get_shape())\n",
    "    conv2 = conv_layer(x, 3, NUM_FILTERS, 2, 2)\n",
    "    print(conv2.get_shape())\n",
    "    conv3 = conv_layer(x, 4, NUM_FILTERS, 2, 2)\n",
    "    print(conv3.get_shape())\n",
    "    \n",
    "    conv = tf.concat(2, [conv1, conv2, conv3])\n",
    "    print(conv.get_shape())\n",
    "    \n",
    "    reshape_length = NUM_FILTERS*FILTERS*conv1.get_shape().as_list()[2]\n",
    "    input_fc = tf.reshape(conv, [-1, reshape_length])\n",
    "    \n",
    "    logits = fully_connected_network(input_fc, [reshape_length, 14, 14], keep_prob, NUM_CLASSES)\n",
    "    \n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_ = loss(logits, labels_placeholder)\n",
    "    \n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    if optimizer_type == \"adam\":\n",
    "        train_op = training_adam(loss_, learning_rate)\n",
    "    elif optimizer_type == \"gradient descent\":\n",
    "        train_op = training_gradient_descent(loss_, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Choose either \\\"adam\\\" or \\\"gradient descent\\\" for \"\n",
    "                         \"`optimizer_type`.\")\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with the actual set of images and labels\n",
    "        # for this particular training step.\n",
    "        feed_dict = fill_feed_dict(train_data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "\n",
    "        feed_dict[keep_prob] = dropout\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss_],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 100 == 0:\n",
    "\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Train Data Eval:')\n",
    "            do_eval_cnn(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        train_data,\n",
    "                        logits,\n",
    "                        batch_size,\n",
    "                        keep_prob, dropout)\n",
    "\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval_cnn(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        test_data,\n",
    "                        logits,\n",
    "                        batch_size,\n",
    "                        keep_prob, dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
