{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from os import getcwd, listdir\n",
    "from os.path import join, dirname, join, splitext, basename\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "sys.path.append(join(dirname(getcwd()), \"src\"))\n",
    "from utils import (read_data, DataSet, inference, loss, training_adam,\n",
    "                   training_gradient_descent, evaluation, fill_feed_dict,\n",
    "                   do_eval_cnn, read_text_files_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_data_path = join(dirname(getcwd()), \"data\")\n",
    "training_data_path = join(base_data_path, \"test_data_revised\",\n",
    "                          \"PRAXIS_rapid_eval_MODEL_TRAINING_2015/*/*.txt\")\n",
    "test_data_path = join(base_data_path, \"test_data_revised\",\n",
    "                      \"PRAXIS_rapid_eval_TESTING_2015/*/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_labels_path = join(base_data_path, \"test_data_revised\",\n",
    "                         \"training_macro.csv\")\n",
    "test_labels_path = join(base_data_path, \"test_data_revised\",\n",
    "                        \"testing_macro.csv\")\n",
    "df = pd.read_csv(train_labels_path)\n",
    "df = pd.concat([df, pd.read_csv(test_labels_path)])\n",
    "df = df[[\"appointment_id\", \"H1\"]]\n",
    "df.rename(columns={\"appointment_id\": \"id\", \"H1\": \"label\"}, inplace=True)\n",
    "ids_to_labels_dict = {}\n",
    "if len(df.id) != len(set(df.id)):\n",
    "    raise ValueError(\"Duplicate IDs!\")\n",
    "for id_ in df.id:\n",
    "    ids_to_labels_dict[id_] = df[df.id == id_].iloc[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create DataSet objects by using read_text_files_and_labels\n",
    "(train_data, test_data, dev_data) = \\\n",
    "    read_text_files_and_labels(ids_to_labels_dict,\n",
    "                               training_data_path,\n",
    "                               test_data_path,\n",
    "                               get_id_from_text_file_func=lambda x: int(x[:16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# show_data = False\n",
    "show_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:\n",
      "\tTraining: (4000, 1219)\n",
      "\tTest: (2750, 1219)\n"
     ]
    }
   ],
   "source": [
    "if show_data:\n",
    "    print(\"Shape of data:\\n\\tTraining: {}\\n\\tTest: {}\"\n",
    "          .format(train_data._features.shape,\n",
    "                  test_data._features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18393, 31788, 11094, ...,    -1,    -1,    -1],\n",
       "       [18393, 16906, 27596, ...,    -1,    -1,    -1],\n",
       "       [18368, 56204,  1884, ...,    -1,    -1,    -1],\n",
       "       ..., \n",
       "       [32218, 33461,  8588, ...,    -1,    -1,    -1],\n",
       "       [10445, 23445, 35110, ...,    -1,    -1,    -1],\n",
       "       [44186, 16906, 40001, ...,    -1,    -1,    -1]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data._features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try different method of loading in text, etc.\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "get_id_from_text_file_func = lambda x: int(x[:16])\n",
    "\n",
    "NON_ALPHA_RE = re.compile(r\"[^a-z0-9\\-']+\")\n",
    "data_paths = [training_data_path, test_data_path]\n",
    "partitions = [\"training\", \"test\"]\n",
    "train_texts = []\n",
    "train_ids = []\n",
    "train_labels = []\n",
    "test_texts = []\n",
    "test_ids = []\n",
    "test_labels = []\n",
    "texts_list = [train_texts, test_texts]\n",
    "ids_lists = [train_ids, test_ids]\n",
    "labels_lists = [train_labels, test_labels]\n",
    "for (texts,\n",
    "     ids_list,\n",
    "     labels_list,\n",
    "     data_path) in zip(texts_list,\n",
    "                       ids_lists,\n",
    "                       labels_lists,\n",
    "                       data_paths):\n",
    "    file_paths = glob(data_path)\n",
    "    if not file_paths:\n",
    "        raise ValueError(\"glob('{}') resulted in no matching file paths!\"\n",
    "                         .format(data_path))\n",
    "    for file_path in file_paths:\n",
    "        id_ = get_id_from_text_file_func(basename(file_path))\n",
    "        ids_list.append(id_)\n",
    "        labels_list.append(ids_to_labels_dict[id_])\n",
    "        with open(file_path) as text_file:\n",
    "            texts.append(clean_str(text_file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colleges should require all students , regardless '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example texts (preview)\n",
    "train_texts[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"in today 's society , the only real function of a \""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor at 0x1252afa58>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "all_texts = list(chain(train_texts, test_texts))\n",
    "max_document_length = max([len(x.split(\" \")) for x in all_texts])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "vocab_processor.fit(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1255"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_texts_vectorized = np.array(list(vocab_processor.transform(train_texts)))\n",
    "test_texts_vectorized = np.array(list(vocab_processor.transform(test_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1255)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2,    3, ...,    0,    0,    0],\n",
       "       [   1,   89,  148, ...,    0,    0,    0],\n",
       "       [  19,  203,  244, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [  53,  942,  340, ...,    0,    0,    0],\n",
       "       [  24,  157,   43, ...,    0,    0,    0],\n",
       "       [ 456,   89, 3212, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: The padding value used is 0 rather than the -1 that the\n",
    "# read_text_files_and_labels function uses\n",
    "train_texts_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2750, 1255)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  53,  942,  340, ...,    0,    0,    0],\n",
       "       [ 456,   89,  300, ...,    0,    0,    0],\n",
       "       [  53,  942,  340, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [ 172, 2208, 3563, ...,    0,    0,    0],\n",
       "       [  53,  203,  169, ...,    0,    0,    0],\n",
       "       [  24, 2152,  346, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new DataSet objects using the other method of reading in\n",
    "# the data, etc.\n",
    "train_texts_vectorized = np.array(train_texts_vectorized, dtype=np.int32)\n",
    "if len(train_ids) != len(np.array(train_ids, dtype=np.int32)):\n",
    "    raise ValueError(\"Decrease in precision causes ID duplicates.\")\n",
    "train_ids = np.array(train_ids, dtype=np.int32)\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "if np.min(train_labels) == 1:\n",
    "    train_labels = train_labels - 1\n",
    "test_texts_vectorized = np.array(test_texts_vectorized, dtype=np.int32)\n",
    "if len(test_ids) != len(np.array(test_ids, dtype=np.int32)):\n",
    "    raise ValueError(\"Decrease in precision causes ID duplicates.\")\n",
    "test_ids = np.array(test_ids, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "if np.min(test_labels) == 1:\n",
    "    test_labels = test_labels - 1\n",
    "\n",
    "train_data = DataSet(train_ids, train_texts_vectorized, train_labels, random_=False)\n",
    "test_data = DataSet(test_ids, test_texts_vectorized, test_labels, random_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2750"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2,    3, ...,    0,    0,    0],\n",
       "       [   1,   89,  148, ...,    0,    0,    0],\n",
       "       [  19,  203,  244, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [  53,  942,  340, ...,    0,    0,    0],\n",
       "       [  24,  157,   43, ...,    0,    0,    0],\n",
       "       [ 456,   89, 3212, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data._features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n",
    "max_steps = 10000\n",
    "optimizer_type = \"adam\"\n",
    "#optimizer_type = \"gradient descent\"\n",
    "learning_rate = 0.01\n",
    "hidden1 = 512\n",
    "hidden2 = 128\n",
    "hidden3 = 16\n",
    "\n",
    "# Use updated value of feature size since the processing method was different\n",
    "# in this case\n",
    "#NUM_FEATURES = 1219\n",
    "NUM_FEATURES = max_document_length\n",
    "\n",
    "batch_size = 10\n",
    "NUM_CLASSES = 6\n",
    "dropout = 0.5\n",
    "NUM_FILTERS = 20\n",
    "FILTERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_network(input_fc, vector_sizes, keep_prob, num_classes):\n",
    "\n",
    "    fc_w1 = tf.Variable(tf.random_normal([vector_sizes[0], vector_sizes[1]]))\n",
    "    fc_b1 = tf.Variable(tf.random_normal([vector_sizes[1]]))\n",
    "\n",
    "    hidden1 = tf.add(tf.matmul(input_fc, fc_w1), fc_b1)\n",
    "    hidden1 = tf.nn.relu(hidden1)\n",
    "    hidden1 = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "    fc_w2 = tf.Variable(tf.random_normal([vector_sizes[1], vector_sizes[2]]))\n",
    "    fc_b2 = tf.Variable(tf.random_normal([vector_sizes[2]]))\n",
    "\n",
    "    hidden2 = tf.add(tf.matmul(hidden1, fc_w2), fc_b2)\n",
    "    hidden2 = tf.nn.relu(hidden2)\n",
    "    hidden2 = tf.nn.dropout(hidden2, keep_prob)\n",
    "\n",
    "    weights = tf.Variable(tf.random_normal([vector_sizes[2], num_classes]))\n",
    "    biases = tf.Variable(tf.random_normal([num_classes]))\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, filter_size, num_filter, max_pool_filter_size, max_pool_stride_size):\n",
    "    \n",
    "    weight = tf.Variable(tf.random_normal([filter_size, 1, 1, num_filter]))\n",
    "    bias = tf.Variable(tf.random_normal([num_filter]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(input, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv, name=\"relu\")\n",
    "    \n",
    "    conv = tf.nn.max_pool(conv,\n",
    "                          ksize=[1, 1, max_pool_filter_size, 1],\n",
    "                          strides=[1, 1, max_pool_stride_size, 1],\n",
    "                          padding='VALID')\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 627, 20)\n",
      "(10, 1, 627, 20)\n",
      "(10, 1, 627, 20)\n",
      "(10, 1, 1881, 20)\n",
      "Step 0: loss = 92481.52 (0.023 sec)\n",
      "Step 100: loss = 1.79 (0.010 sec)\n",
      "Step 200: loss = 1.01 (0.009 sec)\n",
      "Step 300: loss = 1.43 (0.007 sec)\n",
      "Step 400: loss = 1.31 (0.007 sec)\n",
      "Step 500: loss = 1.35 (0.007 sec)\n",
      "Step 600: loss = 0.97 (0.007 sec)\n",
      "Step 700: loss = 1.30 (0.008 sec)\n",
      "Step 800: loss = 1.20 (0.008 sec)\n",
      "Step 900: loss = 1.31 (0.007 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 1000: loss = 0.96 (0.009 sec)\n",
      "Step 1100: loss = 1.41 (0.007 sec)\n",
      "Step 1200: loss = 1.18 (0.007 sec)\n",
      "Step 1300: loss = 1.28 (0.007 sec)\n",
      "Step 1400: loss = 0.89 (0.007 sec)\n",
      "Step 1500: loss = 1.33 (0.007 sec)\n",
      "Step 1600: loss = 1.17 (0.008 sec)\n",
      "Step 1700: loss = 1.30 (0.008 sec)\n",
      "Step 1800: loss = 0.90 (0.014 sec)\n",
      "Step 1900: loss = 1.35 (0.007 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1394  Accuracy @ 1: 0.5069\n",
      "Step 2000: loss = 1.19 (0.008 sec)\n",
      "Step 2100: loss = 1.31 (0.007 sec)\n",
      "Step 2200: loss = 0.90 (0.007 sec)\n",
      "Step 2300: loss = 1.35 (0.010 sec)\n",
      "Step 2400: loss = 1.19 (0.008 sec)\n",
      "Step 2500: loss = 1.30 (0.008 sec)\n",
      "Step 2600: loss = 0.90 (0.007 sec)\n",
      "Step 2700: loss = 1.36 (0.009 sec)\n",
      "Step 2800: loss = 1.19 (0.008 sec)\n",
      "Step 2900: loss = 1.30 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 3000: loss = 0.90 (0.009 sec)\n",
      "Step 3100: loss = 1.36 (0.010 sec)\n",
      "Step 3200: loss = 1.19 (0.009 sec)\n",
      "Step 3300: loss = 1.30 (0.008 sec)\n",
      "Step 3400: loss = 0.90 (0.009 sec)\n",
      "Step 3500: loss = 1.36 (0.008 sec)\n",
      "Step 3600: loss = 1.19 (0.008 sec)\n",
      "Step 3700: loss = 1.30 (0.008 sec)\n",
      "Step 3800: loss = 0.90 (0.008 sec)\n",
      "Step 3900: loss = 1.36 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1393  Accuracy @ 1: 0.5065\n",
      "Step 4000: loss = 1.19 (0.009 sec)\n",
      "Step 4100: loss = 1.30 (0.009 sec)\n",
      "Step 4200: loss = 0.90 (0.008 sec)\n",
      "Step 4300: loss = 1.36 (0.010 sec)\n",
      "Step 4400: loss = 1.19 (0.008 sec)\n",
      "Step 4500: loss = 1.30 (0.007 sec)\n",
      "Step 4600: loss = 0.90 (0.010 sec)\n",
      "Step 4700: loss = 1.36 (0.008 sec)\n",
      "Step 4800: loss = 1.19 (0.008 sec)\n",
      "Step 4900: loss = 1.30 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 5000: loss = 0.90 (0.008 sec)\n",
      "Step 5100: loss = 1.36 (0.011 sec)\n",
      "Step 5200: loss = 1.19 (0.009 sec)\n",
      "Step 5300: loss = 1.30 (0.008 sec)\n",
      "Step 5400: loss = 0.90 (0.009 sec)\n",
      "Step 5500: loss = 1.36 (0.008 sec)\n",
      "Step 5600: loss = 1.19 (0.012 sec)\n",
      "Step 5700: loss = 1.30 (0.009 sec)\n",
      "Step 5800: loss = 0.90 (0.018 sec)\n",
      "Step 5900: loss = 1.36 (0.007 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1392  Accuracy @ 1: 0.5062\n",
      "Step 6000: loss = 1.19 (0.009 sec)\n",
      "Step 6100: loss = 1.30 (0.007 sec)\n",
      "Step 6200: loss = 0.90 (0.008 sec)\n",
      "Step 6300: loss = 1.36 (0.009 sec)\n",
      "Step 6400: loss = 1.19 (0.008 sec)\n",
      "Step 6500: loss = 1.30 (0.008 sec)\n",
      "Step 6600: loss = 0.90 (0.008 sec)\n",
      "Step 6700: loss = 1.36 (0.008 sec)\n",
      "Step 6800: loss = 1.19 (0.008 sec)\n",
      "Step 6900: loss = 1.30 (0.009 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1394  Accuracy @ 1: 0.5069\n",
      "Step 7000: loss = 0.90 (0.010 sec)\n",
      "Step 7100: loss = 1.36 (0.008 sec)\n",
      "Step 7200: loss = 1.19 (0.008 sec)\n",
      "Step 7300: loss = 1.30 (0.008 sec)\n",
      "Step 7400: loss = 0.90 (0.008 sec)\n",
      "Step 7500: loss = 1.36 (0.009 sec)\n",
      "Step 7600: loss = 1.19 (0.009 sec)\n",
      "Step 7700: loss = 1.30 (0.009 sec)\n",
      "Step 7800: loss = 0.90 (0.009 sec)\n",
      "Step 7900: loss = 1.36 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1393  Accuracy @ 1: 0.5065\n",
      "Step 8000: loss = 1.19 (0.035 sec)\n",
      "Step 8100: loss = 1.30 (0.008 sec)\n",
      "Step 8200: loss = 0.90 (0.009 sec)\n",
      "Step 8300: loss = 1.36 (0.009 sec)\n",
      "Step 8400: loss = 1.19 (0.008 sec)\n",
      "Step 8500: loss = 1.30 (0.008 sec)\n",
      "Step 8600: loss = 0.90 (0.009 sec)\n",
      "Step 8700: loss = 1.36 (0.011 sec)\n",
      "Step 8800: loss = 1.19 (0.009 sec)\n",
      "Step 8900: loss = 1.30 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 9000: loss = 0.90 (0.009 sec)\n",
      "Step 9100: loss = 1.36 (0.008 sec)\n",
      "Step 9200: loss = 1.19 (0.008 sec)\n",
      "Step 9300: loss = 1.30 (0.008 sec)\n",
      "Step 9400: loss = 0.90 (0.011 sec)\n",
      "Step 9500: loss = 1.36 (0.008 sec)\n",
      "Step 9600: loss = 1.19 (0.008 sec)\n",
      "Step 9700: loss = 1.30 (0.008 sec)\n",
      "Step 9800: loss = 0.90 (0.009 sec)\n",
      "Step 9900: loss = 1.36 (0.008 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n"
     ]
    }
   ],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Generate placeholders for the input feature data and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           NUM_FEATURES))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    x = tf.reshape(inputs_placeholder, shape=[-1, 1, NUM_FEATURES, 1])\n",
    "\n",
    "    conv1 = conv_layer(x, 2, NUM_FILTERS, 2, 2)\n",
    "    print(conv1.get_shape())\n",
    "    conv2 = conv_layer(x, 3, NUM_FILTERS, 2, 2)\n",
    "    print(conv2.get_shape())\n",
    "    conv3 = conv_layer(x, 4, NUM_FILTERS, 2, 2)\n",
    "    print(conv3.get_shape())\n",
    "    \n",
    "    conv = tf.concat(2, [conv1, conv2, conv3])\n",
    "    print(conv.get_shape())\n",
    "    \n",
    "    reshape_length = NUM_FILTERS*FILTERS*conv1.get_shape().as_list()[2]\n",
    "    input_fc = tf.reshape(conv, [-1, reshape_length])\n",
    "    \n",
    "    logits = fully_connected_network(input_fc, [reshape_length, 14, 14], keep_prob, NUM_CLASSES)\n",
    "    \n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_ = loss(logits, labels_placeholder)\n",
    "    \n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    if optimizer_type == \"adam\":\n",
    "        train_op = training_adam(loss_, learning_rate)\n",
    "    elif optimizer_type == \"gradient descent\":\n",
    "        train_op = training_gradient_descent(loss_, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Choose either \\\"adam\\\" or \\\"gradient descent\\\" for \"\n",
    "                         \"`optimizer_type`.\")\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with the actual set of images and labels\n",
    "        # for this particular training step.\n",
    "        feed_dict = fill_feed_dict(train_data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "\n",
    "        feed_dict[keep_prob] = dropout\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss_],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 100 == 0:\n",
    "\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Train Data Eval:')\n",
    "            do_eval_cnn(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        train_data,\n",
    "                        logits,\n",
    "                        batch_size,\n",
    "                        keep_prob, dropout)\n",
    "\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval_cnn(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        test_data,\n",
    "                        logits,\n",
    "                        batch_size,\n",
    "                        keep_prob, dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
