{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from os import getcwd\n",
    "from os.path import join, dirname\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(join(dirname(getcwd()), \"src\"))\n",
    "from utils import (read_data, DataSet, inference, loss, training_adam,\n",
    "                   training_gradient_descent, evaluation, fill_feed_dict,\n",
    "                   do_eval_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = join(dirname(getcwd()), \"data\", \"test_data_revised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "# NOTE: Download test_data_revised.zip (in email since it can't be shared)\n",
    "# and save it somewhere, preferably in the \"tutorial_notebooks/data\"\n",
    "# directory. If it is somewhere else, just make sure to pass in the path when\n",
    "# this function is used.\n",
    "\n",
    "# Choose \"micro\" or \"macro\". This will change the types of features we're\n",
    "# using. There are 220 \"micro\" features in total while thre are 9 macro\n",
    "# features.\n",
    "# dataset_type = \"macro\"\n",
    "dataset_type = \"micro\"\n",
    "\n",
    "(train_ids, train_features, train_labels,\n",
    " test_ids, test_features, test_labels,\n",
    " dev_ids, dev_features, dev_labels) = read_data(data_path,\n",
    "                                                macro_or_micro=dataset_type,\n",
    "                                                dev_set=False)\n",
    "#random_sampler = False\n",
    "random_sampler = True\n",
    "train_data = DataSet(train_ids, train_features, train_labels, random_=random_sampler)\n",
    "test_data = DataSet(test_ids, test_features, test_labels)\n",
    "if dev_labels is not None:\n",
    "    dev_data = DataSet(dev_ids, dev_features, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show_data = False\n",
    "show_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:\n",
      "\tTraining: (4000, 220)\n",
      "\tTest: (2750, 220)\n",
      "Shape of labels data:\n",
      "\tTraining: (4000,)\n",
      "\tTest: (2750,)\n"
     ]
    }
   ],
   "source": [
    "if show_data:\n",
    "    print(\"Shape of data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "          .format(train_features.shape,\n",
    "                  \"\" if dev_features is None\n",
    "                     else \"Development: {}\\n\\t\".format(dev_features.shape),\n",
    "                  test_features.shape))\n",
    "    print(\"Shape of labels data:\\n\\tTraining: {}\\n\\t{}Test: {}\"\n",
    "          .format(train_labels.shape,\n",
    "                  \"\" if dev_labels is None\n",
    "                     else \"Development: {}\\n\\t\".format(dev_labels.shape),\n",
    "                  test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "log_dir_path = join(getcwd(), \"logs\")\n",
    "max_steps = 20000\n",
    "optimizer_type = \"adam\"\n",
    "#optimizer_type = \"gradient descent\"\n",
    "if dataset_type == \"macro\":\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 8\n",
    "    hidden2 = 8\n",
    "    hidden3 = None\n",
    "    NUM_FEATURES = 9\n",
    "    batch_size = 10\n",
    "else:\n",
    "    learning_rate = 0.01\n",
    "    hidden1 = 512\n",
    "    hidden2 = 128\n",
    "    hidden3 = 16\n",
    "    NUM_FEATURES = 220\n",
    "    batch_size = 10\n",
    "NUM_CLASSES = 6\n",
    "dropout = 0.5\n",
    "\n",
    "FILTER_SIZES = [2, 3, 4, 5]\n",
    "NUM_FILTERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_network(input_fc, vector_sizes, keep_prob, num_classes):\n",
    "\n",
    "    fc_w1 = tf.Variable(tf.random_normal([vector_sizes[0], vector_sizes[1]]))\n",
    "    fc_b1 = tf.Variable(tf.random_normal([vector_sizes[1]]))\n",
    "\n",
    "    hidden1 = tf.add(tf.matmul(input_fc, fc_w1), fc_b1)\n",
    "    hidden1 = tf.nn.relu(hidden1)\n",
    "    hidden1 = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "    fc_w2 = tf.Variable(tf.random_normal([vector_sizes[1], vector_sizes[2]]))\n",
    "    fc_b2 = tf.Variable(tf.random_normal([vector_sizes[2]]))\n",
    "\n",
    "    hidden2 = tf.add(tf.matmul(hidden1, fc_w2), fc_b2)\n",
    "    hidden2 = tf.nn.relu(hidden2)\n",
    "    hidden2 = tf.nn.dropout(hidden2, keep_prob)\n",
    "\n",
    "    weights = tf.Variable(tf.random_normal([vector_sizes[2], num_classes]))\n",
    "    biases = tf.Variable(tf.random_normal([num_classes]))\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, filter_size, num_filter, max_pool_filter_size, max_pool_stride_size):\n",
    "    \n",
    "    weight = tf.Variable(tf.random_normal([filter_size, 1, 1, num_filter]))\n",
    "    bias = tf.Variable(tf.random_normal([num_filter]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(input, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv, name=\"relu\")\n",
    "    \n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, 1, max_pool_filter_size, 1], strides=[1, 1, max_pool_stride_size, 1],\n",
    "                          padding='VALID')\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate placeholders for the input feature data and labels.\n",
    "inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                       NUM_FEATURES))\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "x = tf.reshape(inputs_placeholder, shape=[-1, 1, NUM_FEATURES, 1])\n",
    "\n",
    "conv1 = conv_layer(x, 2, 9, 2, 2)\n",
    "conv2 = conv_layer(x, 3, 9, 2, 2)\n",
    "conv3 = conv_layer(x, 4, 9, 2, 2)\n",
    "\n",
    "conv_concat = tf.concat(2, [conv1, conv2, conv3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id, train_batch, label_batch = train_data.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outs = conv_concat.eval(feed_dict={inputs_placeholder: train_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 330, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_concat_out = conv_concat.eval(feed_dict={inputs_placeholder: train_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 330, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_concat_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 9960648.00 (0.168 sec)\n",
      "Step 100: loss = 1.76 (0.004 sec)\n",
      "Step 200: loss = 1.34 (0.003 sec)\n",
      "Step 300: loss = 0.95 (0.004 sec)\n",
      "Step 400: loss = 0.90 (0.002 sec)\n",
      "Step 500: loss = 1.20 (0.004 sec)\n",
      "Step 600: loss = 1.06 (0.004 sec)\n",
      "Step 700: loss = 0.88 (0.004 sec)\n",
      "Step 800: loss = 0.88 (0.004 sec)\n",
      "Step 900: loss = 1.19 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2075  Accuracy @ 1: 0.5188\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1393  Accuracy @ 1: 0.5065\n",
      "Step 1000: loss = 1.14 (0.004 sec)\n",
      "Step 1100: loss = 0.85 (0.005 sec)\n",
      "Step 1200: loss = 0.85 (0.006 sec)\n",
      "Step 1300: loss = 1.18 (0.003 sec)\n",
      "Step 1400: loss = 1.04 (0.004 sec)\n",
      "Step 1500: loss = 0.85 (0.009 sec)\n",
      "Step 1600: loss = 0.86 (0.005 sec)\n",
      "Step 1700: loss = 1.18 (0.004 sec)\n",
      "Step 1800: loss = 1.19 (0.004 sec)\n",
      "Step 1900: loss = 0.85 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2087  Accuracy @ 1: 0.5218\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 2000: loss = 0.84 (0.004 sec)\n",
      "Step 2100: loss = 1.17 (0.003 sec)\n",
      "Step 2200: loss = 1.04 (0.003 sec)\n",
      "Step 2300: loss = 0.85 (0.004 sec)\n",
      "Step 2400: loss = 0.84 (0.010 sec)\n",
      "Step 2500: loss = 1.17 (0.004 sec)\n",
      "Step 2600: loss = 1.04 (0.004 sec)\n",
      "Step 2700: loss = 0.85 (0.004 sec)\n",
      "Step 2800: loss = 0.84 (0.004 sec)\n",
      "Step 2900: loss = 1.17 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2089  Accuracy @ 1: 0.5222\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1394  Accuracy @ 1: 0.5069\n",
      "Step 3000: loss = 1.04 (0.004 sec)\n",
      "Step 3100: loss = 0.85 (0.008 sec)\n",
      "Step 3200: loss = 0.84 (0.004 sec)\n",
      "Step 3300: loss = 1.19 (0.003 sec)\n",
      "Step 3400: loss = 1.04 (0.004 sec)\n",
      "Step 3500: loss = 0.85 (0.004 sec)\n",
      "Step 3600: loss = 0.84 (0.004 sec)\n",
      "Step 3700: loss = 1.17 (0.004 sec)\n",
      "Step 3800: loss = 1.04 (0.004 sec)\n",
      "Step 3900: loss = 0.85 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 4000: loss = 0.84 (0.004 sec)\n",
      "Step 4100: loss = 1.17 (0.003 sec)\n",
      "Step 4200: loss = 1.04 (0.004 sec)\n",
      "Step 4300: loss = 0.87 (0.004 sec)\n",
      "Step 4400: loss = 0.84 (0.004 sec)\n",
      "Step 4500: loss = 1.17 (0.004 sec)\n",
      "Step 4600: loss = 1.04 (0.004 sec)\n",
      "Step 4700: loss = 0.85 (0.004 sec)\n",
      "Step 4800: loss = 0.84 (0.003 sec)\n",
      "Step 4900: loss = 1.17 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 5000: loss = 1.04 (0.004 sec)\n",
      "Step 5100: loss = 0.85 (0.004 sec)\n",
      "Step 5200: loss = 0.84 (0.003 sec)\n",
      "Step 5300: loss = 1.17 (0.004 sec)\n",
      "Step 5400: loss = 1.04 (0.004 sec)\n",
      "Step 5500: loss = 0.85 (0.004 sec)\n",
      "Step 5600: loss = 0.84 (0.004 sec)\n",
      "Step 5700: loss = 1.17 (0.004 sec)\n",
      "Step 5800: loss = 1.04 (0.004 sec)\n",
      "Step 5900: loss = 0.85 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 6000: loss = 0.84 (0.004 sec)\n",
      "Step 6100: loss = 1.17 (0.003 sec)\n",
      "Step 6200: loss = 1.04 (0.004 sec)\n",
      "Step 6300: loss = 0.85 (0.003 sec)\n",
      "Step 6400: loss = 0.84 (0.009 sec)\n",
      "Step 6500: loss = 1.17 (0.005 sec)\n",
      "Step 6600: loss = 1.04 (0.004 sec)\n",
      "Step 6700: loss = 0.85 (0.004 sec)\n",
      "Step 6800: loss = 0.84 (0.003 sec)\n",
      "Step 6900: loss = 1.17 (0.006 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 7000: loss = 1.04 (0.004 sec)\n",
      "Step 7100: loss = 0.85 (0.003 sec)\n",
      "Step 7200: loss = 0.84 (0.004 sec)\n",
      "Step 7300: loss = 1.17 (0.004 sec)\n",
      "Step 7400: loss = 1.04 (0.011 sec)\n",
      "Step 7500: loss = 0.85 (0.004 sec)\n",
      "Step 7600: loss = 0.84 (0.004 sec)\n",
      "Step 7700: loss = 1.17 (0.004 sec)\n",
      "Step 7800: loss = 1.04 (0.003 sec)\n",
      "Step 7900: loss = 0.85 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 8000: loss = 0.84 (0.004 sec)\n",
      "Step 8100: loss = 1.17 (0.005 sec)\n",
      "Step 8200: loss = 1.04 (0.004 sec)\n",
      "Step 8300: loss = 0.85 (0.004 sec)\n",
      "Step 8400: loss = 0.84 (0.011 sec)\n",
      "Step 8500: loss = 1.17 (0.004 sec)\n",
      "Step 8600: loss = 1.04 (0.004 sec)\n",
      "Step 8700: loss = 0.85 (0.003 sec)\n",
      "Step 8800: loss = 0.84 (0.003 sec)\n",
      "Step 8900: loss = 1.17 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 9000: loss = 1.04 (0.005 sec)\n",
      "Step 9100: loss = 0.85 (0.005 sec)\n",
      "Step 9200: loss = 0.84 (0.004 sec)\n",
      "Step 9300: loss = 1.17 (0.004 sec)\n",
      "Step 9400: loss = 1.04 (0.003 sec)\n",
      "Step 9500: loss = 0.85 (0.003 sec)\n",
      "Step 9600: loss = 0.84 (0.004 sec)\n",
      "Step 9700: loss = 1.17 (0.003 sec)\n",
      "Step 9800: loss = 1.04 (0.004 sec)\n",
      "Step 9900: loss = 0.85 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 10000: loss = 0.84 (0.006 sec)\n",
      "Step 10100: loss = 1.17 (0.005 sec)\n",
      "Step 10200: loss = 1.04 (0.004 sec)\n",
      "Step 10300: loss = 0.85 (0.004 sec)\n",
      "Step 10400: loss = 0.84 (0.004 sec)\n",
      "Step 10500: loss = 1.17 (0.003 sec)\n",
      "Step 10600: loss = 1.04 (0.004 sec)\n",
      "Step 10700: loss = 0.85 (0.004 sec)\n",
      "Step 10800: loss = 0.84 (0.003 sec)\n",
      "Step 10900: loss = 1.17 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 11000: loss = 1.04 (0.006 sec)\n",
      "Step 11100: loss = 0.85 (0.004 sec)\n",
      "Step 11200: loss = 0.84 (0.005 sec)\n",
      "Step 11300: loss = 1.17 (0.003 sec)\n",
      "Step 11400: loss = 1.04 (0.004 sec)\n",
      "Step 11500: loss = 0.85 (0.004 sec)\n",
      "Step 11600: loss = 0.84 (0.004 sec)\n",
      "Step 11700: loss = 1.17 (0.003 sec)\n",
      "Step 11800: loss = 1.04 (0.003 sec)\n",
      "Step 11900: loss = 0.85 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 12000: loss = 0.84 (0.006 sec)\n",
      "Step 12100: loss = 1.17 (0.004 sec)\n",
      "Step 12200: loss = 1.04 (0.004 sec)\n",
      "Step 12300: loss = 0.85 (0.004 sec)\n",
      "Step 12400: loss = 0.84 (0.004 sec)\n",
      "Step 12500: loss = 1.17 (0.004 sec)\n",
      "Step 12600: loss = 1.04 (0.003 sec)\n",
      "Step 12700: loss = 0.85 (0.004 sec)\n",
      "Step 12800: loss = 0.84 (0.003 sec)\n",
      "Step 12900: loss = 1.17 (0.005 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 13000: loss = 1.04 (0.005 sec)\n",
      "Step 13100: loss = 0.85 (0.004 sec)\n",
      "Step 13200: loss = 0.84 (0.006 sec)\n",
      "Step 13300: loss = 1.17 (0.004 sec)\n",
      "Step 13400: loss = 1.04 (0.004 sec)\n",
      "Step 13500: loss = 0.85 (0.004 sec)\n",
      "Step 13600: loss = 0.84 (0.006 sec)\n",
      "Step 13700: loss = 1.17 (0.004 sec)\n",
      "Step 13800: loss = 1.04 (0.004 sec)\n",
      "Step 13900: loss = 0.85 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 14000: loss = 0.84 (0.005 sec)\n",
      "Step 14100: loss = 1.17 (0.004 sec)\n",
      "Step 14200: loss = 1.04 (0.005 sec)\n",
      "Step 14300: loss = 0.85 (0.004 sec)\n",
      "Step 14400: loss = 0.84 (0.003 sec)\n",
      "Step 14500: loss = 1.17 (0.004 sec)\n",
      "Step 14600: loss = 1.04 (0.004 sec)\n",
      "Step 14700: loss = 0.85 (0.004 sec)\n",
      "Step 14800: loss = 0.84 (0.004 sec)\n",
      "Step 14900: loss = 1.17 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 15000: loss = 1.04 (0.005 sec)\n",
      "Step 15100: loss = 0.85 (0.004 sec)\n",
      "Step 15200: loss = 0.84 (0.004 sec)\n",
      "Step 15300: loss = 1.17 (0.004 sec)\n",
      "Step 15400: loss = 1.04 (0.004 sec)\n",
      "Step 15500: loss = 0.85 (0.007 sec)\n",
      "Step 15600: loss = 0.84 (0.004 sec)\n",
      "Step 15700: loss = 1.17 (0.003 sec)\n",
      "Step 15800: loss = 1.04 (0.003 sec)\n",
      "Step 15900: loss = 0.85 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 16000: loss = 0.84 (0.005 sec)\n",
      "Step 16100: loss = 1.17 (0.006 sec)\n",
      "Step 16200: loss = 1.04 (0.004 sec)\n",
      "Step 16300: loss = 0.85 (0.005 sec)\n",
      "Step 16400: loss = 0.84 (0.004 sec)\n",
      "Step 16500: loss = 1.17 (0.004 sec)\n",
      "Step 16600: loss = 1.04 (0.007 sec)\n",
      "Step 16700: loss = 0.85 (0.004 sec)\n",
      "Step 16800: loss = 0.84 (0.004 sec)\n",
      "Step 16900: loss = 1.17 (0.004 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 17000: loss = 1.04 (0.003 sec)\n",
      "Step 17100: loss = 0.85 (0.005 sec)\n",
      "Step 17200: loss = 0.84 (0.004 sec)\n",
      "Step 17300: loss = 1.17 (0.004 sec)\n",
      "Step 17400: loss = 1.04 (0.005 sec)\n",
      "Step 17500: loss = 0.85 (0.004 sec)\n",
      "Step 17600: loss = 0.84 (0.004 sec)\n",
      "Step 17700: loss = 1.17 (0.003 sec)\n",
      "Step 17800: loss = 1.04 (0.005 sec)\n",
      "Step 17900: loss = 0.85 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 18000: loss = 0.84 (0.004 sec)\n",
      "Step 18100: loss = 1.17 (0.004 sec)\n",
      "Step 18200: loss = 1.04 (0.004 sec)\n",
      "Step 18300: loss = 0.85 (0.005 sec)\n",
      "Step 18400: loss = 0.84 (0.004 sec)\n",
      "Step 18500: loss = 1.17 (0.003 sec)\n",
      "Step 18600: loss = 1.04 (0.004 sec)\n",
      "Step 18700: loss = 0.85 (0.004 sec)\n",
      "Step 18800: loss = 0.84 (0.005 sec)\n",
      "Step 18900: loss = 1.17 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n",
      "Step 19000: loss = 1.04 (0.004 sec)\n",
      "Step 19100: loss = 0.85 (0.005 sec)\n",
      "Step 19200: loss = 0.84 (0.004 sec)\n",
      "Step 19300: loss = 1.17 (0.004 sec)\n",
      "Step 19400: loss = 1.04 (0.004 sec)\n",
      "Step 19500: loss = 0.85 (0.003 sec)\n",
      "Step 19600: loss = 0.84 (0.003 sec)\n",
      "Step 19700: loss = 1.17 (0.003 sec)\n",
      "Step 19800: loss = 1.04 (0.003 sec)\n",
      "Step 19900: loss = 0.85 (0.003 sec)\n",
      "Train Data Eval:\n",
      "  Num examples: 4000  Num correct: 2086  Accuracy @ 1: 0.5215\n",
      "Test Data Eval:\n",
      "  Num examples: 2750  Num correct: 1395  Accuracy @ 1: 0.5073\n"
     ]
    }
   ],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # Generate placeholders for the input feature data and labels.\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           NUM_FEATURES))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    x = tf.reshape(inputs_placeholder, shape=[-1, 1, NUM_FEATURES, 1])\n",
    "\n",
    "    conv1 = conv_layer(x, 2, 9, 2, 2)\n",
    "    conv2 = conv_layer(x, 3, 9, 2, 2)\n",
    "    conv3 = conv_layer(x, 4, 9, 2, 2)    \n",
    "    \n",
    "    conv = tf.concat(2, [conv1, conv2, conv3])\n",
    "    \n",
    "    input_fc = tf.reshape(conv, [-1, 2970])\n",
    "    \n",
    "    logits = fully_connected_network(input_fc, [2970, 14, 14], keep_prob, NUM_CLASSES)\n",
    "    \n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_ = loss(logits, labels_placeholder)\n",
    "    \n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    if optimizer_type == \"adam\":\n",
    "        train_op = training_adam(loss_, learning_rate)\n",
    "    elif optimizer_type == \"gradient descent\":\n",
    "        train_op = training_gradient_descent(loss_, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Choose either \\\"adam\\\" or \\\"gradient descent\\\" for \"\n",
    "                         \"`optimizer_type`.\")\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(log_dir_path, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fill a feed dictionary with the actual set of images and labels\n",
    "        # for this particular training step.\n",
    "        feed_dict = fill_feed_dict(train_data,\n",
    "                                   inputs_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   batch_size)\n",
    "\n",
    "        feed_dict[keep_prob] = dropout\n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value = sess.run([train_op, loss_],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 100 == 0:\n",
    "\n",
    "            # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "            checkpoint_file = join(log_dir_path, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "            # Evaluate against the training set.\n",
    "            print('Train Data Eval:')\n",
    "            do_eval_cnn(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        train_data,\n",
    "                        logits,\n",
    "                        batch_size,\n",
    "                        keep_prob, dropout)\n",
    "\n",
    "            # Evaluate against the development set.\n",
    "            if dev_labels is not None:\n",
    "                print('Development Data Eval:')\n",
    "                do_eval_cnn(sess,\n",
    "                            eval_correct,\n",
    "                            inputs_placeholder,\n",
    "                            labels_placeholder,\n",
    "                            dev_data,\n",
    "                            logits,\n",
    "                            batch_size,\n",
    "                            keep_prob, dropout)\n",
    "\n",
    "            # Evaluate against the test set.\n",
    "            print('Test Data Eval:')\n",
    "            do_eval_cnn(sess,\n",
    "                        eval_correct,\n",
    "                        inputs_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        test_data,\n",
    "                        logits,\n",
    "                        batch_size,\n",
    "                        keep_prob, dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
